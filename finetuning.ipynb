{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3364512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1/20\n",
      "Epoch 0, Batch 0, Loss: 2.3712334632873535\n",
      "Epoch 0, Batch 10, Loss: 2.3384757041931152\n",
      "Epoch 0, Batch 20, Loss: 2.242420196533203\n",
      "Epoch 0, Batch 30, Loss: 2.134303331375122\n",
      "Epoch 0, Batch 40, Loss: 2.0908191204071045\n",
      "Epoch 0, Batch 50, Loss: 2.010789155960083\n",
      "Epoch 0, Batch 60, Loss: 2.0256361961364746\n",
      "Epoch 0, Batch 70, Loss: 1.9019819498062134\n",
      "Epoch 0, Batch 80, Loss: 1.7030620574951172\n",
      "Epoch 0, Batch 90, Loss: 1.7157833576202393\n",
      "Epoch 0, Batch 100, Loss: 1.7372769117355347\n",
      "Epoch 0, Batch 110, Loss: 1.7002419233322144\n",
      "Epoch 0, Batch 120, Loss: 1.6217774152755737\n",
      "Epoch 0, Batch 130, Loss: 1.5312106609344482\n",
      "Epoch 0, Batch 140, Loss: 1.5232921838760376\n",
      "Epoch 0, Batch 150, Loss: 1.4916285276412964\n",
      "Epoch 0, Batch 160, Loss: 1.2463301420211792\n",
      "Epoch 0, Batch 170, Loss: 1.1994186639785767\n",
      "Epoch 0, Batch 180, Loss: 1.0945963859558105\n",
      "Epoch 0, Batch 190, Loss: 1.1047077178955078\n",
      "Epoch 0, Batch 200, Loss: 0.9857563972473145\n",
      "Epoch 0, Batch 210, Loss: 1.015592098236084\n",
      "Epoch 0, Batch 220, Loss: 0.8615095019340515\n",
      "Epoch 0, Batch 230, Loss: 0.9371109008789062\n",
      "Epoch 0, Batch 240, Loss: 1.0139923095703125\n",
      "Epoch 0, Batch 250, Loss: 0.8775297403335571\n",
      "Epoch 0, Batch 260, Loss: 0.7675848007202148\n",
      "Epoch 0, Batch 270, Loss: 0.7148625254631042\n",
      "Epoch 0, Batch 280, Loss: 0.6037337183952332\n",
      "Epoch 0, Batch 290, Loss: 0.6002693176269531\n",
      "Epoch 0, Batch 300, Loss: 0.5300403833389282\n",
      "Epoch 0, Batch 310, Loss: 0.470182329416275\n",
      "Epoch 0, Batch 320, Loss: 0.4310569167137146\n",
      "Epoch 0, Batch 330, Loss: 0.4973623752593994\n",
      "Epoch 0, Batch 340, Loss: 0.538217306137085\n",
      "Epoch 0, Batch 350, Loss: 0.4960266053676605\n",
      "Epoch 0, Batch 360, Loss: 0.4390437602996826\n",
      "Epoch 0, Batch 370, Loss: 0.3981535732746124\n",
      "Epoch 0, Batch 380, Loss: 0.3712686598300934\n",
      "Epoch 0, Batch 390, Loss: 0.44423454999923706\n",
      "Epoch 0, Batch 400, Loss: 0.31920912861824036\n",
      "Epoch 0, Batch 410, Loss: 0.26865115761756897\n",
      "Epoch 0, Batch 420, Loss: 0.23964810371398926\n",
      "Epoch 0, Batch 430, Loss: 0.5157164335250854\n",
      "Epoch 0, Batch 440, Loss: 0.31738993525505066\n",
      "Epoch 0, Batch 450, Loss: 0.32088351249694824\n",
      "Epoch 0, Batch 460, Loss: 0.3732205629348755\n",
      "Epoch 0, Batch 470, Loss: 0.32571983337402344\n",
      "Epoch 0, Batch 480, Loss: 0.2556319534778595\n",
      "Epoch 0, Batch 490, Loss: 0.2851778268814087\n",
      "Epoch 0, Batch 500, Loss: 0.17005544900894165\n",
      "Epoch 0, Batch 510, Loss: 0.29460635781288147\n",
      "Epoch 0, Batch 520, Loss: 0.32853609323501587\n",
      "Epoch 0, Batch 530, Loss: 0.29240551590919495\n",
      "Epoch 0, Batch 540, Loss: 0.23818807303905487\n",
      "Epoch 0, Batch 550, Loss: 0.21564331650733948\n",
      "Epoch 0, Batch 560, Loss: 0.31506574153900146\n",
      "Epoch 0, Batch 570, Loss: 0.2244701087474823\n",
      "Epoch 0, Batch 580, Loss: 0.3136998116970062\n",
      "Epoch 0, Batch 590, Loss: 0.17846837639808655\n",
      "Epoch 0, Batch 600, Loss: 0.18655629456043243\n",
      "Epoch 0, Batch 610, Loss: 0.1368158459663391\n",
      "Epoch 0, Batch 620, Loss: 0.17186002433300018\n",
      "Epoch 0, Batch 630, Loss: 0.11936569213867188\n",
      "Epoch 0, Batch 640, Loss: 0.15351876616477966\n",
      "Epoch 0, Batch 650, Loss: 0.23071767389774323\n",
      "Epoch 0, Batch 660, Loss: 0.1463422328233719\n",
      "Epoch 0, Batch 670, Loss: 0.25967130064964294\n",
      "Epoch 0, Batch 680, Loss: 0.1381770223379135\n",
      "Epoch 0, Batch 690, Loss: 0.19507071375846863\n",
      "Epoch 0, Batch 700, Loss: 0.18857702612876892\n",
      "Epoch 0, Batch 710, Loss: 0.23171626031398773\n",
      "Epoch 0, Batch 720, Loss: 0.16762392222881317\n",
      "Epoch 0, Batch 730, Loss: 0.22714006900787354\n",
      "Epoch 0, Batch 740, Loss: 0.18630754947662354\n",
      "Epoch 0, Batch 750, Loss: 0.17656776309013367\n",
      "Epoch 0, Batch 760, Loss: 0.10863222926855087\n",
      "Epoch 0, Batch 770, Loss: 0.22288742661476135\n",
      "Epoch 0, Batch 780, Loss: 0.18861162662506104\n",
      "Epoch 0, Batch 790, Loss: 0.09614571928977966\n",
      "Epoch 0, Batch 800, Loss: 0.2731660008430481\n",
      "Epoch 0, Batch 810, Loss: 0.160660058259964\n",
      "Epoch 0, Batch 820, Loss: 0.21357126533985138\n",
      "Epoch 0, Batch 830, Loss: 0.1609315574169159\n",
      "Epoch 0, Batch 840, Loss: 0.10981844365596771\n",
      "Epoch 0, Batch 850, Loss: 0.17470765113830566\n",
      "Epoch 0, Batch 860, Loss: 0.10220838338136673\n",
      "Epoch 0, Batch 870, Loss: 0.15497736632823944\n",
      "Epoch 0, Batch 880, Loss: 0.05769098177552223\n",
      "Epoch 0, Batch 890, Loss: 0.1488708257675171\n",
      "Epoch 0, Batch 900, Loss: 0.15386687219142914\n",
      "Epoch 0, Batch 910, Loss: 0.1093108057975769\n",
      "Epoch 0, Batch 920, Loss: 0.1640649139881134\n",
      "Epoch 0, Batch 930, Loss: 0.1319148689508438\n",
      "Completed iteration 1/20\n",
      "Starting iteration 2/20\n",
      "Epoch 0, Batch 0, Loss: 0.09530746191740036\n",
      "Epoch 0, Batch 10, Loss: 0.08151364326477051\n",
      "Epoch 0, Batch 20, Loss: 0.13021034002304077\n",
      "Epoch 0, Batch 30, Loss: 0.1946214884519577\n",
      "Epoch 0, Batch 40, Loss: 0.10396656394004822\n",
      "Epoch 0, Batch 50, Loss: 0.14560189843177795\n",
      "Epoch 0, Batch 60, Loss: 0.07684245705604553\n",
      "Epoch 0, Batch 70, Loss: 0.19402028620243073\n",
      "Epoch 0, Batch 80, Loss: 0.21852520108222961\n",
      "Epoch 0, Batch 90, Loss: 0.08558109402656555\n",
      "Epoch 0, Batch 100, Loss: 0.1854521781206131\n",
      "Epoch 0, Batch 110, Loss: 0.10566060990095139\n",
      "Epoch 0, Batch 120, Loss: 0.0427837036550045\n",
      "Epoch 0, Batch 130, Loss: 0.12148116528987885\n",
      "Epoch 0, Batch 140, Loss: 0.1790834218263626\n",
      "Epoch 0, Batch 150, Loss: 0.14312240481376648\n",
      "Epoch 0, Batch 160, Loss: 0.24631455540657043\n",
      "Epoch 0, Batch 170, Loss: 0.11450187861919403\n",
      "Epoch 0, Batch 180, Loss: 0.1067030280828476\n",
      "Epoch 0, Batch 190, Loss: 0.0949702262878418\n",
      "Epoch 0, Batch 200, Loss: 0.14565691351890564\n",
      "Epoch 0, Batch 210, Loss: 0.06307925283908844\n",
      "Epoch 0, Batch 220, Loss: 0.068697489798069\n",
      "Epoch 0, Batch 230, Loss: 0.056500181555747986\n",
      "Epoch 0, Batch 240, Loss: 0.09303762018680573\n",
      "Epoch 0, Batch 250, Loss: 0.133164182305336\n",
      "Epoch 0, Batch 260, Loss: 0.15196512639522552\n",
      "Epoch 0, Batch 270, Loss: 0.15393754839897156\n",
      "Epoch 0, Batch 280, Loss: 0.10395734757184982\n",
      "Epoch 0, Batch 290, Loss: 0.1632968932390213\n",
      "Epoch 0, Batch 300, Loss: 0.06822372227907181\n",
      "Epoch 0, Batch 310, Loss: 0.09473363310098648\n",
      "Epoch 0, Batch 320, Loss: 0.06410273164510727\n",
      "Epoch 0, Batch 330, Loss: 0.10041461884975433\n",
      "Epoch 0, Batch 340, Loss: 0.2512287497520447\n",
      "Epoch 0, Batch 350, Loss: 0.0965585932135582\n",
      "Epoch 0, Batch 360, Loss: 0.10384805500507355\n",
      "Epoch 0, Batch 370, Loss: 0.059538502246141434\n",
      "Epoch 0, Batch 380, Loss: 0.03623054921627045\n",
      "Epoch 0, Batch 390, Loss: 0.10329876095056534\n",
      "Epoch 0, Batch 400, Loss: 0.13901959359645844\n",
      "Epoch 0, Batch 410, Loss: 0.12022856622934341\n",
      "Epoch 0, Batch 420, Loss: 0.05413183942437172\n",
      "Epoch 0, Batch 430, Loss: 0.12954115867614746\n",
      "Epoch 0, Batch 440, Loss: 0.05911247059702873\n",
      "Epoch 0, Batch 450, Loss: 0.044005610048770905\n",
      "Epoch 0, Batch 460, Loss: 0.14859910309314728\n",
      "Epoch 0, Batch 470, Loss: 0.08957348018884659\n",
      "Epoch 0, Batch 480, Loss: 0.1018638014793396\n",
      "Epoch 0, Batch 490, Loss: 0.09150918573141098\n",
      "Epoch 0, Batch 500, Loss: 0.11045537889003754\n",
      "Epoch 0, Batch 510, Loss: 0.06893394887447357\n",
      "Epoch 0, Batch 520, Loss: 0.15209226310253143\n",
      "Epoch 0, Batch 530, Loss: 0.04771450161933899\n",
      "Epoch 0, Batch 540, Loss: 0.06858716905117035\n",
      "Epoch 0, Batch 550, Loss: 0.14460010826587677\n",
      "Epoch 0, Batch 560, Loss: 0.06727603822946548\n",
      "Epoch 0, Batch 570, Loss: 0.07561901211738586\n",
      "Epoch 0, Batch 580, Loss: 0.045322760939598083\n",
      "Epoch 0, Batch 590, Loss: 0.06420756876468658\n",
      "Epoch 0, Batch 600, Loss: 0.14027558267116547\n",
      "Epoch 0, Batch 610, Loss: 0.13231657445430756\n",
      "Epoch 0, Batch 620, Loss: 0.08823023736476898\n",
      "Epoch 0, Batch 630, Loss: 0.03651299327611923\n",
      "Epoch 0, Batch 640, Loss: 0.10640598833560944\n",
      "Epoch 0, Batch 650, Loss: 0.07261744141578674\n",
      "Epoch 0, Batch 660, Loss: 0.11453171819448471\n",
      "Epoch 0, Batch 670, Loss: 0.11934221535921097\n",
      "Epoch 0, Batch 680, Loss: 0.04540648311376572\n",
      "Epoch 0, Batch 690, Loss: 0.08936504274606705\n",
      "Epoch 0, Batch 700, Loss: 0.1351477950811386\n",
      "Epoch 0, Batch 710, Loss: 0.05270596966147423\n",
      "Epoch 0, Batch 720, Loss: 0.03263689950108528\n",
      "Epoch 0, Batch 730, Loss: 0.12473386526107788\n",
      "Epoch 0, Batch 740, Loss: 0.04811728745698929\n",
      "Epoch 0, Batch 750, Loss: 0.057934071868658066\n",
      "Epoch 0, Batch 760, Loss: 0.185398131608963\n",
      "Epoch 0, Batch 770, Loss: 0.04638346657156944\n",
      "Epoch 0, Batch 780, Loss: 0.11667925119400024\n",
      "Epoch 0, Batch 790, Loss: 0.09028852730989456\n",
      "Epoch 0, Batch 800, Loss: 0.051454078406095505\n",
      "Epoch 0, Batch 810, Loss: 0.05040459707379341\n",
      "Epoch 0, Batch 820, Loss: 0.06321649998426437\n",
      "Epoch 0, Batch 830, Loss: 0.08506974577903748\n",
      "Epoch 0, Batch 840, Loss: 0.08523732423782349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 850, Loss: 0.08436258137226105\n",
      "Epoch 0, Batch 860, Loss: 0.23008093237876892\n",
      "Epoch 0, Batch 870, Loss: 0.25425252318382263\n",
      "Epoch 0, Batch 880, Loss: 0.13638131320476532\n",
      "Epoch 0, Batch 890, Loss: 0.08450151234865189\n",
      "Epoch 0, Batch 900, Loss: 0.09500515460968018\n",
      "Epoch 0, Batch 910, Loss: 0.04323992133140564\n",
      "Epoch 0, Batch 920, Loss: 0.11432589590549469\n",
      "Epoch 0, Batch 930, Loss: 0.033766262233257294\n",
      "Completed iteration 2/20\n",
      "Starting iteration 3/20\n",
      "Epoch 0, Batch 0, Loss: 0.10821042209863663\n",
      "Epoch 0, Batch 10, Loss: 0.10602517426013947\n",
      "Epoch 0, Batch 20, Loss: 0.03781226649880409\n",
      "Epoch 0, Batch 30, Loss: 0.17589157819747925\n",
      "Epoch 0, Batch 40, Loss: 0.04018794745206833\n",
      "Epoch 0, Batch 50, Loss: 0.06625708937644958\n",
      "Epoch 0, Batch 60, Loss: 0.039134521037340164\n",
      "Epoch 0, Batch 70, Loss: 0.07853473722934723\n",
      "Epoch 0, Batch 80, Loss: 0.09167983382940292\n",
      "Epoch 0, Batch 90, Loss: 0.0690116211771965\n",
      "Epoch 0, Batch 100, Loss: 0.0745081752538681\n",
      "Epoch 0, Batch 110, Loss: 0.023728182539343834\n",
      "Epoch 0, Batch 120, Loss: 0.017839975655078888\n",
      "Epoch 0, Batch 130, Loss: 0.18153709173202515\n",
      "Epoch 0, Batch 140, Loss: 0.06244872510433197\n",
      "Epoch 0, Batch 150, Loss: 0.12165651470422745\n",
      "Epoch 0, Batch 160, Loss: 0.062028318643569946\n",
      "Epoch 0, Batch 170, Loss: 0.039507508277893066\n",
      "Epoch 0, Batch 180, Loss: 0.02102493867278099\n",
      "Epoch 0, Batch 190, Loss: 0.06645781546831131\n",
      "Epoch 0, Batch 200, Loss: 0.07195428013801575\n",
      "Epoch 0, Batch 210, Loss: 0.06099410355091095\n",
      "Epoch 0, Batch 220, Loss: 0.04012267664074898\n",
      "Epoch 0, Batch 230, Loss: 0.09029155969619751\n",
      "Epoch 0, Batch 240, Loss: 0.09899626672267914\n",
      "Epoch 0, Batch 250, Loss: 0.0794457271695137\n",
      "Epoch 0, Batch 260, Loss: 0.15108811855316162\n",
      "Epoch 0, Batch 270, Loss: 0.16304153203964233\n",
      "Epoch 0, Batch 280, Loss: 0.05358576402068138\n",
      "Epoch 0, Batch 290, Loss: 0.02552066370844841\n",
      "Epoch 0, Batch 300, Loss: 0.03681156411767006\n",
      "Epoch 0, Batch 310, Loss: 0.13915468752384186\n",
      "Epoch 0, Batch 320, Loss: 0.0661315768957138\n",
      "Epoch 0, Batch 330, Loss: 0.028676722198724747\n",
      "Epoch 0, Batch 340, Loss: 0.1345374435186386\n",
      "Epoch 0, Batch 350, Loss: 0.07580479234457016\n",
      "Epoch 0, Batch 360, Loss: 0.03653595224022865\n",
      "Epoch 0, Batch 370, Loss: 0.05955654755234718\n",
      "Epoch 0, Batch 380, Loss: 0.020649665966629982\n",
      "Epoch 0, Batch 390, Loss: 0.09425856918096542\n",
      "Epoch 0, Batch 400, Loss: 0.08489475399255753\n",
      "Epoch 0, Batch 410, Loss: 0.04722970724105835\n",
      "Epoch 0, Batch 420, Loss: 0.03128175437450409\n",
      "Epoch 0, Batch 430, Loss: 0.06653882563114166\n",
      "Epoch 0, Batch 440, Loss: 0.07477312535047531\n",
      "Epoch 0, Batch 450, Loss: 0.022796278819441795\n",
      "Epoch 0, Batch 460, Loss: 0.05522005632519722\n",
      "Epoch 0, Batch 470, Loss: 0.0734533742070198\n",
      "Epoch 0, Batch 480, Loss: 0.02613155171275139\n",
      "Epoch 0, Batch 490, Loss: 0.049722302705049515\n",
      "Epoch 0, Batch 500, Loss: 0.0454670749604702\n",
      "Epoch 0, Batch 510, Loss: 0.07090061157941818\n",
      "Epoch 0, Batch 520, Loss: 0.060216862708330154\n",
      "Epoch 0, Batch 530, Loss: 0.03392282500863075\n",
      "Epoch 0, Batch 540, Loss: 0.030030691996216774\n",
      "Epoch 0, Batch 550, Loss: 0.24709735810756683\n",
      "Epoch 0, Batch 560, Loss: 0.02392515353858471\n",
      "Epoch 0, Batch 570, Loss: 0.04508516564965248\n",
      "Epoch 0, Batch 580, Loss: 0.0267094224691391\n",
      "Epoch 0, Batch 590, Loss: 0.12170574069023132\n",
      "Epoch 0, Batch 600, Loss: 0.14121878147125244\n",
      "Epoch 0, Batch 610, Loss: 0.03455381840467453\n",
      "Epoch 0, Batch 620, Loss: 0.012839335016906261\n",
      "Epoch 0, Batch 630, Loss: 0.07492507994174957\n",
      "Epoch 0, Batch 640, Loss: 0.09526003152132034\n",
      "Epoch 0, Batch 650, Loss: 0.037007108330726624\n",
      "Epoch 0, Batch 660, Loss: 0.09667635709047318\n",
      "Epoch 0, Batch 670, Loss: 0.08116775006055832\n",
      "Epoch 0, Batch 680, Loss: 0.12235358357429504\n",
      "Epoch 0, Batch 690, Loss: 0.04753388836979866\n",
      "Epoch 0, Batch 700, Loss: 0.009735818952322006\n",
      "Epoch 0, Batch 710, Loss: 0.045341603457927704\n",
      "Epoch 0, Batch 720, Loss: 0.050024695694446564\n",
      "Epoch 0, Batch 730, Loss: 0.07328207045793533\n",
      "Epoch 0, Batch 740, Loss: 0.08567054569721222\n",
      "Epoch 0, Batch 750, Loss: 0.05759758502244949\n",
      "Epoch 0, Batch 760, Loss: 0.0399119071662426\n",
      "Epoch 0, Batch 770, Loss: 0.09862799942493439\n",
      "Epoch 0, Batch 780, Loss: 0.01969260536134243\n",
      "Epoch 0, Batch 790, Loss: 0.015609907917678356\n",
      "Epoch 0, Batch 800, Loss: 0.04312565550208092\n",
      "Epoch 0, Batch 810, Loss: 0.050898563116788864\n",
      "Epoch 0, Batch 820, Loss: 0.02034984901547432\n",
      "Epoch 0, Batch 830, Loss: 0.014244137331843376\n",
      "Epoch 0, Batch 840, Loss: 0.11664746701717377\n",
      "Epoch 0, Batch 850, Loss: 0.14284151792526245\n",
      "Epoch 0, Batch 860, Loss: 0.0328233502805233\n",
      "Epoch 0, Batch 870, Loss: 0.024033037945628166\n",
      "Epoch 0, Batch 880, Loss: 0.06022849678993225\n",
      "Epoch 0, Batch 890, Loss: 0.03380439430475235\n",
      "Epoch 0, Batch 900, Loss: 0.06359188258647919\n",
      "Epoch 0, Batch 910, Loss: 0.2151217758655548\n",
      "Epoch 0, Batch 920, Loss: 0.03750240430235863\n",
      "Epoch 0, Batch 930, Loss: 0.1378536969423294\n",
      "Completed iteration 3/20\n",
      "Starting iteration 4/20\n",
      "Epoch 0, Batch 0, Loss: 0.12407076358795166\n",
      "Epoch 0, Batch 10, Loss: 0.05565538629889488\n",
      "Epoch 0, Batch 20, Loss: 0.028934074565768242\n",
      "Epoch 0, Batch 30, Loss: 0.01951955072581768\n",
      "Epoch 0, Batch 40, Loss: 0.0864407867193222\n",
      "Epoch 0, Batch 50, Loss: 0.009717097505927086\n",
      "Epoch 0, Batch 60, Loss: 0.02209840714931488\n",
      "Epoch 0, Batch 70, Loss: 0.008381437510251999\n",
      "Epoch 0, Batch 80, Loss: 0.026951201260089874\n",
      "Epoch 0, Batch 90, Loss: 0.032434191554784775\n",
      "Epoch 0, Batch 100, Loss: 0.04036297649145126\n",
      "Epoch 0, Batch 110, Loss: 0.0966058075428009\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h3/4spgchm975xd5p79jzqsm2480000gn/T/ipykernel_11767/278037292.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting iteration {i+1}/{num_iterations}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0munlearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforget_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'unlearned_checkpoint_{i}.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/h3/4spgchm975xd5p79jzqsm2480000gn/T/ipykernel_11767/278037292.py\u001b[0m in \u001b[0;36munlearning\u001b[0;34m(net, retain_loader, forget_loader, val_loader)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 使用 CPU\n",
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "# HiddenDataset 类定义\n",
    "class HiddenDataset(Dataset):\n",
    "    def __init__(self, split='train'):\n",
    "        super().__init__()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)), # 调整图像大小以适应 ResNet\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "        if split == 'train':\n",
    "            self.dataset = datasets.MNIST(root='/Users/macbook/Downloads/data', train=True, download=True, transform=self.transform)\n",
    "        else:\n",
    "            self.dataset = datasets.MNIST(root='/Users/macbook/Downloads/data', train=False, download=True, transform=self.transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        return {'image': image, 'age_group': label}\n",
    "\n",
    "# 获取数据集\n",
    "def get_dataset(batch_size):\n",
    "    retain_ds = HiddenDataset(split='train')\n",
    "    forget_ds = HiddenDataset(split='validation')\n",
    "    val_ds = HiddenDataset(split='validation')\n",
    "\n",
    "    retain_loader = DataLoader(retain_ds, batch_size=batch_size, shuffle=True)\n",
    "    forget_loader = DataLoader(forget_ds, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return retain_loader, forget_loader, validation_loader\n",
    "\n",
    "# unlearning 函数\n",
    "def unlearning(net, retain_loader, forget_loader, val_loader):\n",
    "    epochs = 1\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    net.train()\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        net.train()\n",
    "        for batch_idx, sample in enumerate(retain_loader):\n",
    "            inputs = sample[\"image\"]\n",
    "            targets = sample[\"age_group\"]\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10 == 0:  # 每处理 10 个批次打印一次信息\n",
    "                print(f\"Epoch {ep}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "    net.eval()\n",
    "\n",
    "# 创建模型保存目录\n",
    "model_save_dir = '/Users/macbook/Downloads/models'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# 训练和保存模型\n",
    "retain_loader, forget_loader, validation_loader = get_dataset(64)\n",
    "\n",
    "# 初始化 ResNet18 模型并修改第一个卷积层\n",
    "net = resnet18(weights=None, num_classes=10)\n",
    "net.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 修改为单通道输入\n",
    "net.to(DEVICE)\n",
    "\n",
    "# 减少迭代次数\n",
    "num_iterations = 20\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    print(f\"Starting iteration {i+1}/{num_iterations}\")\n",
    "    unlearning(net, retain_loader, forget_loader, validation_loader)\n",
    "    state = net.state_dict()\n",
    "    torch.save(state, os.path.join(model_save_dir, f'unlearned_checkpoint_{i}.pth'))\n",
    "    print(f\"Completed iteration {i+1}/{num_iterations}\")\n",
    "\n",
    "# 检查保存的模型数量\n",
    "unlearned_ckpts = os.listdir(model_save_dir)\n",
    "if len(unlearned_ckpts) != num_iterations:\n",
    "    raise RuntimeError(f'Expected exactly {num_iterations} checkpoints, but found {len(unlearned_ckpts)}.')\n",
    "\n",
    "# 打包模型检查点\n",
    "subprocess.run(f'zip {os.path.join(model_save_dir, \"submission.zip\")} {os.path.join(model_save_dir, \"*.pth\")}', shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd0518b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
