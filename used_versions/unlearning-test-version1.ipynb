{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mw/2gycpkq57p97cpwvmwvm69m00000gn/T/ipykernel_3965/1595774001.py:24: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import variational\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from itertools import cycle\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from typing import List\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "from models import *\n",
    "import models\n",
    "from logger import *\n",
    "import wandb\n",
    "\n",
    "from thirdparty.repdistiller.helper.util import adjust_learning_rate as sgda_adjust_learning_rate\n",
    "from thirdparty.repdistiller.distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss\n",
    "from thirdparty.repdistiller.distiller_zoo import PKT, ABLoss, FactorTransfer, KDSVD, FSP, NSTLoss\n",
    "\n",
    "from thirdparty.repdistiller.helper.loops import train_distill, train_distill_hide, train_distill_linear, train_vanilla, train_negrad, train_bcu, train_bcu_distill, validate\n",
    "from thirdparty.repdistiller.helper.pretrain import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb():\n",
    "    import pdb\n",
    "    pdb.set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_count(model):\n",
    "    count=0\n",
    "    for p in model.parameters():\n",
    "        count+=np.prod(np.array(list(p.shape)))\n",
    "    print(f'Total Number of Parameters: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_params(model):\n",
    "    param = []\n",
    "    for p in model.parameters():\n",
    "        param.append(p.data.view(-1).cpu().numpy())\n",
    "    return np.concatenate(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_shape(model):\n",
    "    for k,p in model.named_parameters():\n",
    "        print(k,p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the original model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: mnist_mlp_1_0_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in mnist_mlp_1_0_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 10\n",
      "State OrderedDict([('layers.0.weight', tensor([[-0.0832, -0.0317,  0.0080,  ...,  0.0650, -0.0346, -0.0338],\n",
      "        [-0.0027, -0.0888, -0.0519,  ..., -0.0379,  0.0615,  0.0149],\n",
      "        [-0.1416, -0.0336,  0.0156,  ...,  0.0427,  0.0694,  0.0308],\n",
      "        ...,\n",
      "        [-0.0182, -0.0285,  0.0117,  ...,  0.0094, -0.0225, -0.0526],\n",
      "        [-0.0159,  0.0288, -0.0066,  ...,  0.0381,  0.0633,  0.0451],\n",
      "        [ 0.0131, -0.0608,  0.0516,  ..., -0.0093,  0.0415, -0.0130]])), ('layers.0.bias', tensor([ 0.2570,  0.5360, -0.9828, -0.2736, -0.2282,  0.3082,  0.4359, -0.1057,\n",
      "        -0.0857,  0.2738,  0.0112, -0.4476,  0.2455,  0.2458, -0.0411, -0.3458,\n",
      "        -0.4463,  0.1637, -0.5212,  0.0843,  0.1513, -0.1780, -0.6362,  0.2063,\n",
      "         0.4134,  0.2404, -0.3497, -0.2096, -0.0949, -0.2470,  0.0197,  0.4874])), ('layers.2.weight', tensor([[-0.3519, -0.2154, -0.3365, -0.1412,  0.4156,  0.1818,  0.0200, -0.0604,\n",
      "          0.2205,  0.2322, -0.0053, -0.1393, -0.2035, -0.0579,  0.0095, -0.1644,\n",
      "          0.2235,  0.3024,  0.2238, -0.7361,  0.2985,  0.1393,  0.1623, -0.0921,\n",
      "         -0.0720, -0.1394,  0.2537, -0.1786,  0.1402, -0.0607,  0.0045,  0.0832],\n",
      "        [ 0.3928, -0.1737, -0.1759, -0.4551,  0.2481, -0.4473, -0.0411, -0.2373,\n",
      "         -0.0809,  0.0645,  0.0849,  0.2062, -0.1421, -0.0616,  0.1801, -0.0840,\n",
      "         -0.2335, -0.0692,  0.1498, -0.0470, -0.1586, -0.1081,  0.0933, -0.2024,\n",
      "         -0.1436, -0.1707, -0.0688, -0.5912,  0.1219, -0.1256,  0.2843, -0.1910],\n",
      "        [ 0.4523, -0.3588,  0.0086,  0.0764,  0.1664,  0.4396,  0.0665,  0.4759,\n",
      "         -0.1288, -0.1356, -0.1838,  0.0269, -0.1764, -0.1239,  0.3496, -0.3435,\n",
      "          0.2170, -0.0702,  0.2358,  0.0587, -0.4519, -0.0437,  0.2893, -0.2968,\n",
      "          0.0510,  0.1216,  0.3035,  0.1648, -0.0432,  0.4584,  0.1138, -0.1609],\n",
      "        [-0.2260, -0.0343, -0.1534, -0.0083, -0.2569,  0.0590,  0.2407,  0.2322,\n",
      "         -0.0340,  0.2132,  0.1880,  0.2921, -0.0644, -0.0330,  0.4186,  0.4952,\n",
      "          0.0323,  0.1553, -0.1318, -0.1697,  0.2046, -0.2184,  0.0321, -0.3517,\n",
      "          0.3038, -0.0949, -0.1500,  0.1053, -0.0209, -0.1221, -0.1718, -0.0792],\n",
      "        [ 0.3664,  0.1804, -0.0120,  0.2229, -0.2088,  0.1287, -0.2451, -0.3353,\n",
      "          0.0897, -0.2442,  0.0131, -0.2590, -0.1019,  0.1473,  0.3632,  0.2329,\n",
      "          0.5305,  0.1337, -0.1440,  0.1620,  0.3382, -0.3171,  0.0806,  0.1881,\n",
      "          0.0109,  0.5281, -0.3445,  0.2525, -0.2200, -0.4710,  0.2022, -0.2743],\n",
      "        [ 0.1823, -0.0562, -0.2757, -0.0546, -0.1238, -0.1061,  0.0172, -0.0347,\n",
      "          0.4565,  0.1192, -0.0601,  0.2493, -0.2201,  0.0361,  0.0090,  0.2869,\n",
      "          0.0090, -0.2832, -0.0779, -0.2768, -0.3961, -0.1289,  0.1251, -0.0015,\n",
      "          0.2930,  0.1101,  0.0197,  0.2491,  0.0698,  0.3177,  0.0416,  0.0137],\n",
      "        [-0.3832,  0.2309, -0.1809,  0.1982,  0.0751,  0.0873, -0.1564,  0.0144,\n",
      "         -0.0449, -0.3521,  0.0023, -0.1973, -0.3833,  0.2016, -0.5147, -0.0191,\n",
      "          0.0122, -0.0809,  0.0038,  0.1643,  0.2994,  0.0281, -0.0899, -0.0444,\n",
      "          0.2220,  0.0072, -0.0604, -0.2090, -0.1963, -0.1330, -0.2076,  0.0046],\n",
      "        [-0.2165, -0.1100,  0.0688, -0.3747,  0.4035, -0.0398, -0.0595, -0.1467,\n",
      "         -0.0216,  0.1590, -0.0791, -0.5878,  0.2140,  0.0116,  0.0842,  0.0181,\n",
      "         -0.2411,  0.1744, -0.0815,  0.3069, -0.1610,  0.1093,  0.1149, -0.2111,\n",
      "         -0.4566, -0.0443,  0.1989, -0.1942, -0.2622, -0.1263,  0.3670, -0.2812],\n",
      "        [-0.1144,  0.1658, -0.1994,  0.2232, -0.0683,  0.2219,  0.2655,  0.4258,\n",
      "         -0.0326, -0.1462, -0.0584,  0.2980, -0.0663,  0.1896, -0.0486,  0.3528,\n",
      "         -0.2598,  0.0683,  0.2263, -0.3286,  0.0065, -0.0126, -0.1758,  0.2649,\n",
      "         -0.1740,  0.2994, -0.0692, -0.3554,  0.0484, -0.0219, -0.4493,  0.4110],\n",
      "        [ 0.4281, -0.0311, -0.3707,  0.5601, -0.2563, -0.1194, -0.2108, -0.0102,\n",
      "         -0.1516, -0.0340, -0.2634,  0.4547, -0.1523,  0.6067, -0.4058,  0.1322,\n",
      "         -0.3815,  0.2264, -0.0823, -0.2132, -0.0130,  0.1347,  0.1836,  0.1278,\n",
      "         -0.0120, -0.0764, -0.3119, -0.1457,  0.3770, -0.0323, -0.4128, -0.1316]])), ('layers.2.bias', tensor([ 0.2496,  0.6326,  0.5569,  0.1073,  0.2888, -0.0639,  0.1895,  0.2966,\n",
      "        -0.0117, -0.5746]))])\n",
      "Args checkpoints/standard_model_for_pilot.pt\n",
      "[0] train metrics:{\"loss\": 1.6957053787638154, \"error\": 0.4350262478126823}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 1.2201567613601685, \"error\": 0.2158}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 7.56 sec\n",
      "[1] train metrics:{\"loss\": 1.135481510219172, \"error\": 0.18546371135738687}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.94 sec\n",
      "[2] train metrics:{\"loss\": 1.0566367169994382, \"error\": 0.1521956503624698}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.49 sec\n",
      "[3] train metrics:{\"loss\": 1.0370397905877864, \"error\": 0.14057161903174736}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 7.53 sec\n",
      "[4] train metrics:{\"loss\": 1.0285105837671848, \"error\": 0.13388467627697692}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.23 sec\n",
      "[5] train metrics:{\"loss\": 1.0239834974342739, \"error\": 0.13053078910090826}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.02 sec\n",
      "[6] train metrics:{\"loss\": 1.0214508123054533, \"error\": 0.12780184984584617}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.91 sec\n",
      "[7] train metrics:{\"loss\": 1.0196682121115777, \"error\": 0.1277601866511124}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.41 sec\n",
      "[8] train metrics:{\"loss\": 1.0185629796230855, \"error\": 0.12644779601699857}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.24 sec\n",
      "[9] train metrics:{\"loss\": 1.0178989100958862, \"error\": 0.1261144904591284}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.78 sec\n",
      "[10] train metrics:{\"loss\": 1.0175718008781531, \"error\": 0.12505207899341722}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.99 sec\n",
      "[11] train metrics:{\"loss\": 1.0170587113515286, \"error\": 0.1247812682276477}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.73 sec\n",
      "[12] train metrics:{\"loss\": 1.016697402378289, \"error\": 0.1252187317723523}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.29 sec\n",
      "[13] train metrics:{\"loss\": 1.016376818798848, \"error\": 0.12488542621448212}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.36 sec\n",
      "[14] train metrics:{\"loss\": 1.0164363585892404, \"error\": 0.12386467794350471}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.31 sec\n",
      "[15] train metrics:{\"loss\": 1.0160483885343825, \"error\": 0.12453128905924506}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.49 sec\n",
      "[16] train metrics:{\"loss\": 1.0159531660292926, \"error\": 0.12482293142238148}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.63 sec\n",
      "[17] train metrics:{\"loss\": 1.015965955176559, \"error\": 0.12442713107241063}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.45 sec\n",
      "[18] train metrics:{\"loss\": 1.0159101984856695, \"error\": 0.1240104991250729}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.62 sec\n",
      "[19] train metrics:{\"loss\": 1.0157534404452986, \"error\": 0.12465627864344637}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.82 sec\n",
      "[20] train metrics:{\"loss\": 1.0156692651229822, \"error\": 0.12405216231980669}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.06 sec\n",
      "[21] train metrics:{\"loss\": 1.015744189621657, \"error\": 0.12451045746187818}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.54 sec\n",
      "[22] train metrics:{\"loss\": 1.015743130570143, \"error\": 0.12415632030664112}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.35 sec\n",
      "[23] train metrics:{\"loss\": 1.015751474828047, \"error\": 0.12469794183818016}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.36 sec\n",
      "[24] train metrics:{\"loss\": 1.0157363732977018, \"error\": 0.12426047829347554}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.36 sec\n",
      "[25] train metrics:{\"loss\": 1.0156331737730246, \"error\": 0.12367719356720273}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.36 sec\n",
      "[26] train metrics:{\"loss\": 1.0157239567924565, \"error\": 0.12380218315140405}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.35 sec\n",
      "[27] train metrics:{\"loss\": 1.0156740184009458, \"error\": 0.12392717273560537}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.31 sec\n",
      "[28] train metrics:{\"loss\": 1.0157745137292935, \"error\": 0.12438546787767686}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.12 sec\n",
      "[29] train metrics:{\"loss\": 1.0156028505246169, \"error\": 0.12455212065661195}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.12 sec\n",
      "[30] train metrics:{\"loss\": 1.015438807938419, \"error\": 0.12313557203566369}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.14 sec\n",
      "Pure training time: 181.59 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset mnist --model mlp --dataroot=data/MNIST/ --filters 1.0 --lr 0.001 \\\n",
    "--resume checkpoints/standard_model_for_pilot.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 --seed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: mnist_mlp_1_0_forget_[0, 1, 2, 3, 4, 5]_num_300_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in mnist_mlp_1_0_forget_[0, 1, 2, 3, 4, 5]_num_300_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Replacing indexes [ 8526 16694 15152 29352   141 36509 37843 16926 35321 26042 21345  5714\n",
      " 47547 14437 39793 44980 45137  3897 35780 29999 33655 37420 40980 28992\n",
      "   105 40544 41016 46885 46091 40786  5543  7014 20505 25936 16792  2457\n",
      "  1600 13007  7760 15774 46362 33702 32877  2543 33599 32886 35244 29239\n",
      "  8640 23665 45315  3038 22738  3554  4458 36789 11462  2191 23706 10872\n",
      "  9290 32216 25081 12968 28129 20523  5261 16762 26382 19263 12728 37875\n",
      " 36911  7326 21202 31998 26810 28700 36488 42838  3180 24261 25761 13600\n",
      " 18425 29148 39305  4254  2025  4307 45980 28150 36331  8747 35060 28517\n",
      "  8694 12015 38188 36605 26116 12256 18607 11847  1442 19196 18989 33194\n",
      " 26487  5803   828 40152 11342 43196 20715 41802 14043  8865 20144 41373\n",
      " 47285  8202 21858  1815 22335  2739 16404  6882  9838 14297  4773 29745\n",
      "  8784 20420  9033 18865 39460 23585 14421 25902 31362 17568 47282 10213\n",
      " 10851 27398  9097 13317  3946 12763 22347 26179 27559 12416 12204 19720\n",
      " 28625 43760   991 18748  3274 11284 33410 22005  3957 23606 30636 39992\n",
      "  1264 25266 17560 31423 46872  6399  8159 35119  1818 44722 35781 44342\n",
      " 25267   634 46251  4819 19794 29351 33480 27853 45953 19391  8342 23363\n",
      " 10898 22639 11929 33139 20729 23386 14207 42742  6533 15472 42530 19571\n",
      " 22377 37233 37954 28603  5153 40736 28158 13824 24596 47089  4148 32854\n",
      " 44602 27694 42719 24075  6046 21712  7058  1106 33206 27058 40874 12132\n",
      " 27016 21465 25892  8743  8810 33435 33167 23515 47454 24149 46878 44043\n",
      " 12757 41548 24280 38149  3902 38656 29473  9135 19714 31691 38719 42099\n",
      " 41461  6753 39036 38931 19219 20760  6409 44396  3192 30070 21387 10447\n",
      " 47641  5579 43237 20422 15418 29575 24229 42701  5380  5442 12690 44635\n",
      " 35380 24691 28596 36012 38802  8417 18947 17449 14339 25511  4570 22447\n",
      " 12060 46231 41953 29310 37274 10157  1795 18569 36490 23796 29340 43197]\n",
      "Number of Classes: 10\n",
      "State OrderedDict([('layers.0.weight', tensor([[-0.0832, -0.0317,  0.0080,  ...,  0.0650, -0.0346, -0.0338],\n",
      "        [-0.0027, -0.0888, -0.0519,  ..., -0.0379,  0.0615,  0.0149],\n",
      "        [-0.1416, -0.0336,  0.0156,  ...,  0.0427,  0.0694,  0.0308],\n",
      "        ...,\n",
      "        [-0.0182, -0.0285,  0.0117,  ...,  0.0094, -0.0225, -0.0526],\n",
      "        [-0.0159,  0.0288, -0.0066,  ...,  0.0381,  0.0633,  0.0451],\n",
      "        [ 0.0131, -0.0608,  0.0516,  ..., -0.0093,  0.0415, -0.0130]])), ('layers.0.bias', tensor([ 0.2570,  0.5360, -0.9828, -0.2736, -0.2282,  0.3082,  0.4359, -0.1057,\n",
      "        -0.0857,  0.2738,  0.0112, -0.4476,  0.2455,  0.2458, -0.0411, -0.3458,\n",
      "        -0.4463,  0.1637, -0.5212,  0.0843,  0.1513, -0.1780, -0.6362,  0.2063,\n",
      "         0.4134,  0.2404, -0.3497, -0.2096, -0.0949, -0.2470,  0.0197,  0.4874])), ('layers.2.weight', tensor([[-0.3519, -0.2154, -0.3365, -0.1412,  0.4156,  0.1818,  0.0200, -0.0604,\n",
      "          0.2205,  0.2322, -0.0053, -0.1393, -0.2035, -0.0579,  0.0095, -0.1644,\n",
      "          0.2235,  0.3024,  0.2238, -0.7361,  0.2985,  0.1393,  0.1623, -0.0921,\n",
      "         -0.0720, -0.1394,  0.2537, -0.1786,  0.1402, -0.0607,  0.0045,  0.0832],\n",
      "        [ 0.3928, -0.1737, -0.1759, -0.4551,  0.2481, -0.4473, -0.0411, -0.2373,\n",
      "         -0.0809,  0.0645,  0.0849,  0.2062, -0.1421, -0.0616,  0.1801, -0.0840,\n",
      "         -0.2335, -0.0692,  0.1498, -0.0470, -0.1586, -0.1081,  0.0933, -0.2024,\n",
      "         -0.1436, -0.1707, -0.0688, -0.5912,  0.1219, -0.1256,  0.2843, -0.1910],\n",
      "        [ 0.4523, -0.3588,  0.0086,  0.0764,  0.1664,  0.4396,  0.0665,  0.4759,\n",
      "         -0.1288, -0.1356, -0.1838,  0.0269, -0.1764, -0.1239,  0.3496, -0.3435,\n",
      "          0.2170, -0.0702,  0.2358,  0.0587, -0.4519, -0.0437,  0.2893, -0.2968,\n",
      "          0.0510,  0.1216,  0.3035,  0.1648, -0.0432,  0.4584,  0.1138, -0.1609],\n",
      "        [-0.2260, -0.0343, -0.1534, -0.0083, -0.2569,  0.0590,  0.2407,  0.2322,\n",
      "         -0.0340,  0.2132,  0.1880,  0.2921, -0.0644, -0.0330,  0.4186,  0.4952,\n",
      "          0.0323,  0.1553, -0.1318, -0.1697,  0.2046, -0.2184,  0.0321, -0.3517,\n",
      "          0.3038, -0.0949, -0.1500,  0.1053, -0.0209, -0.1221, -0.1718, -0.0792],\n",
      "        [ 0.3664,  0.1804, -0.0120,  0.2229, -0.2088,  0.1287, -0.2451, -0.3353,\n",
      "          0.0897, -0.2442,  0.0131, -0.2590, -0.1019,  0.1473,  0.3632,  0.2329,\n",
      "          0.5305,  0.1337, -0.1440,  0.1620,  0.3382, -0.3171,  0.0806,  0.1881,\n",
      "          0.0109,  0.5281, -0.3445,  0.2525, -0.2200, -0.4710,  0.2022, -0.2743],\n",
      "        [ 0.1823, -0.0562, -0.2757, -0.0546, -0.1238, -0.1061,  0.0172, -0.0347,\n",
      "          0.4565,  0.1192, -0.0601,  0.2493, -0.2201,  0.0361,  0.0090,  0.2869,\n",
      "          0.0090, -0.2832, -0.0779, -0.2768, -0.3961, -0.1289,  0.1251, -0.0015,\n",
      "          0.2930,  0.1101,  0.0197,  0.2491,  0.0698,  0.3177,  0.0416,  0.0137],\n",
      "        [-0.3832,  0.2309, -0.1809,  0.1982,  0.0751,  0.0873, -0.1564,  0.0144,\n",
      "         -0.0449, -0.3521,  0.0023, -0.1973, -0.3833,  0.2016, -0.5147, -0.0191,\n",
      "          0.0122, -0.0809,  0.0038,  0.1643,  0.2994,  0.0281, -0.0899, -0.0444,\n",
      "          0.2220,  0.0072, -0.0604, -0.2090, -0.1963, -0.1330, -0.2076,  0.0046],\n",
      "        [-0.2165, -0.1100,  0.0688, -0.3747,  0.4035, -0.0398, -0.0595, -0.1467,\n",
      "         -0.0216,  0.1590, -0.0791, -0.5878,  0.2140,  0.0116,  0.0842,  0.0181,\n",
      "         -0.2411,  0.1744, -0.0815,  0.3069, -0.1610,  0.1093,  0.1149, -0.2111,\n",
      "         -0.4566, -0.0443,  0.1989, -0.1942, -0.2622, -0.1263,  0.3670, -0.2812],\n",
      "        [-0.1144,  0.1658, -0.1994,  0.2232, -0.0683,  0.2219,  0.2655,  0.4258,\n",
      "         -0.0326, -0.1462, -0.0584,  0.2980, -0.0663,  0.1896, -0.0486,  0.3528,\n",
      "         -0.2598,  0.0683,  0.2263, -0.3286,  0.0065, -0.0126, -0.1758,  0.2649,\n",
      "         -0.1740,  0.2994, -0.0692, -0.3554,  0.0484, -0.0219, -0.4493,  0.4110],\n",
      "        [ 0.4281, -0.0311, -0.3707,  0.5601, -0.2563, -0.1194, -0.2108, -0.0102,\n",
      "         -0.1516, -0.0340, -0.2634,  0.4547, -0.1523,  0.6067, -0.4058,  0.1322,\n",
      "         -0.3815,  0.2264, -0.0823, -0.2132, -0.0130,  0.1347,  0.1836,  0.1278,\n",
      "         -0.0120, -0.0764, -0.3119, -0.1457,  0.3770, -0.0323, -0.4128, -0.1316]])), ('layers.2.bias', tensor([ 0.2496,  0.6326,  0.5569,  0.1073,  0.2888, -0.0639,  0.1895,  0.2966,\n",
      "        -0.0117, -0.5746]))])\n",
      "Args checkpoints/standard_model_for_pilot.pt\n",
      "[0] train metrics:{\"loss\": 1.6955508042470524, \"error\": 0.4350262478126823}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 1.220088168334961, \"error\": 0.2163}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 8.4 sec\n",
      "[1] train metrics:{\"loss\": 1.1354534324730707, \"error\": 0.18535955337055246}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 7.4 sec\n",
      "[2] train metrics:{\"loss\": 1.056726030901704, \"error\": 0.15227897675193733}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.95 sec\n",
      "[3] train metrics:{\"loss\": 1.0371734988102366, \"error\": 0.14057161903174736}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.64 sec\n",
      "[4] train metrics:{\"loss\": 1.028648248931466, \"error\": 0.13371802349804182}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.82 sec\n",
      "[5] train metrics:{\"loss\": 1.024162956063285, \"error\": 0.13086409465877843}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.56 sec\n",
      "[6] train metrics:{\"loss\": 1.0216317594711923, \"error\": 0.12803099741688193}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.67 sec\n",
      "[7] train metrics:{\"loss\": 1.0198602803487915, \"error\": 0.1279476710274144}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.7 sec\n",
      "[8] train metrics:{\"loss\": 1.0187525477630677, \"error\": 0.12640613282226482}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.12 sec\n",
      "[9] train metrics:{\"loss\": 1.0180947887193221, \"error\": 0.12623948004332972}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.71 sec\n",
      "[10] train metrics:{\"loss\": 1.0177762444918677, \"error\": 0.12536455295392052}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.53 sec\n",
      "[11] train metrics:{\"loss\": 1.017264149882537, \"error\": 0.12498958420131656}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.6 sec\n",
      "[12] train metrics:{\"loss\": 1.016896297669234, \"error\": 0.1252187317723523}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.24 sec\n",
      "[13] train metrics:{\"loss\": 1.0165873311497888, \"error\": 0.1253437213565536}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.91 sec\n",
      "[14] train metrics:{\"loss\": 1.0166354835376115, \"error\": 0.12396883593033914}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.94 sec\n",
      "[15] train metrics:{\"loss\": 1.0162662003658363, \"error\": 0.12498958420131656}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.77 sec\n",
      "[16] train metrics:{\"loss\": 1.0161743731997765, \"error\": 0.12490625781184901}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.84 sec\n",
      "[17] train metrics:{\"loss\": 1.0162088719817681, \"error\": 0.12465627864344637}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.56 sec\n",
      "[18] train metrics:{\"loss\": 1.016125079642017, \"error\": 0.12413548870927422}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.06 sec\n",
      "[19] train metrics:{\"loss\": 1.0159665824681379, \"error\": 0.12457295225397884}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.23 sec\n",
      "[20] train metrics:{\"loss\": 1.0158672984883483, \"error\": 0.12428130989084243}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.9 sec\n",
      "[21] train metrics:{\"loss\": 1.0159629398599763, \"error\": 0.12455212065661195}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.04 sec\n",
      "[22] train metrics:{\"loss\": 1.0159606663110543, \"error\": 0.12409382551454046}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.64 sec\n",
      "[23] train metrics:{\"loss\": 1.0159797972316056, \"error\": 0.12498958420131656}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.92 sec\n",
      "[24] train metrics:{\"loss\": 1.0159649011910175, \"error\": 0.12407299391717357}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.82 sec\n",
      "[25] train metrics:{\"loss\": 1.015828314502779, \"error\": 0.12384384634613782}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.7 sec\n",
      "[26] train metrics:{\"loss\": 1.0159221884906515, \"error\": 0.12384384634613782}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.38 sec\n",
      "[27] train metrics:{\"loss\": 1.0158880807669817, \"error\": 0.12388550954087159}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.57 sec\n",
      "[28] train metrics:{\"loss\": 1.0159736355884146, \"error\": 0.12469794183818016}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.47 sec\n",
      "[29] train metrics:{\"loss\": 1.0158080630834456, \"error\": 0.12457295225397884}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.52 sec\n",
      "[30] train metrics:{\"loss\": 1.0156555478547615, \"error\": 0.12323973002249812}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.65 sec\n",
      "Pure training time: 182.15999999999997 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset mnist --model mlp --dataroot=data/MNIST/ --filters 1.0 --lr 0.001 \\\n",
    "--resume checkpoints/standard_model_for_pilot.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 \\\n",
    "--forget-class 0,1,2,3,4,5 --num-to-forget 300 --seed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.2353,  0.7190,  0.5154,  0.0759,  0.2476, -0.0129,  0.2155,  0.3334,\n",
      "        -0.1101, -0.5481], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2353,  0.7188,  0.5152,  0.0761,  0.2476, -0.0130,  0.2156,  0.3336,\n",
      "        -0.1102, -0.5480], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "model0 = copy.deepcopy(model)\n",
    "model_initial = copy.deepcopy(model)\n",
    "\n",
    "arch = args.model \n",
    "filters=args.filters\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "augment = False\n",
    "dataset = args.dataset\n",
    "class_to_forget = args.forget_class\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "num_classes=args.num_classes\n",
    "num_to_forget = args.num_to_forget\n",
    "num_total = len(train_loader.dataset)\n",
    "num_to_retain = num_total - 300#num_to_forget\n",
    "seed = args.seed\n",
    "unfreeze_start = None\n",
    "\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "\n",
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(m_name))\n",
    "model0.load_state_dict(torch.load(m0_name))\n",
    "model_initial.load_state_dict(torch.load(init_checkpoint))\n",
    "\n",
    "teacher = copy.deepcopy(model)\n",
    "student = copy.deepcopy(model)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "print(p)\n",
    "for p in model0.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "log_dict={}\n",
    "training_epochs=30"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "log_dict['epoch']=training_epochs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 33130\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameter_count(copy.deepcopy(model))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loads checkpoints"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['args']=args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between w(D) and w(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(model,model0):\n",
    "    distance=0\n",
    "    normalization=0\n",
    "    print(\"model.named_parameters\",model.named_parameters)\n",
    "    print(\"model0.named_parameters\",model0.named_parameters)\n",
    "    print(\"For Loop\")\n",
    "    for (k, p), (k0, p0) in zip(model.named_parameters(), model0.named_parameters()):\n",
    "        space='  ' if 'bias' in k else ''\n",
    "        print(\"p.data0\", p.data0)\n",
    "        print(\"p0.data0\", p0.data0)\n",
    "        current_dist=(p.data0-p0.data0).pow(2).sum().item()\n",
    "        current_norm=p.data0.pow(2).sum().item()\n",
    "        distance+=current_dist\n",
    "        normalization+=current_norm\n",
    "    print(f'Distance: {np.sqrt(distance)}')\n",
    "    print(f'Normalized Distance: {1.0*np.sqrt(distance/normalization)}')\n",
    "    return 1.0*np.sqrt(distance/normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.named_parameters <bound method Module.named_parameters of MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): StandardLinearLayer(in_features=1024, out_features=32, bias=True, beta=0.31622776601683794)\n",
      "    (1): ReLU()\n",
      "    (2): StandardLinearLayer(in_features=32, out_features=10, bias=True, beta=0.31622776601683794)\n",
      "  )\n",
      ")>\n",
      "model0.named_parameters <bound method Module.named_parameters of MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): StandardLinearLayer(in_features=1024, out_features=32, bias=True, beta=0.31622776601683794)\n",
      "    (1): ReLU()\n",
      "    (2): StandardLinearLayer(in_features=32, out_features=10, bias=True, beta=0.31622776601683794)\n",
      "  )\n",
      ")>\n",
      "For Loop\n",
      "p.data0 tensor([[-0.0832, -0.0317,  0.0080,  ...,  0.0650, -0.0346, -0.0338],\n",
      "        [-0.0027, -0.0888, -0.0519,  ..., -0.0379,  0.0615,  0.0149],\n",
      "        [-0.1416, -0.0336,  0.0156,  ...,  0.0427,  0.0694,  0.0308],\n",
      "        ...,\n",
      "        [-0.0182, -0.0285,  0.0117,  ...,  0.0094, -0.0225, -0.0526],\n",
      "        [-0.0159,  0.0288, -0.0066,  ...,  0.0381,  0.0633,  0.0451],\n",
      "        [ 0.0131, -0.0608,  0.0516,  ..., -0.0093,  0.0415, -0.0130]])\n",
      "p0.data0 tensor([[-0.0832, -0.0317,  0.0080,  ...,  0.0650, -0.0346, -0.0338],\n",
      "        [-0.0027, -0.0888, -0.0519,  ..., -0.0379,  0.0615,  0.0149],\n",
      "        [-0.1416, -0.0336,  0.0156,  ...,  0.0427,  0.0694,  0.0308],\n",
      "        ...,\n",
      "        [-0.0182, -0.0285,  0.0117,  ...,  0.0094, -0.0225, -0.0526],\n",
      "        [-0.0159,  0.0288, -0.0066,  ...,  0.0381,  0.0633,  0.0451],\n",
      "        [ 0.0131, -0.0608,  0.0516,  ..., -0.0093,  0.0415, -0.0130]])\n",
      "p.data0 tensor([ 0.3523,  0.5483, -0.9828, -0.2779, -0.1628,  0.2864,  0.4416, -0.1606,\n",
      "        -0.0483,  0.3558,  0.0197, -0.4532,  0.2846,  0.2724, -0.0176, -0.3266,\n",
      "        -0.3908,  0.1753, -0.5212,  0.1862,  0.1354, -0.1771, -0.6206,  0.2103,\n",
      "         0.4602,  0.2141, -0.3035, -0.1616, -0.0902, -0.2189,  0.0902,  0.4726])\n",
      "p0.data0 tensor([ 0.3517,  0.5483, -0.9828, -0.2779, -0.1626,  0.2863,  0.4415, -0.1606,\n",
      "        -0.0482,  0.3560,  0.0195, -0.4532,  0.2849,  0.2726, -0.0176, -0.3265,\n",
      "        -0.3910,  0.1756, -0.5212,  0.1861,  0.1358, -0.1771, -0.6210,  0.2102,\n",
      "         0.4602,  0.2138, -0.3035, -0.1617, -0.0902, -0.2190,  0.0900,  0.4723])\n",
      "p.data0 tensor([[-0.5000, -0.3251, -0.3365, -0.1788,  0.4894,  0.2327, -0.0765, -0.0404,\n",
      "          0.2216,  0.3087, -0.0066, -0.1394, -0.2659, -0.0909, -0.0340, -0.2131,\n",
      "          0.3019,  0.4025,  0.2239, -0.8721,  0.3750,  0.1391,  0.2175, -0.0609,\n",
      "          0.0107, -0.1616,  0.4162, -0.2156,  0.2203, -0.0159,  0.0403,  0.1495],\n",
      "        [ 0.7464, -0.0514, -0.1759, -0.5607,  0.3197, -0.5390,  0.2760, -0.2360,\n",
      "         -0.0792,  0.1164,  0.0944,  0.2060, -0.1709, -0.1799,  0.2492, -0.1172,\n",
      "         -0.2856, -0.1900,  0.1498,  0.0698, -0.1845, -0.1089,  0.0879, -0.2805,\n",
      "         -0.1899, -0.2201, -0.1493, -0.6315,  0.1065, -0.1849,  0.2712,  0.0030],\n",
      "        [ 0.4544, -0.4825,  0.0086,  0.0596,  0.1640,  0.5571, -0.0252,  0.5836,\n",
      "         -0.1507, -0.2913, -0.1849,  0.0267, -0.2307, -0.2046,  0.4391, -0.4908,\n",
      "          0.2097, -0.1443,  0.2358,  0.0762, -0.5024, -0.0404,  0.2732, -0.3896,\n",
      "          0.0253,  0.1157,  0.3603,  0.1169, -0.0619,  0.4673,  0.0564, -0.2486],\n",
      "        [-0.3006, -0.1324, -0.1534, -0.0812, -0.3729,  0.0195,  0.3679,  0.2771,\n",
      "         -0.0744,  0.3458,  0.1851,  0.2920, -0.0663, -0.1341,  0.5953,  0.5827,\n",
      "         -0.0176,  0.1400, -0.1318, -0.1937,  0.1863, -0.2186,  0.0059, -0.4966,\n",
      "          0.3096, -0.1462, -0.1744,  0.2239, -0.0505, -0.1025, -0.2681, -0.1763],\n",
      "        [ 0.3478,  0.2201, -0.0120,  0.2485, -0.3640,  0.0918, -0.4404, -0.4285,\n",
      "          0.1242, -0.4168,  0.0110, -0.2588, -0.0998,  0.1629,  0.3328,  0.2110,\n",
      "          0.5901,  0.1152, -0.1442,  0.1331,  0.3748, -0.3174,  0.0736,  0.2367,\n",
      "         -0.1039,  0.6116, -0.4349,  0.2274, -0.2250, -0.5619,  0.2514, -0.4256],\n",
      "        [ 0.0725,  0.0014, -0.2757, -0.0888, -0.2022, -0.1422, -0.0222, -0.1010,\n",
      "          0.5663,  0.1802, -0.0546,  0.2493, -0.2337,  0.0492, -0.0831,  0.3853,\n",
      "          0.0565, -0.2891, -0.0779, -0.4092, -0.4267, -0.1293,  0.1621,  0.1448,\n",
      "          0.4808,  0.1886, -0.0021,  0.3639,  0.1127,  0.4449,  0.1943,  0.1405],\n",
      "        [-0.3716,  0.3318, -0.1809,  0.2978,  0.1655,  0.1727, -0.1718,  0.0568,\n",
      "         -0.0833, -0.5383,  0.0047, -0.1965, -0.5115,  0.1941, -0.5990, -0.1176,\n",
      "          0.1238, -0.2298,  0.0037,  0.2974,  0.4185,  0.0278, -0.1018, -0.0543,\n",
      "          0.4831, -0.0544, -0.0487, -0.2401, -0.2051, -0.1922, -0.2380,  0.1000],\n",
      "        [-0.2915, -0.1124,  0.0688, -0.3937,  0.5025, -0.0323, -0.0609, -0.2232,\n",
      "         -0.0076,  0.3156, -0.0806, -0.5877,  0.4272,  0.1610,  0.0655,  0.0656,\n",
      "         -0.2953,  0.3413, -0.0813,  0.4133, -0.2327,  0.1092,  0.1093, -0.2305,\n",
      "         -0.5675, -0.0468,  0.2133, -0.2092, -0.2803, -0.1559,  0.4501, -0.4131],\n",
      "        [-0.1213,  0.1674, -0.1994,  0.2401, -0.0445,  0.1686,  0.2911,  0.5145,\n",
      "         -0.0625, -0.2171, -0.0649,  0.2979, -0.1322,  0.1573, -0.0600,  0.4469,\n",
      "         -0.3149,  0.0086,  0.2262, -0.4450, -0.0278, -0.0134, -0.1913,  0.3128,\n",
      "         -0.3330,  0.3732, -0.0839, -0.4113,  0.0244,  0.0104, -0.5322,  0.5026],\n",
      "        [ 0.4939, -0.0191, -0.3707,  0.7042, -0.2629, -0.1233, -0.2409, -0.0793,\n",
      "         -0.1820,  0.0730, -0.2653,  0.4545, -0.0126,  0.8016, -0.4608,  0.1545,\n",
      "         -0.4601,  0.4027, -0.0822, -0.1493, -0.0139,  0.1346,  0.1790,  0.1990,\n",
      "         -0.0927, -0.1192, -0.3255, -0.1269,  0.3737, -0.0261, -0.4533, -0.2376]])\n",
      "p0.data0 tensor([[-0.5000, -0.3253, -0.3365, -0.1791,  0.4895,  0.2327, -0.0760, -0.0403,\n",
      "          0.2216,  0.3087, -0.0066, -0.1394, -0.2660, -0.0908, -0.0335, -0.2133,\n",
      "          0.3019,  0.4020,  0.2239, -0.8718,  0.3747,  0.1391,  0.2168, -0.0608,\n",
      "          0.0111, -0.1618,  0.4165, -0.2155,  0.2202, -0.0158,  0.0403,  0.1499],\n",
      "        [ 0.7462, -0.0515, -0.1759, -0.5609,  0.3207, -0.5386,  0.2755, -0.2358,\n",
      "         -0.0793,  0.1160,  0.0944,  0.2060, -0.1715, -0.1801,  0.2489, -0.1168,\n",
      "         -0.2853, -0.1898,  0.1498,  0.0695, -0.1836, -0.1089,  0.0880, -0.2806,\n",
      "         -0.1899, -0.2202, -0.1493, -0.6313,  0.1066, -0.1847,  0.2709,  0.0025],\n",
      "        [ 0.4539, -0.4824,  0.0086,  0.0599,  0.1639,  0.5566, -0.0253,  0.5831,\n",
      "         -0.1506, -0.2910, -0.1849,  0.0267, -0.2303, -0.2045,  0.4393, -0.4904,\n",
      "          0.2103, -0.1444,  0.2358,  0.0760, -0.5025, -0.0404,  0.2734, -0.3899,\n",
      "          0.0256,  0.1158,  0.3594,  0.1170, -0.0621,  0.4668,  0.0561, -0.2484],\n",
      "        [-0.3006, -0.1323, -0.1534, -0.0810, -0.3734,  0.0197,  0.3679,  0.2770,\n",
      "         -0.0744,  0.3458,  0.1852,  0.2920, -0.0663, -0.1341,  0.5952,  0.5829,\n",
      "         -0.0175,  0.1395, -0.1318, -0.1937,  0.1862, -0.2186,  0.0063, -0.4970,\n",
      "          0.3090, -0.1458, -0.1747,  0.2239, -0.0503, -0.1026, -0.2676, -0.1760],\n",
      "        [ 0.3476,  0.2199, -0.0120,  0.2484, -0.3642,  0.0919, -0.4400, -0.4282,\n",
      "          0.1247, -0.4165,  0.0110, -0.2588, -0.0996,  0.1631,  0.3332,  0.2112,\n",
      "          0.5898,  0.1150, -0.1442,  0.1335,  0.3742, -0.3174,  0.0738,  0.2364,\n",
      "         -0.1038,  0.6118, -0.4349,  0.2275, -0.2253, -0.5621,  0.2510, -0.4254],\n",
      "        [ 0.0733,  0.0010, -0.2757, -0.0893, -0.2022, -0.1422, -0.0225, -0.1006,\n",
      "          0.5653,  0.1800, -0.0546,  0.2493, -0.2336,  0.0492, -0.0832,  0.3848,\n",
      "          0.0561, -0.2888, -0.0779, -0.4093, -0.4267, -0.1293,  0.1617,  0.1445,\n",
      "          0.4806,  0.1882, -0.0013,  0.3638,  0.1127,  0.4452,  0.1941,  0.1396],\n",
      "        [-0.3714,  0.3320, -0.1809,  0.2979,  0.1662,  0.1730, -0.1718,  0.0565,\n",
      "         -0.0832, -0.5384,  0.0046, -0.1965, -0.5121,  0.1935, -0.5993, -0.1176,\n",
      "          0.1238, -0.2300,  0.0037,  0.2972,  0.4192,  0.0278, -0.1016, -0.0542,\n",
      "          0.4837, -0.0543, -0.0490, -0.2401, -0.2050, -0.1923, -0.2377,  0.1002],\n",
      "        [-0.2915, -0.1122,  0.0688, -0.3938,  0.5024, -0.0324, -0.0610, -0.2235,\n",
      "         -0.0079,  0.3158, -0.0806, -0.5877,  0.4277,  0.1613,  0.0653,  0.0650,\n",
      "         -0.2954,  0.3416, -0.0813,  0.4135, -0.2330,  0.1092,  0.1094, -0.2305,\n",
      "         -0.5677, -0.0468,  0.2137, -0.2093, -0.2802, -0.1559,  0.4503, -0.4132],\n",
      "        [-0.1214,  0.1674, -0.1994,  0.2402, -0.0446,  0.1687,  0.2913,  0.5151,\n",
      "         -0.0618, -0.2175, -0.0649,  0.2979, -0.1324,  0.1569, -0.0596,  0.4471,\n",
      "         -0.3148,  0.0091,  0.2262, -0.4451, -0.0280, -0.0134, -0.1912,  0.3131,\n",
      "         -0.3335,  0.3734, -0.0839, -0.4113,  0.0242,  0.0106, -0.5319,  0.5027],\n",
      "        [ 0.4938, -0.0190, -0.3707,  0.7045, -0.2636, -0.1236, -0.2410, -0.0795,\n",
      "         -0.1821,  0.0732, -0.2653,  0.4545, -0.0123,  0.8021, -0.4612,  0.1545,\n",
      "         -0.4604,  0.4029, -0.0822, -0.1496, -0.0140,  0.1346,  0.1788,  0.1995,\n",
      "         -0.0926, -0.1194, -0.3256, -0.1271,  0.3740, -0.0260, -0.4534, -0.2376]])\n",
      "p.data0 tensor([ 0.2353,  0.7190,  0.5154,  0.0759,  0.2476, -0.0129,  0.2155,  0.3334,\n",
      "        -0.1101, -0.5481])\n",
      "p0.data0 tensor([ 0.2353,  0.7188,  0.5152,  0.0761,  0.2476, -0.0130,  0.2156,  0.3336,\n",
      "        -0.1102, -0.5480])\n",
      "Distance: 0.01723946476033257\n",
      "Normalized Distance: 0.0017215296878199434\n"
     ]
    }
   ],
   "source": [
    "log_dict['dist_Original_Retrain']=distance(model,model0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance of w(D) from initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntk_init(resume,seed=1):\n",
    "    manual_seed(seed)\n",
    "    model_init = models.get_model(arch, num_classes=num_classes, filters_percentage=filters).to(args.device)\n",
    "    model_init.load_state_dict(torch.load(resume))\n",
    "    return model_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "for p in model_init.parameters():\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.named_parameters <bound method Module.named_parameters of MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): StandardLinearLayer(in_features=1024, out_features=32, bias=True, beta=0.31622776601683794)\n",
      "    (1): ReLU()\n",
      "    (2): StandardLinearLayer(in_features=32, out_features=10, bias=True, beta=0.31622776601683794)\n",
      "  )\n",
      ")>\n",
      "model0.named_parameters <bound method Module.named_parameters of MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): StandardLinearLayer(in_features=1024, out_features=32, bias=True, beta=0.31622776601683794)\n",
      "    (1): ReLU()\n",
      "    (2): StandardLinearLayer(in_features=32, out_features=10, bias=True, beta=0.31622776601683794)\n",
      "  )\n",
      ")>\n",
      "For Loop\n",
      "p.data0 tensor([[-0.0832, -0.0317,  0.0080,  ...,  0.0650, -0.0346, -0.0338],\n",
      "        [-0.0027, -0.0888, -0.0519,  ..., -0.0379,  0.0615,  0.0149],\n",
      "        [-0.1416, -0.0336,  0.0156,  ...,  0.0427,  0.0694,  0.0308],\n",
      "        ...,\n",
      "        [-0.0182, -0.0285,  0.0117,  ...,  0.0094, -0.0225, -0.0526],\n",
      "        [-0.0159,  0.0288, -0.0066,  ...,  0.0381,  0.0633,  0.0451],\n",
      "        [ 0.0131, -0.0608,  0.0516,  ..., -0.0093,  0.0415, -0.0130]])\n",
      "p0.data0 tensor([[-0.0832, -0.0317,  0.0080,  ...,  0.0650, -0.0346, -0.0338],\n",
      "        [-0.0027, -0.0888, -0.0519,  ..., -0.0379,  0.0615,  0.0149],\n",
      "        [-0.1416, -0.0336,  0.0156,  ...,  0.0427,  0.0694,  0.0308],\n",
      "        ...,\n",
      "        [-0.0182, -0.0285,  0.0117,  ...,  0.0094, -0.0225, -0.0526],\n",
      "        [-0.0159,  0.0288, -0.0066,  ...,  0.0381,  0.0633,  0.0451],\n",
      "        [ 0.0131, -0.0608,  0.0516,  ..., -0.0093,  0.0415, -0.0130]])\n",
      "p.data0 tensor([ 0.2570,  0.5360, -0.9828, -0.2736, -0.2282,  0.3082,  0.4359, -0.1057,\n",
      "        -0.0857,  0.2738,  0.0112, -0.4476,  0.2455,  0.2458, -0.0411, -0.3458,\n",
      "        -0.4463,  0.1637, -0.5212,  0.0843,  0.1513, -0.1780, -0.6362,  0.2063,\n",
      "         0.4134,  0.2404, -0.3497, -0.2096, -0.0949, -0.2470,  0.0197,  0.4874])\n",
      "p0.data0 tensor([ 0.3523,  0.5483, -0.9828, -0.2779, -0.1628,  0.2864,  0.4416, -0.1606,\n",
      "        -0.0483,  0.3558,  0.0197, -0.4532,  0.2846,  0.2724, -0.0176, -0.3266,\n",
      "        -0.3908,  0.1753, -0.5212,  0.1862,  0.1354, -0.1771, -0.6206,  0.2103,\n",
      "         0.4602,  0.2141, -0.3035, -0.1616, -0.0902, -0.2189,  0.0902,  0.4726])\n",
      "p.data0 tensor([[-0.3519, -0.2154, -0.3365, -0.1412,  0.4156,  0.1818,  0.0200, -0.0604,\n",
      "          0.2205,  0.2322, -0.0053, -0.1393, -0.2035, -0.0579,  0.0095, -0.1644,\n",
      "          0.2235,  0.3024,  0.2238, -0.7361,  0.2985,  0.1393,  0.1623, -0.0921,\n",
      "         -0.0720, -0.1394,  0.2537, -0.1786,  0.1402, -0.0607,  0.0045,  0.0832],\n",
      "        [ 0.3928, -0.1737, -0.1759, -0.4551,  0.2481, -0.4473, -0.0411, -0.2373,\n",
      "         -0.0809,  0.0645,  0.0849,  0.2062, -0.1421, -0.0616,  0.1801, -0.0840,\n",
      "         -0.2335, -0.0692,  0.1498, -0.0470, -0.1586, -0.1081,  0.0933, -0.2024,\n",
      "         -0.1436, -0.1707, -0.0688, -0.5912,  0.1219, -0.1256,  0.2843, -0.1910],\n",
      "        [ 0.4523, -0.3588,  0.0086,  0.0764,  0.1664,  0.4396,  0.0665,  0.4759,\n",
      "         -0.1288, -0.1356, -0.1838,  0.0269, -0.1764, -0.1239,  0.3496, -0.3435,\n",
      "          0.2170, -0.0702,  0.2358,  0.0587, -0.4519, -0.0437,  0.2893, -0.2968,\n",
      "          0.0510,  0.1216,  0.3035,  0.1648, -0.0432,  0.4584,  0.1138, -0.1609],\n",
      "        [-0.2260, -0.0343, -0.1534, -0.0083, -0.2569,  0.0590,  0.2407,  0.2322,\n",
      "         -0.0340,  0.2132,  0.1880,  0.2921, -0.0644, -0.0330,  0.4186,  0.4952,\n",
      "          0.0323,  0.1553, -0.1318, -0.1697,  0.2046, -0.2184,  0.0321, -0.3517,\n",
      "          0.3038, -0.0949, -0.1500,  0.1053, -0.0209, -0.1221, -0.1718, -0.0792],\n",
      "        [ 0.3664,  0.1804, -0.0120,  0.2229, -0.2088,  0.1287, -0.2451, -0.3353,\n",
      "          0.0897, -0.2442,  0.0131, -0.2590, -0.1019,  0.1473,  0.3632,  0.2329,\n",
      "          0.5305,  0.1337, -0.1440,  0.1620,  0.3382, -0.3171,  0.0806,  0.1881,\n",
      "          0.0109,  0.5281, -0.3445,  0.2525, -0.2200, -0.4710,  0.2022, -0.2743],\n",
      "        [ 0.1823, -0.0562, -0.2757, -0.0546, -0.1238, -0.1061,  0.0172, -0.0347,\n",
      "          0.4565,  0.1192, -0.0601,  0.2493, -0.2201,  0.0361,  0.0090,  0.2869,\n",
      "          0.0090, -0.2832, -0.0779, -0.2768, -0.3961, -0.1289,  0.1251, -0.0015,\n",
      "          0.2930,  0.1101,  0.0197,  0.2491,  0.0698,  0.3177,  0.0416,  0.0137],\n",
      "        [-0.3832,  0.2309, -0.1809,  0.1982,  0.0751,  0.0873, -0.1564,  0.0144,\n",
      "         -0.0449, -0.3521,  0.0023, -0.1973, -0.3833,  0.2016, -0.5147, -0.0191,\n",
      "          0.0122, -0.0809,  0.0038,  0.1643,  0.2994,  0.0281, -0.0899, -0.0444,\n",
      "          0.2220,  0.0072, -0.0604, -0.2090, -0.1963, -0.1330, -0.2076,  0.0046],\n",
      "        [-0.2165, -0.1100,  0.0688, -0.3747,  0.4035, -0.0398, -0.0595, -0.1467,\n",
      "         -0.0216,  0.1590, -0.0791, -0.5878,  0.2140,  0.0116,  0.0842,  0.0181,\n",
      "         -0.2411,  0.1744, -0.0815,  0.3069, -0.1610,  0.1093,  0.1149, -0.2111,\n",
      "         -0.4566, -0.0443,  0.1989, -0.1942, -0.2622, -0.1263,  0.3670, -0.2812],\n",
      "        [-0.1144,  0.1658, -0.1994,  0.2232, -0.0683,  0.2219,  0.2655,  0.4258,\n",
      "         -0.0326, -0.1462, -0.0584,  0.2980, -0.0663,  0.1896, -0.0486,  0.3528,\n",
      "         -0.2598,  0.0683,  0.2263, -0.3286,  0.0065, -0.0126, -0.1758,  0.2649,\n",
      "         -0.1740,  0.2994, -0.0692, -0.3554,  0.0484, -0.0219, -0.4493,  0.4110],\n",
      "        [ 0.4281, -0.0311, -0.3707,  0.5601, -0.2563, -0.1194, -0.2108, -0.0102,\n",
      "         -0.1516, -0.0340, -0.2634,  0.4547, -0.1523,  0.6067, -0.4058,  0.1322,\n",
      "         -0.3815,  0.2264, -0.0823, -0.2132, -0.0130,  0.1347,  0.1836,  0.1278,\n",
      "         -0.0120, -0.0764, -0.3119, -0.1457,  0.3770, -0.0323, -0.4128, -0.1316]])\n",
      "p0.data0 tensor([[-0.5000, -0.3251, -0.3365, -0.1788,  0.4894,  0.2327, -0.0765, -0.0404,\n",
      "          0.2216,  0.3087, -0.0066, -0.1394, -0.2659, -0.0909, -0.0340, -0.2131,\n",
      "          0.3019,  0.4025,  0.2239, -0.8721,  0.3750,  0.1391,  0.2175, -0.0609,\n",
      "          0.0107, -0.1616,  0.4162, -0.2156,  0.2203, -0.0159,  0.0403,  0.1495],\n",
      "        [ 0.7464, -0.0514, -0.1759, -0.5607,  0.3197, -0.5390,  0.2760, -0.2360,\n",
      "         -0.0792,  0.1164,  0.0944,  0.2060, -0.1709, -0.1799,  0.2492, -0.1172,\n",
      "         -0.2856, -0.1900,  0.1498,  0.0698, -0.1845, -0.1089,  0.0879, -0.2805,\n",
      "         -0.1899, -0.2201, -0.1493, -0.6315,  0.1065, -0.1849,  0.2712,  0.0030],\n",
      "        [ 0.4544, -0.4825,  0.0086,  0.0596,  0.1640,  0.5571, -0.0252,  0.5836,\n",
      "         -0.1507, -0.2913, -0.1849,  0.0267, -0.2307, -0.2046,  0.4391, -0.4908,\n",
      "          0.2097, -0.1443,  0.2358,  0.0762, -0.5024, -0.0404,  0.2732, -0.3896,\n",
      "          0.0253,  0.1157,  0.3603,  0.1169, -0.0619,  0.4673,  0.0564, -0.2486],\n",
      "        [-0.3006, -0.1324, -0.1534, -0.0812, -0.3729,  0.0195,  0.3679,  0.2771,\n",
      "         -0.0744,  0.3458,  0.1851,  0.2920, -0.0663, -0.1341,  0.5953,  0.5827,\n",
      "         -0.0176,  0.1400, -0.1318, -0.1937,  0.1863, -0.2186,  0.0059, -0.4966,\n",
      "          0.3096, -0.1462, -0.1744,  0.2239, -0.0505, -0.1025, -0.2681, -0.1763],\n",
      "        [ 0.3478,  0.2201, -0.0120,  0.2485, -0.3640,  0.0918, -0.4404, -0.4285,\n",
      "          0.1242, -0.4168,  0.0110, -0.2588, -0.0998,  0.1629,  0.3328,  0.2110,\n",
      "          0.5901,  0.1152, -0.1442,  0.1331,  0.3748, -0.3174,  0.0736,  0.2367,\n",
      "         -0.1039,  0.6116, -0.4349,  0.2274, -0.2250, -0.5619,  0.2514, -0.4256],\n",
      "        [ 0.0725,  0.0014, -0.2757, -0.0888, -0.2022, -0.1422, -0.0222, -0.1010,\n",
      "          0.5663,  0.1802, -0.0546,  0.2493, -0.2337,  0.0492, -0.0831,  0.3853,\n",
      "          0.0565, -0.2891, -0.0779, -0.4092, -0.4267, -0.1293,  0.1621,  0.1448,\n",
      "          0.4808,  0.1886, -0.0021,  0.3639,  0.1127,  0.4449,  0.1943,  0.1405],\n",
      "        [-0.3716,  0.3318, -0.1809,  0.2978,  0.1655,  0.1727, -0.1718,  0.0568,\n",
      "         -0.0833, -0.5383,  0.0047, -0.1965, -0.5115,  0.1941, -0.5990, -0.1176,\n",
      "          0.1238, -0.2298,  0.0037,  0.2974,  0.4185,  0.0278, -0.1018, -0.0543,\n",
      "          0.4831, -0.0544, -0.0487, -0.2401, -0.2051, -0.1922, -0.2380,  0.1000],\n",
      "        [-0.2915, -0.1124,  0.0688, -0.3937,  0.5025, -0.0323, -0.0609, -0.2232,\n",
      "         -0.0076,  0.3156, -0.0806, -0.5877,  0.4272,  0.1610,  0.0655,  0.0656,\n",
      "         -0.2953,  0.3413, -0.0813,  0.4133, -0.2327,  0.1092,  0.1093, -0.2305,\n",
      "         -0.5675, -0.0468,  0.2133, -0.2092, -0.2803, -0.1559,  0.4501, -0.4131],\n",
      "        [-0.1213,  0.1674, -0.1994,  0.2401, -0.0445,  0.1686,  0.2911,  0.5145,\n",
      "         -0.0625, -0.2171, -0.0649,  0.2979, -0.1322,  0.1573, -0.0600,  0.4469,\n",
      "         -0.3149,  0.0086,  0.2262, -0.4450, -0.0278, -0.0134, -0.1913,  0.3128,\n",
      "         -0.3330,  0.3732, -0.0839, -0.4113,  0.0244,  0.0104, -0.5322,  0.5026],\n",
      "        [ 0.4939, -0.0191, -0.3707,  0.7042, -0.2629, -0.1233, -0.2409, -0.0793,\n",
      "         -0.1820,  0.0730, -0.2653,  0.4545, -0.0126,  0.8016, -0.4608,  0.1545,\n",
      "         -0.4601,  0.4027, -0.0822, -0.1493, -0.0139,  0.1346,  0.1790,  0.1990,\n",
      "         -0.0927, -0.1192, -0.3255, -0.1269,  0.3737, -0.0261, -0.4533, -0.2376]])\n",
      "p.data0 tensor([ 0.2496,  0.6326,  0.5569,  0.1073,  0.2888, -0.0639,  0.1895,  0.2966,\n",
      "        -0.0117, -0.5746])\n",
      "p0.data0 tensor([ 0.2353,  0.7190,  0.5154,  0.0759,  0.2476, -0.0129,  0.2155,  0.3334,\n",
      "        -0.1101, -0.5481])\n",
      "Distance: 2.838469428891656\n",
      "Normalized Distance: 0.30469125188309615\n"
     ]
    }
   ],
   "source": [
    "log_dict['dist_Original_Original_init']=distance(model_init,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.retain_bs = 32\n",
    "args.forget_bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confuse mode: False\n",
      "split mode: train\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Replacing indexes [ 8526 16694 15152 29352   141 36509 37843 16926 35321 26042 21345  5714\n",
      " 47547 14437 39793 44980 45137  3897 35780 29999 33655 37420 40980 28992\n",
      "   105 40544 41016 46885 46091 40786  5543  7014 20505 25936 16792  2457\n",
      "  1600 13007  7760 15774 46362 33702 32877  2543 33599 32886 35244 29239\n",
      "  8640 23665 45315  3038 22738  3554  4458 36789 11462  2191 23706 10872\n",
      "  9290 32216 25081 12968 28129 20523  5261 16762 26382 19263 12728 37875\n",
      " 36911  7326 21202 31998 26810 28700 36488 42838  3180 24261 25761 13600\n",
      " 18425 29148 39305  4254  2025  4307 45980 28150 36331  8747 35060 28517\n",
      "  8694 12015 38188 36605 26116 12256 18607 11847  1442 19196 18989 33194\n",
      " 26487  5803   828 40152 11342 43196 20715 41802 14043  8865 20144 41373\n",
      " 47285  8202 21858  1815 22335  2739 16404  6882  9838 14297  4773 29745\n",
      "  8784 20420  9033 18865 39460 23585 14421 25902 31362 17568 47282 10213\n",
      " 10851 27398  9097 13317  3946 12763 22347 26179 27559 12416 12204 19720\n",
      " 28625 43760   991 18748  3274 11284 33410 22005  3957 23606 30636 39992\n",
      "  1264 25266 17560 31423 46872  6399  8159 35119  1818 44722 35781 44342\n",
      " 25267   634 46251  4819 19794 29351 33480 27853 45953 19391  8342 23363\n",
      " 10898 22639 11929 33139 20729 23386 14207 42742  6533 15472 42530 19571\n",
      " 22377 37233 37954 28603  5153 40736 28158 13824 24596 47089  4148 32854\n",
      " 44602 27694 42719 24075  6046 21712  7058  1106 33206 27058 40874 12132\n",
      " 27016 21465 25892  8743  8810 33435 33167 23515 47454 24149 46878 44043\n",
      " 12757 41548 24280 38149  3902 38656 29473  9135 19714 31691 38719 42099\n",
      " 41461  6753 39036 38931 19219 20760  6409 44396  3192 30070 21387 10447\n",
      " 47641  5579 43237 20422 15418 29575 24229 42701  5380  5442 12690 44635\n",
      " 35380 24691 28596 36012 38802  8417 18947 17449 14339 25511  4570 22447\n",
      " 12060 46231 41953 29310 37274 10157  1795 18569 36490 23796 29340 43197]\n"
     ]
    }
   ],
   "source": [
    "train_loader_full, valid_loader_full, test_loader_full   = datasets.get_loaders(dataset, batch_size=args.batch_size, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "marked_loader, _, _ = datasets.get_loaders(dataset, class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "\n",
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "    manual_seed(seed)\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "    def _init_fn(worker_id):\n",
    "        np.random.seed(int(seed))\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)\n",
    "    \n",
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = forget_dataset.targets < 0\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, batch_size=args.forget_bs, seed=seed, shuffle=True)\n",
    "\n",
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = retain_dataset.targets >= 0\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, batch_size=args.retain_bs, seed=seed, shuffle=True)\n",
    "\n",
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(forget_loader.dataset))\n",
    "print (len(retain_loader.dataset))\n",
    "print (len(test_loader_full.dataset))\n",
    "print (len(train_loader_full.dataset))\n",
    "from collections import Counter\n",
    "print(dict(Counter(train_loader_full.dataset.targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.optim = 'adam'\n",
    "args.gamma = 1\n",
    "args.alpha = 0.5\n",
    "args.beta = 0\n",
    "args.smoothing = 0.5\n",
    "args.msteps = 3\n",
    "args.clip = 0.2\n",
    "args.sstart = 10\n",
    "args.kd_T = 2\n",
    "args.distill = 'kd'\n",
    "\n",
    "args.sgda_epochs = 10\n",
    "args.sgda_learning_rate = 0.0005\n",
    "args.lr_decay_epochs = [5,8,9]\n",
    "args.lr_decay_rate = 0.1\n",
    "args.sgda_weight_decay = 0.1#5e-4\n",
    "args.sgda_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = copy.deepcopy(teacher)\n",
    "model_s = copy.deepcopy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is from https://github.com/ojus1/SmoothedGradientDescentAscent/blob/main/SGDA.py\n",
    "#For SGDA smoothing\n",
    "beta = 0.1\n",
    "def avg_fn(averaged_model_parameter, model_parameter, num_averaged): return (\n",
    "    1 - beta) * averaged_model_parameter + beta * model_parameter\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(\n",
    "    model_s, avg_fn=avg_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = nn.ModuleList([])\n",
    "module_list.append(model_s)\n",
    "trainable_list = nn.ModuleList([])\n",
    "trainable_list.append(model_s)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_div = DistillKL(args.kd_T)\n",
    "criterion_kd = DistillKL(args.kd_T)\n",
    "\n",
    "\n",
    "criterion_list = nn.ModuleList([])\n",
    "criterion_list.append(criterion_cls)    # classification loss\n",
    "criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
    "criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "\n",
    "# optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = optim.SGD(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"adam\": \n",
    "    optimizer = optim.Adam(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"rmsp\":\n",
    "    optimizer = optim.RMSprop(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list.append(model_t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    module_list.cuda()\n",
    "    criterion_list.cuda()\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True\n",
    "    swa_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_rs = []\n",
    "acc_fs = []\n",
    "acc_ts = []\n",
    "for epoch in range(1, args.sgda_epochs + 1):\n",
    "\n",
    "    lr = sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "\n",
    "    print(\"==> scrub unlearning ...\")\n",
    "\n",
    "    acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True)\n",
    "    acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True)\n",
    "    acc_rs.append(100-acc_r.item())\n",
    "    acc_fs.append(100-acc_f.item())\n",
    "\n",
    "    maximize_loss = 0\n",
    "    if epoch <= args.msteps:\n",
    "        maximize_loss = train_distill(epoch, forget_loader, module_list, swa_model, criterion_list, optimizer, args, \"maximize\")\n",
    "    train_acc, train_loss = train_distill(epoch, retain_loader, module_list, swa_model, criterion_list, optimizer, args, \"minimize\",)\n",
    "    if epoch >= args.sstart:\n",
    "        swa_model.update_parameters(model_s)\n",
    "\n",
    "    print (\"maximize loss: {:.2f}\\t minimize loss: {:.2f}\\t train_acc: {}\".format(maximize_loss, train_loss, train_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "indices = list(range(0,len(acc_rs)))\n",
    "plt.plot(indices, acc_rs, marker='*', alpha=1, label='retain-set')\n",
    "plt.plot(indices, acc_fs, marker='o', alpha=1, label='forget-set')\n",
    "plt.legend(prop={'size': 14})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.title('scrub retain- and forget- set error',size=18)\n",
    "plt.xlabel('epoch',size=14)\n",
    "plt.ylabel('error',size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTK based Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NTK Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_w_utils(model_init,dataloader,name='complete'):\n",
    "    model_init.eval()\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    G_list = []\n",
    "    f0_minus_y = []\n",
    "    for idx, batch in enumerate(dataloader):#(tqdm(dataloader,leave=False)):\n",
    "        print(\"One iteration:\", time.time())\n",
    "        batch = [tensor.to(next(model_init.parameters()).device) for tensor in batch]\n",
    "        input, target = batch\n",
    "        if 'mnist' in args.dataset:\n",
    "            input = input.view(input.shape[0],-1)\n",
    "        target = target.cpu().detach().numpy()\n",
    "        output = model_init(input)\n",
    "        G_sample=[]\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output[0,cls],model_init.parameters(),retain_graph=True)\n",
    "            grads = np.concatenate([g.view(-1).cpu().numpy() for g in grads])\n",
    "            G_sample.append(grads)\n",
    "            G_list.append(grads)\n",
    "        if args.lossfn=='mse':\n",
    "            p = output.cpu().detach().numpy().transpose()\n",
    "            #loss_hess = np.eye(len(p))\n",
    "            target = 2*target-1\n",
    "            f0_y_update = p-target\n",
    "        elif args.lossfn=='ce':\n",
    "            p = torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().transpose()\n",
    "            p[target]-=1\n",
    "            f0_y_update = copy.deepcopy(p)\n",
    "        f0_minus_y.append(f0_y_update)\n",
    "    return np.stack(G_list).transpose(),np.vstack(f0_minus_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobians and Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntk_time = 0\n",
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "t1 = time.time()\n",
    "G_r,f0_minus_y_r = delta_w_utils(copy.deepcopy(model),retain_loader,'complete')\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/G_r.npy',G_r)\n",
    "np.save('NTK_data/f0_minus_y_r.npy',f0_minus_y_r)\n",
    "del G_r, f0_minus_y_r\n",
    "\n",
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "t1 = time.time()\n",
    "G_f,f0_minus_y_f = delta_w_utils(copy.deepcopy(model),forget_loader,'retain') \n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/G_f.npy',G_f)\n",
    "np.save('NTK_data/f0_minus_y_f.npy',f0_minus_y_f)\n",
    "del G_f, f0_minus_y_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "G_f = np.load('NTK_data/G_f.npy')\n",
    "G = np.concatenate([G_r,G_f],axis=1)\n",
    "\n",
    "np.save('NTK_data/G.npy',G)\n",
    "del G, G_f, G_r\n",
    "\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "f0_minus_y_f = np.load('NTK_data/f0_minus_y_f.npy')\n",
    "f0_minus_y = np.concatenate([f0_minus_y_r,f0_minus_y_f])\n",
    "\n",
    "np.save('NTK_data/f0_minus_y.npy',f0_minus_y)\n",
    "del f0_minus_y, f0_minus_y_r, f0_minus_y_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only requires access to the gradients and the initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w_lin(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.load('NTK_data/G.npy')\n",
    "t1 = time.time()\n",
    "theta = G.transpose().dot(G) + num_total*args.weight_decay*np.eye(G.shape[1])\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "del G\n",
    "\n",
    "t1 = time.time()\n",
    "theta_inv = np.linalg.inv(theta)\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta.npy',theta)\n",
    "del theta\n",
    "\n",
    "G = np.load('NTK_data/G.npy')\n",
    "f0_minus_y = np.load('NTK_data/f0_minus_y.npy')\n",
    "t1 = time.time()\n",
    "w_complete = -G.dot(theta_inv.dot(f0_minus_y))\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_inv.npy',theta_inv)\n",
    "np.save('NTK_data/w_complete.npy',w_complete)\n",
    "\n",
    "del G, f0_minus_y, theta_inv, w_complete "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w_lin(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "t1 = time.time()\n",
    "theta_r = G_r.transpose().dot(G_r) + num_to_retain*args.weight_decay*np.eye(G_r.shape[1])\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "del G_r\n",
    "\n",
    "t1 = time.time()\n",
    "theta_r_inv = np.linalg.inv(theta_r)\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_r.npy',theta_r)\n",
    "del theta_r\n",
    "\n",
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "t1 = time.time()\n",
    "w_retain = -G_r.dot(theta_r_inv.dot(f0_minus_y_r))\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_r_inv.npy',theta_r_inv)\n",
    "np.save('NTK_data/w_retain.npy',w_retain)\n",
    "\n",
    "del G_r, f0_minus_y_r, theta_r_inv, w_retain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrubbing Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Scrubbing Direction\n",
    "w_complete = np.load('NTK_data/w_complete.npy')\n",
    "w_retain = np.load('NTK_data/w_retain.npy')\n",
    "delta_w = (w_retain-w_complete).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w_copy = copy.deepcopy(delta_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual Change in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w_actual = vectorize_params(model0)-vectorize_params(model)\n",
    "\n",
    "print(f'Actual Norm-: {np.linalg.norm(delta_w_actual)}')\n",
    "print(f'Predtn Norm-: {np.linalg.norm(delta_w)}')\n",
    "scale_ratio = np.linalg.norm(delta_w_actual)/np.linalg.norm(delta_w)\n",
    "print('Actual Scale: {}'.format(scale_ratio))\n",
    "log_dict['actual_scale_ratio']=scale_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trapezium Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pred_error = vectorize_params(model)-vectorize_params(model_init)-w_retain.squeeze()\n",
    "print(f\"Delta w -------: {np.linalg.norm(delta_w)}\")\n",
    "\n",
    "inner = np.inner(delta_w/np.linalg.norm(delta_w),m_pred_error/np.linalg.norm(m_pred_error))\n",
    "print(f\"Inner Product--: {inner}\")\n",
    "\n",
    "if inner<0:\n",
    "    angle = np.arccos(inner)-np.pi/2\n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.sin(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "else:\n",
    "    angle = np.arccos(inner) \n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.cos(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "\n",
    "predicted_scale=predicted_norm/np.linalg.norm(delta_w)\n",
    "predicted_scale\n",
    "print(f\"Predicted Scale:  {predicted_scale}\")\n",
    "log_dict['predicted_scale_ratio']=predicted_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized Inner Product between Prediction and Actual Scrubbing Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NIP(v1,v2):\n",
    "    nip = (np.inner(v1/np.linalg.norm(v1),v2/np.linalg.norm(v2)))\n",
    "    print(nip)\n",
    "    return nip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nip=NIP(delta_w_actual,delta_w)\n",
    "log_dict['nip']=nip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape delta_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_w_dict(delta_w,model):\n",
    "    # Give normalized delta_w\n",
    "    delta_w_dict = OrderedDict()\n",
    "    params_visited = 0\n",
    "    for k,p in model.named_parameters():\n",
    "        num_params = np.prod(list(p.shape))\n",
    "        update_params = delta_w[params_visited:params_visited+num_params]\n",
    "        delta_w_dict[k] = torch.Tensor(update_params).view_as(p)\n",
    "        params_visited+=num_params\n",
    "    return delta_w_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_predictions(model,dataloader,name):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metrics,activations,predictions=get_metrics(model,dataloader,criterion,True)\n",
    "    print(f\"{name} -> Loss:{np.round(metrics['loss'],3)}, Error:{metrics['error']}\")\n",
    "    log_dict[f\"{name}_loss\"]=metrics['loss']\n",
    "    log_dict[f\"{name}_error\"]=metrics['error']\n",
    "\n",
    "    return activations,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_distance(l1,l2,name):\n",
    "    dist = np.sum(np.abs(l1-l2))\n",
    "    print(f\"Predictions Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_predictions\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_distance(a1,a2,name):\n",
    "    dist = np.linalg.norm(a1-a2,ord=1,axis=1).mean()\n",
    "    print(f\"Activations Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_activations\"]=dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrub using NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=predicted_scale\n",
    "direction = get_delta_w_dict(delta_w,model)\n",
    "\n",
    "model_scrub = copy.deepcopy(model)\n",
    "for k,p in model_scrub.named_parameters():\n",
    "    p.data += (direction[k]*scale).to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune and Fisher Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        if scrub_act:\n",
    "            G = []\n",
    "            for cls in range(num_classes):\n",
    "                grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                grads = torch.cat([g.view(-1) for g in grads])\n",
    "                G.append(grads)\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            G = torch.stack(G).pow(2)\n",
    "            delta_f = torch.matmul(G,delta_w)\n",
    "            output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(model,model_init,weight_decay):\n",
    "    l2_loss = 0\n",
    "    for (k,p),(k_init,p_init) in zip(model.named_parameters(),model_init.named_parameters()):\n",
    "        if p.requires_grad:\n",
    "            l2_loss += (p-p_init).pow(2).sum()\n",
    "    l2_loss *= (weight_decay/2.)\n",
    "    return l2_loss\n",
    "\n",
    "def run_train_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    negative_gradient=False, negative_multiplier=-1, random_labels=False,\n",
    "                    quiet=False,delta_w=None,scrub_act=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            if split=='test' and scrub_act:\n",
    "                G = []\n",
    "                for cls in range(num_classes):\n",
    "                    grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                    grads = torch.cat([g.view(-1) for g in grads])\n",
    "                    G.append(grads)\n",
    "                grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "                G = torch.stack(G).pow(2)\n",
    "                delta_f = torch.matmul(G,delta_w)\n",
    "                output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "            \n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg\n",
    "\n",
    "def run_neggrad_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    forget_loader: torch.utils.data.DataLoader,\n",
    "                    alpha: float,\n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    quiet=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, (batch_retain,batch_forget) in enumerate(tqdm(zip(data_loader,cycle(forget_loader)), leave=False)):\n",
    "            batch_retain = [tensor.to(next(model.parameters()).device) for tensor in batch_retain]\n",
    "            batch_forget = [tensor.to(next(model.parameters()).device) for tensor in batch_forget]\n",
    "            input_r, target_r = batch_retain\n",
    "            input_f, target_f = batch_forget\n",
    "            output_r = model(input_r)\n",
    "            output_f = model(input_f)\n",
    "            loss = alpha*(loss_fn(output_r, target_r) + l2_penalty(model,model_init,args.weight_decay)) - (1-alpha)*loss_fn(output_f, target_f)\n",
    "            metrics.update(n=input_r.size(0), loss=loss_fn(output_r,target_r).item(), error=get_error(output_r, target_r))\n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model: nn.Module, data_loader: torch.utils.data.DataLoader, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_vanilla(epoch, data_loader, model, loss_fn, optimizer, args)\n",
    "\n",
    "def negative_grad(model: nn.Module, data_loader: torch.utils.data.DataLoader, forget_loader: torch.utils.data.DataLoader, alpha: float, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        run_neggrad_epoch(model, model_init, data_loader, forget_loader, alpha, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_negrad(epoch, data_loader, forget_loader, model, loss_fn, optimizer,  alpha)\n",
    "\n",
    "def fk_fientune(model: nn.Module, data_loader: torch.utils.data.DataLoader, args, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "        run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_negrad(epoch, data_loader, forget_loader, model, loss_fn, optimizer,  alpha)\n",
    "def test(model, data_loader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model_init=copy.deepcopy(model)\n",
    "    return run_train_epoch(model, model_init, data_loader, loss_fn, optimizer=None, split='test', epoch=epoch, ignore_index=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout_retrain(model, data_loader, test_loader, lr=0.1, epochs=500, threshold=0.01, quiet=True):\n",
    "    torch.manual_seed(seed)\n",
    "    model = copy.deepcopy(model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=True, num_samples=500)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=data_loader.batch_size, sampler=sampler, num_workers=data_loader.num_workers)\n",
    "    metrics = []\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        metrics.append(run_train_epoch(model, model_init, test_loader, loss_fn, optimizer, split='test', epoch=epoch, ignore_index=None, quiet=quiet))\n",
    "        if metrics[-1]['loss'] <= threshold:\n",
    "            break\n",
    "        run_train_epoch(model, model_init, data_loader_small, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "    return epoch, metrics\n",
    "\n",
    "def extract_retrain_time(metrics, threshold=0.1):\n",
    "    losses = np.array([m['loss'] for m in metrics])\n",
    "    return np.argmax(losses < threshold)\n",
    "\n",
    "def all_readouts(model,thresh=0.1,name='method'):\n",
    "    train_loader = torch.utils.data.DataLoader(train_loader_full.dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    retrain_time, _ = readout_retrain(model, train_loader, forget_loader, epochs=100, lr=0.1, threshold=thresh)\n",
    "    test_error = test(model, test_loader_full)['error']\n",
    "    forget_error = test(model, forget_loader)['error']\n",
    "    retain_error = test(model, retain_loader)['error']\n",
    "    print(f\"{name} ->\"\n",
    "          f\"\\tFull test error: {test_error:.2%}\"\n",
    "          f\"\\tForget error: {forget_error:.2%}\\tRetain error: {retain_error:.2%}\"\n",
    "          f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    log_dict[f\"{name}_retrain_time\"]=retrain_time+1\n",
    "    return(dict(test_error=test_error, forget_error=forget_error, retain_error=retain_error, retrain_time=retrain_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scrubf = copy.deepcopy(model_scrub)\n",
    "modelf = copy.deepcopy(model)\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters(), model_scrubf.parameters()):\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(dataset, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc = 0\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(args.device), orig_target.to(args.device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad_acc += (orig_target == target).float() * p.grad.data\n",
    "                    p.grad2_acc += prob[:, y] * p.grad.data.pow(2)\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc /= len(train_loader)\n",
    "        p.grad2_acc /= len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, model_scrubf)\n",
    "hessian(retain_loader.dataset, modelf)\n",
    "hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_var(p, is_base_dist=False, alpha=3e-6):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3)\n",
    "    if p.size(0) == num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var\n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    if not is_base_dist:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    else:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    if p.size(0) == num_classes and num_to_forget is None:\n",
    "        mu[class_to_forget] = 0\n",
    "        var[class_to_forget] = 0.0001\n",
    "    if p.size(0) == num_classes:\n",
    "        # Last layer\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        # BatchNorm\n",
    "        var *= 10\n",
    "#         var*=1\n",
    "    return mu, var\n",
    "\n",
    "def kl_divergence_fisher(mu0, var0, mu1, var1):\n",
    "    return ((mu1 - mu0).pow(2)/var0 + var1/var0 - torch.log(var1/var0) - 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Noise in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computes the amount of information not forgotten at all layers using the given alpha\n",
    "alpha = 1e-7\n",
    "total_kl = 0\n",
    "torch.manual_seed(seed)\n",
    "for (k, p), (k0, p0) in zip(modelf.named_parameters(), modelf0.named_parameters()):\n",
    "    mu0, var0 = get_mean_var(p, False, alpha=alpha)\n",
    "    mu1, var1 = get_mean_var(p0, True, alpha=alpha)\n",
    "    kl = kl_divergence_fisher(mu0, var0, mu1, var1).item()\n",
    "    total_kl += kl\n",
    "    print(k, f'{kl:.1f}')\n",
    "print(\"Total:\", total_kl)\n",
    "log_dict['fisher_info']=total_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_dir = []\n",
    "alpha = 1e-6\n",
    "torch.manual_seed(seed)\n",
    "for i, p in enumerate(modelf.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "    fisher_dir.append(var.sqrt().view(-1).cpu().detach().numpy())\n",
    "\n",
    "for i, p in enumerate(modelf0.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test(modelf, retain_loader))\n",
    "print(test(modelf, forget_loader))\n",
    "print(test(modelf, valid_loader_full))\n",
    "print(test(modelf, test_loader_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = copy.deepcopy(model)\n",
    "retain_loader = replace_loader_dataset(train_loader_full,retain_dataset, seed=seed, batch_size=args.batch_size, shuffle=True)    \n",
    "finetune(model_ft, retain_loader, epochs=10, quiet=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.ng_alpha = 0.95\n",
    "args.ng_epochs = 10\n",
    "args.ng_lr = 0.01\n",
    "model_ng = copy.deepcopy(model)    \n",
    "negative_grad(model_ng, retain_loader, forget_loader, alpha=args.ng_alpha, epochs=args.ng_epochs, quiet=True, lr=args.ng_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catastrophic Forgetting k layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr_decay_epochs = [10,15,20]\n",
    "args.cfk_lr = 0.01\n",
    "args.cfk_epochs = 10\n",
    "\n",
    "model_cfk = copy.deepcopy(model)\n",
    "\n",
    "for param in model_cfk.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "if args.model == 'allcnn':\n",
    "    layers = [9]\n",
    "    for k in layers:\n",
    "        for param in model_cfk.features[k].parameters():\n",
    "            param.requires_grad_(True)\n",
    "    \n",
    "elif args.model == \"resnet\":\n",
    "    for param in model_cfk.layer4.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "fk_fientune(model_cfk, retain_loader, args=args, epochs=args.cfk_epochs, quiet=True, lr=args.cfk_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Unlearning k layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The last block and classifier of resnet-18\n",
    "(layer4): Sequential(\n",
    "    (0): _ResBlock(\n",
    "      (bn1): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv1): Conv2d(102, 204, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "      (bn2): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (shortcut): Sequential(\n",
    "        (0): Conv2d(102, 204, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "      )\n",
    "    )\n",
    "    (1): _ResBlock(\n",
    "      (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv1): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (bn2): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    )\n",
    "  )\n",
    "(linear): Linear(in_features=204, out_features=5, bias=True)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" The last block and classifier of allcnn\n",
    "AllCNN(\n",
    "  (features): Sequential(\n",
    "    ...\n",
    "    (9): Conv(\n",
    "      (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (2): ReLU()\n",
    "    )\n",
    "    (10): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
    "    (11): Flatten()\n",
    "  )\n",
    "  (classifier): Sequential(\n",
    "    (0): Linear(in_features=192, out_features=5, bias=True)\n",
    "  )\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr_decay_epochs = [10,15,20]\n",
    "args.euk_lr = 0.01\n",
    "args.euk_epochs = training_epochs\n",
    "model_euk = copy.deepcopy(model)\n",
    "\n",
    "for param in model_euk.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "if args.model == 'allcnn':\n",
    "    with torch.no_grad():\n",
    "        for k in layers:\n",
    "            for i in range(0,3):\n",
    "                try:\n",
    "                    model_euk.features[k][i].weight.copy_(model_initial.features[k][i].weight)\n",
    "                except:\n",
    "                    print (\"block {}, layer {} does not have weights\".format(k,i))\n",
    "                try:\n",
    "                    model_euk.features[k][i].bias.copy_(model_initial.features[k][i].bias)\n",
    "                except:\n",
    "                    print (\"block {}, layer {} does not have bias\".format(k,i))\n",
    "        model_euk.classifier[0].weight.copy_(model_initial.classifier[0].weight)\n",
    "        model_euk.classifier[0].bias.copy_(model_initial.classifier[0].bias)\n",
    "    \n",
    "    for k in layers:\n",
    "        for param in model_euk.features[k].parameters():\n",
    "            param.requires_grad_(True)\n",
    "    \n",
    "elif args.model == \"resnet\":\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,2):\n",
    "            try:\n",
    "                model_euk.layer4[i].bn1.weight.copy_(model_initial.layer4[i].bn1.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].bn1.bias.copy_(model_initial.layer4[i].bn1.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv1.weight.copy_(model_initial.layer4[i].conv1.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv1.bias.copy_(model_initial.layer4[i].conv1.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "\n",
    "            try:\n",
    "                model_euk.layer4[i].bn2.weight.copy_(model_initial.layer4[i].bn2.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].bn2.bias.copy_(model_initial.layer4[i].bn2.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv2.weight.copy_(model_initial.layer4[i].conv2.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv2.bias.copy_(model_initial.layer4[i].conv2.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "\n",
    "        model_euk.layer4[0].shortcut[0].weight.copy_(model_initial.layer4[0].shortcut[0].weight)\n",
    "        \n",
    "    for param in model_euk.layer4.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "fk_fientune(model_euk, retain_loader, epochs=args.euk_epochs, quiet=True, lr=args.euk_lr, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try: readouts\n",
    "except: readouts = {}\n",
    "\n",
    "_,_=activations_predictions(copy.deepcopy(model),forget_loader,'Original_Model_D_f')\n",
    "thresh=log_dict['Original_Model_D_f_loss']+1e-5\n",
    "print(thresh)\n",
    "readouts[\"a\"] = all_readouts(copy.deepcopy(model),thresh,'Original')\n",
    "readouts[\"b\"] = all_readouts(copy.deepcopy(model0),thresh,'Retrain')\n",
    "readouts[\"c\"] = all_readouts(copy.deepcopy(model_ft),thresh,'Finetune')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model_ng),thresh,'NegGrad')\n",
    "readouts[\"e\"] = all_readouts(copy.deepcopy(model_cfk),thresh,'CF-k')\n",
    "readouts[\"f\"] = all_readouts(copy.deepcopy(model_euk),thresh,'EU-k')\n",
    "readouts[\"g\"] = all_readouts(copy.deepcopy(modelf),thresh,'Fisher')\n",
    "readouts[\"h\"] = all_readouts(copy.deepcopy(model_scrub),thresh,'NTK')\n",
    "readouts[\"i\"] = all_readouts(copy.deepcopy(model_s),thresh,'SCRUB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
