{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### MACHINE UNLEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mw/2gycpkq57p97cpwvmwvm69m00000gn/T/ipykernel_19482/1376361763.py:21: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%matplotlib inline\n",
    "import utils\n",
    "import variational\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from itertools import cycle\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from typing import List\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "from models import *\n",
    "import models\n",
    "from logger import *\n",
    "import wandb\n",
    "from thirdparty.repdistiller.helper.util import adjust_learning_rate as sgda_adjust_learning_rate\n",
    "from thirdparty.repdistiller.distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss\n",
    "from thirdparty.repdistiller.distiller_zoo import PKT, ABLoss, FactorTransfer, KDSVD, FSP, NSTLoss\n",
    "from thirdparty.repdistiller.helper.loops import train_distill, train_distill_hide, train_distill_linear, train_vanilla, train_negrad, train_bcu, train_bcu_distill, validate\n",
    "from thirdparty.repdistiller.helper.pretrain import init\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train Original Model\n",
    "\n",
    "This cell will run main.py which generate a model and leave many checkpoints for later references. We\n",
    "prepare standard_model_for_pilot for the sake of pilot.\n",
    "\n",
    "dataset: mnist\n",
    "\n",
    "possible datasets, (specifically refer to the dataset_multiclass.py)\n",
    "cifar10,small_cifar5,small_cifar6,small_cifar10,small_binary_cifar10,\n",
    "cifar100,mnist,small_mnist,lacuna100,lacuna10,small_lacuna5,small_lacuna6,small_lacuna10\n",
    "small_binary_lacuna10,tinyimagenet_pretrain,tinyimagenet_finetune,tinyimagenet_finetune5,mix10,mix100\n",
    "\n",
    "model: mlp\n",
    "\n",
    "possible models (refer to models.py)\n",
    "ntk_wide_resnet, is_wide_resnet, wide_resnet, resnet_small, resnet, allcnn_no_bn, ntk_allcnn\n",
    "smallallcnn, allcnn, ntk_mlp, ntk_linear, mlp\n",
    "\n",
    "dataroot: data/MNIST\n",
    "resume from standard_model_for_pilot.pt in checkpoints folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: mnist_mlp_1_0_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in mnist_mlp_1_0_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 31681415.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 31285044.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 18285946.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 6904867.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/MNIST/raw\n",
      "\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 10\n",
      "State OrderedDict([('layers.0.weight', tensor([[-0.0832, -0.0317,  0.0080,  ...,  0.0650, -0.0346, -0.0338],\n",
      "        [-0.0027, -0.0888, -0.0519,  ..., -0.0379,  0.0615,  0.0149],\n",
      "        [-0.1416, -0.0336,  0.0156,  ...,  0.0427,  0.0694,  0.0308],\n",
      "        ...,\n",
      "        [-0.0182, -0.0285,  0.0117,  ...,  0.0094, -0.0225, -0.0526],\n",
      "        [-0.0159,  0.0288, -0.0066,  ...,  0.0381,  0.0633,  0.0451],\n",
      "        [ 0.0131, -0.0608,  0.0516,  ..., -0.0093,  0.0415, -0.0130]])), ('layers.0.bias', tensor([ 0.2570,  0.5360, -0.9828, -0.2736, -0.2282,  0.3082,  0.4359, -0.1057,\n",
      "        -0.0857,  0.2738,  0.0112, -0.4476,  0.2455,  0.2458, -0.0411, -0.3458,\n",
      "        -0.4463,  0.1637, -0.5212,  0.0843,  0.1513, -0.1780, -0.6362,  0.2063,\n",
      "         0.4134,  0.2404, -0.3497, -0.2096, -0.0949, -0.2470,  0.0197,  0.4874])), ('layers.2.weight', tensor([[-0.3519, -0.2154, -0.3365, -0.1412,  0.4156,  0.1818,  0.0200, -0.0604,\n",
      "          0.2205,  0.2322, -0.0053, -0.1393, -0.2035, -0.0579,  0.0095, -0.1644,\n",
      "          0.2235,  0.3024,  0.2238, -0.7361,  0.2985,  0.1393,  0.1623, -0.0921,\n",
      "         -0.0720, -0.1394,  0.2537, -0.1786,  0.1402, -0.0607,  0.0045,  0.0832],\n",
      "        [ 0.3928, -0.1737, -0.1759, -0.4551,  0.2481, -0.4473, -0.0411, -0.2373,\n",
      "         -0.0809,  0.0645,  0.0849,  0.2062, -0.1421, -0.0616,  0.1801, -0.0840,\n",
      "         -0.2335, -0.0692,  0.1498, -0.0470, -0.1586, -0.1081,  0.0933, -0.2024,\n",
      "         -0.1436, -0.1707, -0.0688, -0.5912,  0.1219, -0.1256,  0.2843, -0.1910],\n",
      "        [ 0.4523, -0.3588,  0.0086,  0.0764,  0.1664,  0.4396,  0.0665,  0.4759,\n",
      "         -0.1288, -0.1356, -0.1838,  0.0269, -0.1764, -0.1239,  0.3496, -0.3435,\n",
      "          0.2170, -0.0702,  0.2358,  0.0587, -0.4519, -0.0437,  0.2893, -0.2968,\n",
      "          0.0510,  0.1216,  0.3035,  0.1648, -0.0432,  0.4584,  0.1138, -0.1609],\n",
      "        [-0.2260, -0.0343, -0.1534, -0.0083, -0.2569,  0.0590,  0.2407,  0.2322,\n",
      "         -0.0340,  0.2132,  0.1880,  0.2921, -0.0644, -0.0330,  0.4186,  0.4952,\n",
      "          0.0323,  0.1553, -0.1318, -0.1697,  0.2046, -0.2184,  0.0321, -0.3517,\n",
      "          0.3038, -0.0949, -0.1500,  0.1053, -0.0209, -0.1221, -0.1718, -0.0792],\n",
      "        [ 0.3664,  0.1804, -0.0120,  0.2229, -0.2088,  0.1287, -0.2451, -0.3353,\n",
      "          0.0897, -0.2442,  0.0131, -0.2590, -0.1019,  0.1473,  0.3632,  0.2329,\n",
      "          0.5305,  0.1337, -0.1440,  0.1620,  0.3382, -0.3171,  0.0806,  0.1881,\n",
      "          0.0109,  0.5281, -0.3445,  0.2525, -0.2200, -0.4710,  0.2022, -0.2743],\n",
      "        [ 0.1823, -0.0562, -0.2757, -0.0546, -0.1238, -0.1061,  0.0172, -0.0347,\n",
      "          0.4565,  0.1192, -0.0601,  0.2493, -0.2201,  0.0361,  0.0090,  0.2869,\n",
      "          0.0090, -0.2832, -0.0779, -0.2768, -0.3961, -0.1289,  0.1251, -0.0015,\n",
      "          0.2930,  0.1101,  0.0197,  0.2491,  0.0698,  0.3177,  0.0416,  0.0137],\n",
      "        [-0.3832,  0.2309, -0.1809,  0.1982,  0.0751,  0.0873, -0.1564,  0.0144,\n",
      "         -0.0449, -0.3521,  0.0023, -0.1973, -0.3833,  0.2016, -0.5147, -0.0191,\n",
      "          0.0122, -0.0809,  0.0038,  0.1643,  0.2994,  0.0281, -0.0899, -0.0444,\n",
      "          0.2220,  0.0072, -0.0604, -0.2090, -0.1963, -0.1330, -0.2076,  0.0046],\n",
      "        [-0.2165, -0.1100,  0.0688, -0.3747,  0.4035, -0.0398, -0.0595, -0.1467,\n",
      "         -0.0216,  0.1590, -0.0791, -0.5878,  0.2140,  0.0116,  0.0842,  0.0181,\n",
      "         -0.2411,  0.1744, -0.0815,  0.3069, -0.1610,  0.1093,  0.1149, -0.2111,\n",
      "         -0.4566, -0.0443,  0.1989, -0.1942, -0.2622, -0.1263,  0.3670, -0.2812],\n",
      "        [-0.1144,  0.1658, -0.1994,  0.2232, -0.0683,  0.2219,  0.2655,  0.4258,\n",
      "         -0.0326, -0.1462, -0.0584,  0.2980, -0.0663,  0.1896, -0.0486,  0.3528,\n",
      "         -0.2598,  0.0683,  0.2263, -0.3286,  0.0065, -0.0126, -0.1758,  0.2649,\n",
      "         -0.1740,  0.2994, -0.0692, -0.3554,  0.0484, -0.0219, -0.4493,  0.4110],\n",
      "        [ 0.4281, -0.0311, -0.3707,  0.5601, -0.2563, -0.1194, -0.2108, -0.0102,\n",
      "         -0.1516, -0.0340, -0.2634,  0.4547, -0.1523,  0.6067, -0.4058,  0.1322,\n",
      "         -0.3815,  0.2264, -0.0823, -0.2132, -0.0130,  0.1347,  0.1836,  0.1278,\n",
      "         -0.0120, -0.0764, -0.3119, -0.1457,  0.3770, -0.0323, -0.4128, -0.1316]])), ('layers.2.bias', tensor([ 0.2496,  0.6326,  0.5569,  0.1073,  0.2888, -0.0639,  0.1895,  0.2966,\n",
      "        -0.0117, -0.5746]))])\n",
      "Args checkpoints/standard_model_for_pilot.pt\n",
      "[0] train metrics:{\"loss\": 1.6957053787638154, \"error\": 0.4350262478126823}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 1.2201567613601685, \"error\": 0.2158}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 7.84 sec\n",
      "[1] train metrics:{\"loss\": 1.135481510219172, \"error\": 0.18546371135738687}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.79 sec\n",
      "[2] train metrics:{\"loss\": 1.0566367169994382, \"error\": 0.1521956503624698}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.58 sec\n",
      "[3] train metrics:{\"loss\": 1.0370397905877864, \"error\": 0.14057161903174736}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.14 sec\n",
      "[4] train metrics:{\"loss\": 1.0285105837671848, \"error\": 0.13388467627697692}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.92 sec\n",
      "[5] train metrics:{\"loss\": 1.0239834974342739, \"error\": 0.13053078910090826}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.75 sec\n",
      "[6] train metrics:{\"loss\": 1.0214508123054533, \"error\": 0.12780184984584617}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.75 sec\n",
      "[7] train metrics:{\"loss\": 1.0196682121115777, \"error\": 0.1277601866511124}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 7.38 sec\n",
      "[8] train metrics:{\"loss\": 1.0185629796230855, \"error\": 0.12644779601699857}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.54 sec\n",
      "[9] train metrics:{\"loss\": 1.0178989100958862, \"error\": 0.1261144904591284}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.93 sec\n",
      "[10] train metrics:{\"loss\": 1.0175718008781531, \"error\": 0.12505207899341722}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.57 sec\n",
      "[11] train metrics:{\"loss\": 1.0170587113515286, \"error\": 0.1247812682276477}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 7.11 sec\n",
      "[12] train metrics:{\"loss\": 1.016697402378289, \"error\": 0.1252187317723523}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.11 sec\n",
      "[13] train metrics:{\"loss\": 1.016376818798848, \"error\": 0.12488542621448212}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.74 sec\n",
      "[14] train metrics:{\"loss\": 1.0164363585892404, \"error\": 0.12386467794350471}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.68 sec\n",
      "[15] train metrics:{\"loss\": 1.0160483885343825, \"error\": 0.12453128905924506}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.64 sec\n",
      "[16] train metrics:{\"loss\": 1.0159531660292926, \"error\": 0.12482293142238148}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.63 sec\n",
      "[17] train metrics:{\"loss\": 1.015965955176559, \"error\": 0.12442713107241063}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.56 sec\n",
      "[18] train metrics:{\"loss\": 1.0159101984856695, \"error\": 0.1240104991250729}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.63 sec\n",
      "[19] train metrics:{\"loss\": 1.0157534404452986, \"error\": 0.12465627864344637}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.59 sec\n",
      "[20] train metrics:{\"loss\": 1.0156692651229822, \"error\": 0.12405216231980669}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.53 sec\n",
      "[21] train metrics:{\"loss\": 1.015744189621657, \"error\": 0.12451045746187818}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.65 sec\n",
      "[22] train metrics:{\"loss\": 1.015743130570143, \"error\": 0.12415632030664112}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.57 sec\n",
      "[23] train metrics:{\"loss\": 1.015751474828047, \"error\": 0.12469794183818016}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.51 sec\n",
      "[24] train metrics:{\"loss\": 1.0157363732977018, \"error\": 0.12426047829347554}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.61 sec\n",
      "[25] train metrics:{\"loss\": 1.0156331737730246, \"error\": 0.12367719356720273}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.47 sec\n",
      "[26] train metrics:{\"loss\": 1.0157239567924565, \"error\": 0.12380218315140405}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.6 sec\n",
      "[27] train metrics:{\"loss\": 1.0156740184009458, \"error\": 0.12392717273560537}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.53 sec\n",
      "[28] train metrics:{\"loss\": 1.0157745137292935, \"error\": 0.12438546787767686}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.52 sec\n",
      "[29] train metrics:{\"loss\": 1.0156028505246169, \"error\": 0.12455212065661195}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.53 sec\n",
      "[30] train metrics:{\"loss\": 1.015438807938419, \"error\": 0.12313557203566369}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.51 sec\n",
      "Pure training time: 180.81999999999996 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset mnist --model mlp --dataroot=data/MNIST/ --filters 1.0 --lr 0.001 \\\n",
    "--resume checkpoints/standard_model_for_pilot.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 --seed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Note\n",
    "After running this cell, the code will generate a trace of checkpoints under the \"checkpoints\" folder.\n",
    "They are useful for later cell codes. But you could remove them after completing the experiment.\n",
    "\n",
    "Also, the code would also download a MNIST dataset in the data folder. You could remove it after the\n",
    "experiment.\n",
    "\n",
    "The code also generate a log file under \"logs\" folder if one wants to analyze using matlab. You could\n",
    "remove it after the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train Retain Model\n",
    "\n",
    "This code is used to train the retain model. The specific change compared to the upper cell (to train\n",
    "the original model) is the forget class command. This command refers datasets.get_loaders() in main.py\n",
    "and in dataset_multiclass.py, four functions are used to separate the retain class and the forget\n",
    "class. They are replace_indexes(), replace_class(), confuse_class(), get_loaders().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: mnist_mlp_1_0_forget_[0, 1, 2, 3, 4, 5]_num_300_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in mnist_mlp_1_0_forget_[0, 1, 2, 3, 4, 5]_num_300_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Replacing indexes [ 8526 16694 15152 29352   141 36509 37843 16926 35321 26042 21345  5714\n",
      " 47547 14437 39793 44980 45137  3897 35780 29999 33655 37420 40980 28992\n",
      "   105 40544 41016 46885 46091 40786  5543  7014 20505 25936 16792  2457\n",
      "  1600 13007  7760 15774 46362 33702 32877  2543 33599 32886 35244 29239\n",
      "  8640 23665 45315  3038 22738  3554  4458 36789 11462  2191 23706 10872\n",
      "  9290 32216 25081 12968 28129 20523  5261 16762 26382 19263 12728 37875\n",
      " 36911  7326 21202 31998 26810 28700 36488 42838  3180 24261 25761 13600\n",
      " 18425 29148 39305  4254  2025  4307 45980 28150 36331  8747 35060 28517\n",
      "  8694 12015 38188 36605 26116 12256 18607 11847  1442 19196 18989 33194\n",
      " 26487  5803   828 40152 11342 43196 20715 41802 14043  8865 20144 41373\n",
      " 47285  8202 21858  1815 22335  2739 16404  6882  9838 14297  4773 29745\n",
      "  8784 20420  9033 18865 39460 23585 14421 25902 31362 17568 47282 10213\n",
      " 10851 27398  9097 13317  3946 12763 22347 26179 27559 12416 12204 19720\n",
      " 28625 43760   991 18748  3274 11284 33410 22005  3957 23606 30636 39992\n",
      "  1264 25266 17560 31423 46872  6399  8159 35119  1818 44722 35781 44342\n",
      " 25267   634 46251  4819 19794 29351 33480 27853 45953 19391  8342 23363\n",
      " 10898 22639 11929 33139 20729 23386 14207 42742  6533 15472 42530 19571\n",
      " 22377 37233 37954 28603  5153 40736 28158 13824 24596 47089  4148 32854\n",
      " 44602 27694 42719 24075  6046 21712  7058  1106 33206 27058 40874 12132\n",
      " 27016 21465 25892  8743  8810 33435 33167 23515 47454 24149 46878 44043\n",
      " 12757 41548 24280 38149  3902 38656 29473  9135 19714 31691 38719 42099\n",
      " 41461  6753 39036 38931 19219 20760  6409 44396  3192 30070 21387 10447\n",
      " 47641  5579 43237 20422 15418 29575 24229 42701  5380  5442 12690 44635\n",
      " 35380 24691 28596 36012 38802  8417 18947 17449 14339 25511  4570 22447\n",
      " 12060 46231 41953 29310 37274 10157  1795 18569 36490 23796 29340 43197]\n",
      "Number of Classes: 10\n",
      "State OrderedDict([('layers.0.weight', tensor([[-0.0832, -0.0317,  0.0080,  ...,  0.0650, -0.0346, -0.0338],\n",
      "        [-0.0027, -0.0888, -0.0519,  ..., -0.0379,  0.0615,  0.0149],\n",
      "        [-0.1416, -0.0336,  0.0156,  ...,  0.0427,  0.0694,  0.0308],\n",
      "        ...,\n",
      "        [-0.0182, -0.0285,  0.0117,  ...,  0.0094, -0.0225, -0.0526],\n",
      "        [-0.0159,  0.0288, -0.0066,  ...,  0.0381,  0.0633,  0.0451],\n",
      "        [ 0.0131, -0.0608,  0.0516,  ..., -0.0093,  0.0415, -0.0130]])), ('layers.0.bias', tensor([ 0.2570,  0.5360, -0.9828, -0.2736, -0.2282,  0.3082,  0.4359, -0.1057,\n",
      "        -0.0857,  0.2738,  0.0112, -0.4476,  0.2455,  0.2458, -0.0411, -0.3458,\n",
      "        -0.4463,  0.1637, -0.5212,  0.0843,  0.1513, -0.1780, -0.6362,  0.2063,\n",
      "         0.4134,  0.2404, -0.3497, -0.2096, -0.0949, -0.2470,  0.0197,  0.4874])), ('layers.2.weight', tensor([[-0.3519, -0.2154, -0.3365, -0.1412,  0.4156,  0.1818,  0.0200, -0.0604,\n",
      "          0.2205,  0.2322, -0.0053, -0.1393, -0.2035, -0.0579,  0.0095, -0.1644,\n",
      "          0.2235,  0.3024,  0.2238, -0.7361,  0.2985,  0.1393,  0.1623, -0.0921,\n",
      "         -0.0720, -0.1394,  0.2537, -0.1786,  0.1402, -0.0607,  0.0045,  0.0832],\n",
      "        [ 0.3928, -0.1737, -0.1759, -0.4551,  0.2481, -0.4473, -0.0411, -0.2373,\n",
      "         -0.0809,  0.0645,  0.0849,  0.2062, -0.1421, -0.0616,  0.1801, -0.0840,\n",
      "         -0.2335, -0.0692,  0.1498, -0.0470, -0.1586, -0.1081,  0.0933, -0.2024,\n",
      "         -0.1436, -0.1707, -0.0688, -0.5912,  0.1219, -0.1256,  0.2843, -0.1910],\n",
      "        [ 0.4523, -0.3588,  0.0086,  0.0764,  0.1664,  0.4396,  0.0665,  0.4759,\n",
      "         -0.1288, -0.1356, -0.1838,  0.0269, -0.1764, -0.1239,  0.3496, -0.3435,\n",
      "          0.2170, -0.0702,  0.2358,  0.0587, -0.4519, -0.0437,  0.2893, -0.2968,\n",
      "          0.0510,  0.1216,  0.3035,  0.1648, -0.0432,  0.4584,  0.1138, -0.1609],\n",
      "        [-0.2260, -0.0343, -0.1534, -0.0083, -0.2569,  0.0590,  0.2407,  0.2322,\n",
      "         -0.0340,  0.2132,  0.1880,  0.2921, -0.0644, -0.0330,  0.4186,  0.4952,\n",
      "          0.0323,  0.1553, -0.1318, -0.1697,  0.2046, -0.2184,  0.0321, -0.3517,\n",
      "          0.3038, -0.0949, -0.1500,  0.1053, -0.0209, -0.1221, -0.1718, -0.0792],\n",
      "        [ 0.3664,  0.1804, -0.0120,  0.2229, -0.2088,  0.1287, -0.2451, -0.3353,\n",
      "          0.0897, -0.2442,  0.0131, -0.2590, -0.1019,  0.1473,  0.3632,  0.2329,\n",
      "          0.5305,  0.1337, -0.1440,  0.1620,  0.3382, -0.3171,  0.0806,  0.1881,\n",
      "          0.0109,  0.5281, -0.3445,  0.2525, -0.2200, -0.4710,  0.2022, -0.2743],\n",
      "        [ 0.1823, -0.0562, -0.2757, -0.0546, -0.1238, -0.1061,  0.0172, -0.0347,\n",
      "          0.4565,  0.1192, -0.0601,  0.2493, -0.2201,  0.0361,  0.0090,  0.2869,\n",
      "          0.0090, -0.2832, -0.0779, -0.2768, -0.3961, -0.1289,  0.1251, -0.0015,\n",
      "          0.2930,  0.1101,  0.0197,  0.2491,  0.0698,  0.3177,  0.0416,  0.0137],\n",
      "        [-0.3832,  0.2309, -0.1809,  0.1982,  0.0751,  0.0873, -0.1564,  0.0144,\n",
      "         -0.0449, -0.3521,  0.0023, -0.1973, -0.3833,  0.2016, -0.5147, -0.0191,\n",
      "          0.0122, -0.0809,  0.0038,  0.1643,  0.2994,  0.0281, -0.0899, -0.0444,\n",
      "          0.2220,  0.0072, -0.0604, -0.2090, -0.1963, -0.1330, -0.2076,  0.0046],\n",
      "        [-0.2165, -0.1100,  0.0688, -0.3747,  0.4035, -0.0398, -0.0595, -0.1467,\n",
      "         -0.0216,  0.1590, -0.0791, -0.5878,  0.2140,  0.0116,  0.0842,  0.0181,\n",
      "         -0.2411,  0.1744, -0.0815,  0.3069, -0.1610,  0.1093,  0.1149, -0.2111,\n",
      "         -0.4566, -0.0443,  0.1989, -0.1942, -0.2622, -0.1263,  0.3670, -0.2812],\n",
      "        [-0.1144,  0.1658, -0.1994,  0.2232, -0.0683,  0.2219,  0.2655,  0.4258,\n",
      "         -0.0326, -0.1462, -0.0584,  0.2980, -0.0663,  0.1896, -0.0486,  0.3528,\n",
      "         -0.2598,  0.0683,  0.2263, -0.3286,  0.0065, -0.0126, -0.1758,  0.2649,\n",
      "         -0.1740,  0.2994, -0.0692, -0.3554,  0.0484, -0.0219, -0.4493,  0.4110],\n",
      "        [ 0.4281, -0.0311, -0.3707,  0.5601, -0.2563, -0.1194, -0.2108, -0.0102,\n",
      "         -0.1516, -0.0340, -0.2634,  0.4547, -0.1523,  0.6067, -0.4058,  0.1322,\n",
      "         -0.3815,  0.2264, -0.0823, -0.2132, -0.0130,  0.1347,  0.1836,  0.1278,\n",
      "         -0.0120, -0.0764, -0.3119, -0.1457,  0.3770, -0.0323, -0.4128, -0.1316]])), ('layers.2.bias', tensor([ 0.2496,  0.6326,  0.5569,  0.1073,  0.2888, -0.0639,  0.1895,  0.2966,\n",
      "        -0.0117, -0.5746]))])\n",
      "Args checkpoints/standard_model_for_pilot.pt\n",
      "[0] train metrics:{\"loss\": 1.6955508042470524, \"error\": 0.4350262478126823}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 1.220088168334961, \"error\": 0.2163}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.65 sec\n",
      "[1] train metrics:{\"loss\": 1.1354534324730707, \"error\": 0.18535955337055246}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.51 sec\n",
      "[2] train metrics:{\"loss\": 1.056726030901704, \"error\": 0.15227897675193733}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.51 sec\n",
      "[3] train metrics:{\"loss\": 1.0371734988102366, \"error\": 0.14057161903174736}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.51 sec\n",
      "[4] train metrics:{\"loss\": 1.028648248931466, \"error\": 0.13371802349804182}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.75 sec\n",
      "[5] train metrics:{\"loss\": 1.024162956063285, \"error\": 0.13086409465877843}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.54 sec\n",
      "[6] train metrics:{\"loss\": 1.0216317594711923, \"error\": 0.12803099741688193}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.5 sec\n",
      "[7] train metrics:{\"loss\": 1.0198602803487915, \"error\": 0.1279476710274144}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.61 sec\n",
      "[8] train metrics:{\"loss\": 1.0187525477630677, \"error\": 0.12640613282226482}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.73 sec\n",
      "[9] train metrics:{\"loss\": 1.0180947887193221, \"error\": 0.12623948004332972}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 7.71 sec\n",
      "[10] train metrics:{\"loss\": 1.0177762444918677, \"error\": 0.12536455295392052}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.81 sec\n",
      "[11] train metrics:{\"loss\": 1.017264149882537, \"error\": 0.12498958420131656}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.38 sec\n",
      "[12] train metrics:{\"loss\": 1.016896297669234, \"error\": 0.1252187317723523}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.52 sec\n",
      "[13] train metrics:{\"loss\": 1.0165873311497888, \"error\": 0.1253437213565536}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.69 sec\n",
      "[14] train metrics:{\"loss\": 1.0166354835376115, \"error\": 0.12396883593033914}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.72 sec\n",
      "[15] train metrics:{\"loss\": 1.0162662003658363, \"error\": 0.12498958420131656}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.44 sec\n",
      "[16] train metrics:{\"loss\": 1.0161743731997765, \"error\": 0.12490625781184901}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.47 sec\n",
      "[17] train metrics:{\"loss\": 1.0162088719817681, \"error\": 0.12465627864344637}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.32 sec\n",
      "[18] train metrics:{\"loss\": 1.016125079642017, \"error\": 0.12413548870927422}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.21 sec\n",
      "[19] train metrics:{\"loss\": 1.0159665824681379, \"error\": 0.12457295225397884}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 8.94 sec\n",
      "[20] train metrics:{\"loss\": 1.0158672984883483, \"error\": 0.12428130989084243}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.8 sec\n",
      "[21] train metrics:{\"loss\": 1.0159629398599763, \"error\": 0.12455212065661195}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.62 sec\n",
      "[22] train metrics:{\"loss\": 1.0159606663110543, \"error\": 0.12409382551454046}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.66 sec\n",
      "[23] train metrics:{\"loss\": 1.0159797972316056, \"error\": 0.12498958420131656}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.69 sec\n",
      "[24] train metrics:{\"loss\": 1.0159649011910175, \"error\": 0.12407299391717357}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.79 sec\n",
      "[25] train metrics:{\"loss\": 1.015828314502779, \"error\": 0.12384384634613782}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.68 sec\n",
      "[26] train metrics:{\"loss\": 1.0159221884906515, \"error\": 0.12384384634613782}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.64 sec\n",
      "[27] train metrics:{\"loss\": 1.0158880807669817, \"error\": 0.12388550954087159}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.75 sec\n",
      "[28] train metrics:{\"loss\": 1.0159736355884146, \"error\": 0.12469794183818016}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.51 sec\n",
      "[29] train metrics:{\"loss\": 1.0158080630834456, \"error\": 0.12457295225397884}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.82 sec\n",
      "[30] train metrics:{\"loss\": 1.0156555478547615, \"error\": 0.12323973002249812}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.57 sec\n",
      "Pure training time: 180.9399999999999 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset mnist --model mlp --dataroot=data/MNIST/ --filters 1.0 --lr 0.001 \\\n",
    "--resume checkpoints/standard_model_for_pilot.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 \\\n",
    "--forget-class 0,1,2,3,4,5 --num-to-forget 300 --seed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This function will count the parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parameter_count(model):\n",
    "    count=0\n",
    "    for p in model.parameters():\n",
    "        count+=np.prod(np.array(list(p.shape)))\n",
    "    print(f'Total Number of Parameters: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Importantly, the model object is an empty object in this. It is not initialized with our previous trained models.\n",
    "Here, we measure the parameters in the empty model object. (Original interpretation)\n",
    "\n",
    "Note: deepcopy will give you the entire object, while other methods only gives you the reference pointer (for references,\n",
    "see Java, C basics).\n",
    "\n",
    "Or, the model object is from the main.py code.\n",
    "\n",
    "I have not figured out this yet. I think the model object is from the main.py code and the later code use\n",
    "deep copy to make sure everything is aligned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 33130\n"
     ]
    }
   ],
   "source": [
    "parameter_count(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Initialize original model and retain model\n",
    "\n",
    "Here, we will use the checkpoints references to get the models that we have trained in the previous cells. The models\n",
    "that is initialized is in the cache of this jupyter notebook workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Initialize three empty model objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_original = copy.deepcopy(model)\n",
    "model_retain = copy.deepcopy(model)\n",
    "model_pretrain = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We use checkpoint name to refer to the objects. Thus, here we initialize all the name parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arch = args.model\n",
    "filters=args.filters\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "augment = False\n",
    "dataset = args.dataset\n",
    "class_to_forget = args.forget_class\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "num_classes=args.num_classes\n",
    "num_to_forget = args.num_to_forget\n",
    "num_total = len(train_loader.dataset)\n",
    "num_to_retain = num_total - 300#num_to_forget\n",
    "seed = args.seed\n",
    "unfreeze_start = None\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "training_epochs=30\n",
    "log_dict={}\n",
    "log_dict['epoch']=training_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here, we get the name of the model we want to get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Make sure the names are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/mnist_mlp_1_0_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_30.pt\n"
     ]
    }
   ],
   "source": [
    "print(m_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/mnist_mlp_1_0_forget_[0, 1, 2, 3, 4, 5]_num_300_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_30.pt\n"
     ]
    }
   ],
   "source": [
    "print(m0_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/mnist_mlp_1_0_forget_[0, 1, 2, 3, 4, 5]_num_300_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_init.pt\n"
     ]
    }
   ],
   "source": [
    "print(init_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We get the original model, the retain, and the pretrained model that we used to train the original model and the\n",
    "retain model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_original.load_state_dict(torch.load(m_name))\n",
    "model_retain.load_state_dict(torch.load(m0_name))\n",
    "model_pretrain.load_state_dict(torch.load(init_checkpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We use parameters to see if we actually get the models from the checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 33130\n",
      "Total Number of Parameters: 33130\n",
      "Total Number of Parameters: 33130\n"
     ]
    }
   ],
   "source": [
    "parameter_count(copy.deepcopy(model_original))\n",
    "parameter_count(copy.deepcopy(model_retain))\n",
    "parameter_count(copy.deepcopy(model_pretrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We analyze the parameters in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_Original: Parameter containing:\n",
      "tensor([ 0.2353,  0.7190,  0.5154,  0.0759,  0.2476, -0.0129,  0.2155,  0.3334,\n",
      "        -0.1101, -0.5481], requires_grad=True)\n",
      "Model_Pretrain Parameter containing:\n",
      "tensor([ 0.2496,  0.6326,  0.5569,  0.1073,  0.2888, -0.0639,  0.1895,  0.2966,\n",
      "        -0.0117, -0.5746], requires_grad=True)\n",
      "Model_Retain Parameter containing:\n",
      "tensor([ 0.2496,  0.6326,  0.5569,  0.1073,  0.2888, -0.0639,  0.1895,  0.2966,\n",
      "        -0.0117, -0.5746], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in model_original.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "print(\"Model_Original:\", p)\n",
    "\n",
    "for p in model_pretrain.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "print(\"Model_Pretrain\", p)\n",
    "\n",
    "for p in model_pretrain.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "print(\"Model_Retain\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We use distances to further analyze the three models. First, we define the distance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this method take the model object, then, iterate each parameter in the model.parameters() [this will\n",
    "# return the parameters set in the model object]. for each parameter object, we name another parameter\n",
    "# that is the same as the parameter, in named_parameters, later, in the distance function, we use\n",
    "# this copy of parameters, to measure the distance.\n",
    "def named_parameters_creat(model_):\n",
    "    for p in model_.parameters():\n",
    "        p.data0 = p.data.clone()\n",
    "\n",
    "def distance(model,model0):\n",
    "    distance=0\n",
    "    normalization=0\n",
    "\n",
    "    # Create named parameters in the model object as a copy of the original parameters\n",
    "    named_parameters_creat(model)\n",
    "    named_parameters_creat(model0)\n",
    "\n",
    "    # Calculate distance iterating through the named parameters\n",
    "    for (k, p), (k0, p0) in zip(model.named_parameters(), model0.named_parameters()):\n",
    "        space='  ' if 'bias' in k else ''\n",
    "        current_dist=(p.data0-p0.data0).pow(2).sum().item()\n",
    "        current_norm=p.data0.pow(2).sum().item()\n",
    "        distance+=current_dist\n",
    "        normalization+=current_norm\n",
    "\n",
    "    print(f'Distance: {np.sqrt(distance)}')\n",
    "    print(f'Normalized Distance: {1.0*np.sqrt(distance/normalization)}')\n",
    "    return 1.0*np.sqrt(distance/normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We compare original model and the retain model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 0.017273186520086487\n",
      "Normalized Distance: 0.0017248963048984115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0017248963048984115"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance(model_original,model_retain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We compare the retain model and the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 2.838099897601149\n",
      "Normalized Distance: 0.3046515850998754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3046515850998754"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance(model_pretrain,model_retain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We compare the pretrain model and the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 2.8384955687527356\n",
      "Normalized Distance: 0.3046940578273545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3046940578273545"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance(model_pretrain, model_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Prepare Dataset for Unlearning\n",
    "\n",
    "explicit documentation has been add to datasets_multiclass.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "initialize retain and forget dataset batch size. And get forget dataset.\n",
    "\n",
    "Forget class has been originated when we train the model in main.py and it is given in jupyter notebook\n",
    "in our previous initialization section. To change the forget class, we need to re-run the main.py where\n",
    "we get the retain model.\n",
    "\n",
    "It is in the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confuse mode: False\n",
      "split mode: train\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Replacing indexes [ 8526 16694 15152 29352   141 36509 37843 16926 35321 26042 21345  5714\n",
      " 47547 14437 39793 44980 45137  3897 35780 29999 33655 37420 40980 28992\n",
      "   105 40544 41016 46885 46091 40786  5543  7014 20505 25936 16792  2457\n",
      "  1600 13007  7760 15774 46362 33702 32877  2543 33599 32886 35244 29239\n",
      "  8640 23665 45315  3038 22738  3554  4458 36789 11462  2191 23706 10872\n",
      "  9290 32216 25081 12968 28129 20523  5261 16762 26382 19263 12728 37875\n",
      " 36911  7326 21202 31998 26810 28700 36488 42838  3180 24261 25761 13600\n",
      " 18425 29148 39305  4254  2025  4307 45980 28150 36331  8747 35060 28517\n",
      "  8694 12015 38188 36605 26116 12256 18607 11847  1442 19196 18989 33194\n",
      " 26487  5803   828 40152 11342 43196 20715 41802 14043  8865 20144 41373\n",
      " 47285  8202 21858  1815 22335  2739 16404  6882  9838 14297  4773 29745\n",
      "  8784 20420  9033 18865 39460 23585 14421 25902 31362 17568 47282 10213\n",
      " 10851 27398  9097 13317  3946 12763 22347 26179 27559 12416 12204 19720\n",
      " 28625 43760   991 18748  3274 11284 33410 22005  3957 23606 30636 39992\n",
      "  1264 25266 17560 31423 46872  6399  8159 35119  1818 44722 35781 44342\n",
      " 25267   634 46251  4819 19794 29351 33480 27853 45953 19391  8342 23363\n",
      " 10898 22639 11929 33139 20729 23386 14207 42742  6533 15472 42530 19571\n",
      " 22377 37233 37954 28603  5153 40736 28158 13824 24596 47089  4148 32854\n",
      " 44602 27694 42719 24075  6046 21712  7058  1106 33206 27058 40874 12132\n",
      " 27016 21465 25892  8743  8810 33435 33167 23515 47454 24149 46878 44043\n",
      " 12757 41548 24280 38149  3902 38656 29473  9135 19714 31691 38719 42099\n",
      " 41461  6753 39036 38931 19219 20760  6409 44396  3192 30070 21387 10447\n",
      " 47641  5579 43237 20422 15418 29575 24229 42701  5380  5442 12690 44635\n",
      " 35380 24691 28596 36012 38802  8417 18947 17449 14339 25511  4570 22447\n",
      " 12060 46231 41953 29310 37274 10157  1795 18569 36490 23796 29340 43197]\n"
     ]
    }
   ],
   "source": [
    "args.retain_bs = 32\n",
    "args.forget_bs = 64\n",
    "train_loader_full, valid_loader_full, test_loader_full   = datasets.get_loaders(dataset, batch_size=args.batch_size, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "marked_loader, _, _ = datasets.get_loaders(dataset, class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, root=args.dataroot, augment=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this case, we define a function that puts the dataset into the data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "    manual_seed(seed)\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "    def _init_fn(worker_id):\n",
    "        np.random.seed(int(seed))\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Get forget dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = forget_dataset.targets < 0\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, batch_size=args.forget_bs, seed=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Get retain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = retain_dataset.targets >= 0\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, batch_size=args.retain_bs, seed=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Make sure everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "47704\n",
      "10000\n",
      "48004\n",
      "{5: 4337, 4: 4674, 9: 4760, 2: 4767, 1: 5394, 3: 4905, 6: 4735, 7: 5012, 8: 4681, 0: 4739}\n"
     ]
    }
   ],
   "source": [
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))\n",
    "print (len(forget_loader.dataset))\n",
    "print (len(retain_loader.dataset))\n",
    "print (len(test_loader_full.dataset))\n",
    "print (len(train_loader_full.dataset))\n",
    "from collections import Counter\n",
    "print(dict(Counter(train_loader_full.dataset.targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fine-tuning with Retain Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This function calculates the L2 regularization penalty term for the model's weights by comparing them to the initial\n",
    " (untrained) model's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def l2_penalty(model,model_init,weight_decay):\n",
    "    l2_loss = 0\n",
    "    for (k,p),(k_init,p_init) in zip(model.named_parameters(),model_init.named_parameters()):\n",
    "        if p.requires_grad:\n",
    "            l2_loss += (p-p_init).pow(2).sum()\n",
    "    l2_loss *= (weight_decay/2.)\n",
    "    return l2_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define training function for fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "def run_finetune_train_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader,\n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, epoch=int):\n",
    "\n",
    "    # you set the model's internal state to indicate that it's being used for inference or evaluation, not for training.\n",
    "    # It ensures that certain layers and behaviors in the model are adjusted for this purpose\n",
    "    model.eval()\n",
    "\n",
    "    # Computes and stores the average and current value, defined in utils.py\n",
    "    metrics = AverageMeter()\n",
    "\n",
    "    with torch.set_grad_enabled(True):\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def finetune(model: nn.Module, data_loader: torch.utils.data.DataLoader, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        run_finetune_train_epoch(model, model_init, data_loader, loss_fn, optimizer, epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7796aaec3c4aaaa88f2501f89ee29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3bcfbff91c444ca5c246bf15ac0876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a8cbf2a354436dac11d6439b53078d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c54f89ca6d431badf6e6e826e1b3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa0e9a662434ddaa123eff8f5eb2cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e48861c01f499696d1e5ce68554997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b693ffabb8449d8a1a892ac5bf91578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66e1f73c5084f768504f7680070358c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6b89e3961f4544922ec25457aaad1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4132ccfe001408eb7b49d02fa90f092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_ft = copy.deepcopy(model)\n",
    "retain_loader = replace_loader_dataset(train_loader_full,retain_dataset, seed=seed, batch_size=args.batch_size, shuffle=True)\n",
    "finetune(model_ft, retain_loader, epochs=10, quiet=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Negative Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def negative_grad(model: nn.Module, data_loader: torch.utils.data.DataLoader, forget_loader: torch.utils.data.DataLoader, alpha: float, lr=0.01, epochs=10):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        run_neggrad_epoch(model, model_init, data_loader, forget_loader, alpha, loss_fn, optimizer, epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_neggrad_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader,\n",
    "                    forget_loader: torch.utils.data.DataLoader,\n",
    "                    alpha: float,\n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, epoch: int):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    with torch.set_grad_enabled(True):\n",
    "        for idx, (batch_retain,batch_forget) in enumerate(tqdm(zip(data_loader,cycle(forget_loader)), leave=False)):\n",
    "            batch_retain = [tensor.to(next(model.parameters()).device) for tensor in batch_retain]\n",
    "            batch_forget = [tensor.to(next(model.parameters()).device) for tensor in batch_forget]\n",
    "            input_r, target_r = batch_retain\n",
    "            input_f, target_f = batch_forget\n",
    "            output_r = model(input_r)\n",
    "            output_f = model(input_f)\n",
    "            # Negative Gradient Loss Function\n",
    "            loss = alpha*(loss_fn(output_r, target_r) + l2_penalty(model,model_init,args.weight_decay)) - (1-alpha)*loss_fn(output_f, target_f)\n",
    "            metrics.update(n=input_r.size(0), loss=loss_fn(output_r,target_r).item(), error=get_error(output_r, target_r))\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6dd93bdf184f3aa8e11ffed4df0c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca9fd9d25ac4ae9abdf6de7c0e076d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2eff4a5965d4c0f9ac7e7d3349a4bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6291178419344298516f818adedea73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1cdc263dc84163ab31b7e4d0e7ea75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94a329ca14b4f72b6441fde2c1e6c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ad5924012f4fb18e3815a2bc7ab240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d42a7699046494f9d7ac72a2319624b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6919e926b78c4b5caf5adcef8ef7a006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab4bbd0aa3d4c3f96f64316c4c1bdfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args.ng_alpha = 0.95\n",
    "args.ng_epochs = 10\n",
    "args.ng_lr = 0.01\n",
    "model_negative_gradient = copy.deepcopy(model)\n",
    "negative_grad(model_negative_gradient, retain_loader, forget_loader, alpha=args.ng_alpha, epochs=args.ng_epochs, lr=args.ng_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fisher Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_fisher_on_original = copy.deepcopy(model_original)\n",
    "model_fisher_on_pretrained = copy.deepcopy(model_pretrain)\n",
    "for p in itertools.chain(model_fisher_on_original.parameters(), model_fisher_on_pretrained.parameters()):\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hessian(data_set, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(data_set, batch_size=1, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc = 0\n",
    "        p.grad2_acc = 0\n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(args.device), orig_target.to(args.device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad_acc += (orig_target == target).float() * p.grad.data\n",
    "                    p.grad2_acc += prob[:, y] * p.grad.data.pow(2)\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc /= len(train_loader)\n",
    "        p.grad2_acc /= len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_mean_var(p, is_base_dist=False, alpha=3e-6):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3)\n",
    "    if p.size(0) == num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var\n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    if not is_base_dist:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    else:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    if p.size(0) == num_classes and num_to_forget is None:\n",
    "        mu[class_to_forget] = 0\n",
    "        var[class_to_forget] = 0.0001\n",
    "    if p.size(0) == num_classes:\n",
    "        # Last layer\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        # BatchNorm\n",
    "        var *= 10\n",
    "    return mu, var\n",
    "def kl_divergence_fisher(mu0, var0, mu1, var1):\n",
    "    return ((mu1 - mu0).pow(2)/var0 + var1/var0 - torch.log(var1/var0) - 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533cdde52aee423e9d4a182b4086f97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdd5c26d2424c269456d3f872cd234a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hessian(retain_loader.dataset, model_fisher_on_original)\n",
    "hessian(retain_loader.dataset, model_fisher_on_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.weight 84078.1\n",
      "layers.0.bias 2873.3\n",
      "layers.2.weight 47328.5\n",
      "layers.2.bias 1491.0\n",
      "Total: 135770.9493408203\n"
     ]
    }
   ],
   "source": [
    "# Computes the amount of information not forgotten at all layers using the given alpha\n",
    "\n",
    "alpha = 1e-7\n",
    "total_kl = 0\n",
    "torch.manual_seed(seed)\n",
    "for (k, p), (k0, p0) in zip(model_fisher_on_original.named_parameters(), model_fisher_on_pretrained.named_parameters()):\n",
    "    mu0, var0 = get_mean_var(p, False, alpha=alpha)\n",
    "    mu1, var1 = get_mean_var(p0, True, alpha=alpha)\n",
    "    kl = kl_divergence_fisher(mu0, var0, mu1, var1).item()\n",
    "    total_kl += kl\n",
    "    print(k, f'{kl:.1f}')\n",
    "print(\"Total:\", total_kl)\n",
    "log_dict['fisher_info']=total_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fisher_dir = []\n",
    "alpha = 1e-6\n",
    "torch.manual_seed(seed)\n",
    "for i, p in enumerate(model_fisher_on_original.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "    fisher_dir.append(var.sqrt().view(-1).cpu().detach().numpy())\n",
    "for i, p in enumerate(model_fisher_on_pretrained.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.02581239, 0.02581239, 0.02581239, ..., 0.02663051, 0.02663051,\n",
      "       0.02663051], dtype=float32), array([0.0112033 , 0.01925907, 0.1       , 0.01317145, 0.01422679,\n",
      "       0.01722576, 0.01903651, 0.01380545, 0.02635191, 0.01516563,\n",
      "       0.08655215, 0.1       , 0.02218408, 0.01602794, 0.01262238,\n",
      "       0.01443665, 0.01447893, 0.01836849, 0.1       , 0.01328964,\n",
      "       0.01359341, 0.1       , 0.05436911, 0.01596258, 0.01382499,\n",
      "       0.01617315, 0.01868596, 0.01819735, 0.03455006, 0.0157376 ,\n",
      "       0.01481927, 0.01508808], dtype=float32), array([0.02051884, 0.02051884, 0.02051884, 0.02051884, 0.02051884,\n",
      "       0.02051884, 0.02051884, 0.02051884, 0.02051884, 0.02051884,\n",
      "       0.02051884, 0.02051884, 0.02051884, 0.02051884, 0.02051884,\n",
      "       0.02051884, 0.02051884, 0.02051884, 0.02051884, 0.02051884,\n",
      "       0.02051884, 0.02051884, 0.02051884, 0.02051884, 0.02051884,\n",
      "       0.02051884, 0.02051884, 0.02051884, 0.02051884, 0.02051884,\n",
      "       0.02051884, 0.02051884, 0.02422607, 0.02422607, 0.02422607,\n",
      "       0.02422607, 0.02422607, 0.02422607, 0.02422607, 0.02422607,\n",
      "       0.02422607, 0.02422607, 0.02422607, 0.02422607, 0.02422607,\n",
      "       0.02422607, 0.02422607, 0.02422607, 0.02422607, 0.02422607,\n",
      "       0.02422607, 0.02422607, 0.02422607, 0.02422607, 0.02422607,\n",
      "       0.02422607, 0.02422607, 0.02422607, 0.02422607, 0.02422607,\n",
      "       0.02422607, 0.02422607, 0.02422607, 0.02422607, 0.02052028,\n",
      "       0.02052028, 0.02052028, 0.02052028, 0.02052028, 0.02052028,\n",
      "       0.02052028, 0.02052028, 0.02052028, 0.02052028, 0.02052028,\n",
      "       0.02052028, 0.02052028, 0.02052028, 0.02052028, 0.02052028,\n",
      "       0.02052028, 0.02052028, 0.02052028, 0.02052028, 0.02052028,\n",
      "       0.02052028, 0.02052028, 0.02052028, 0.02052028, 0.02052028,\n",
      "       0.02052028, 0.02052028, 0.02052028, 0.02052028, 0.02052028,\n",
      "       0.02052028, 0.01931612, 0.01931612, 0.01931612, 0.01931612,\n",
      "       0.01931612, 0.01931612, 0.01931612, 0.01931612, 0.01931612,\n",
      "       0.01931612, 0.01931612, 0.01931612, 0.01931612, 0.01931612,\n",
      "       0.01931612, 0.01931612, 0.01931612, 0.01931612, 0.01931612,\n",
      "       0.01931612, 0.01931612, 0.01931612, 0.01931612, 0.01931612,\n",
      "       0.01931612, 0.01931612, 0.01931612, 0.01931612, 0.01931612,\n",
      "       0.01931612, 0.01931612, 0.01931612, 0.02009388, 0.02009388,\n",
      "       0.02009388, 0.02009388, 0.02009388, 0.02009388, 0.02009388,\n",
      "       0.02009388, 0.02009388, 0.02009388, 0.02009388, 0.02009388,\n",
      "       0.02009388, 0.02009388, 0.02009388, 0.02009388, 0.02009388,\n",
      "       0.02009388, 0.02009388, 0.02009388, 0.02009388, 0.02009388,\n",
      "       0.02009388, 0.02009388, 0.02009388, 0.02009388, 0.02009388,\n",
      "       0.02009388, 0.02009388, 0.02009388, 0.02009388, 0.02009388,\n",
      "       0.0184122 , 0.0184122 , 0.0184122 , 0.0184122 , 0.0184122 ,\n",
      "       0.0184122 , 0.0184122 , 0.0184122 , 0.0184122 , 0.0184122 ,\n",
      "       0.0184122 , 0.0184122 , 0.0184122 , 0.0184122 , 0.0184122 ,\n",
      "       0.0184122 , 0.0184122 , 0.0184122 , 0.0184122 , 0.0184122 ,\n",
      "       0.0184122 , 0.0184122 , 0.0184122 , 0.0184122 , 0.0184122 ,\n",
      "       0.0184122 , 0.0184122 , 0.0184122 , 0.0184122 , 0.0184122 ,\n",
      "       0.0184122 , 0.0184122 , 0.0213054 , 0.0213054 , 0.0213054 ,\n",
      "       0.0213054 , 0.0213054 , 0.0213054 , 0.0213054 , 0.0213054 ,\n",
      "       0.0213054 , 0.0213054 , 0.0213054 , 0.0213054 , 0.0213054 ,\n",
      "       0.0213054 , 0.0213054 , 0.0213054 , 0.0213054 , 0.0213054 ,\n",
      "       0.0213054 , 0.0213054 , 0.0213054 , 0.0213054 , 0.0213054 ,\n",
      "       0.0213054 , 0.0213054 , 0.0213054 , 0.0213054 , 0.0213054 ,\n",
      "       0.0213054 , 0.0213054 , 0.0213054 , 0.0213054 , 0.02164385,\n",
      "       0.02164385, 0.02164385, 0.02164385, 0.02164385, 0.02164385,\n",
      "       0.02164385, 0.02164385, 0.02164385, 0.02164385, 0.02164385,\n",
      "       0.02164385, 0.02164385, 0.02164385, 0.02164385, 0.02164385,\n",
      "       0.02164385, 0.02164385, 0.02164385, 0.02164385, 0.02164385,\n",
      "       0.02164385, 0.02164385, 0.02164385, 0.02164385, 0.02164385,\n",
      "       0.02164385, 0.02164385, 0.02164385, 0.02164385, 0.02164385,\n",
      "       0.02164385, 0.01873186, 0.01873186, 0.01873186, 0.01873186,\n",
      "       0.01873186, 0.01873186, 0.01873186, 0.01873186, 0.01873186,\n",
      "       0.01873186, 0.01873186, 0.01873186, 0.01873186, 0.01873186,\n",
      "       0.01873186, 0.01873186, 0.01873186, 0.01873186, 0.01873186,\n",
      "       0.01873186, 0.01873186, 0.01873186, 0.01873186, 0.01873186,\n",
      "       0.01873186, 0.01873186, 0.01873186, 0.01873186, 0.01873186,\n",
      "       0.01873186, 0.01873186, 0.01873186, 0.01990515, 0.01990515,\n",
      "       0.01990515, 0.01990515, 0.01990515, 0.01990515, 0.01990515,\n",
      "       0.01990515, 0.01990515, 0.01990515, 0.01990515, 0.01990515,\n",
      "       0.01990515, 0.01990515, 0.01990515, 0.01990515, 0.01990515,\n",
      "       0.01990515, 0.01990515, 0.01990515, 0.01990515, 0.01990515,\n",
      "       0.01990515, 0.01990515, 0.01990515, 0.01990515, 0.01990515,\n",
      "       0.01990515, 0.01990515, 0.01990515, 0.01990515, 0.01990515],\n",
      "      dtype=float32), array([0.01689424, 0.01538974, 0.01409053, 0.01357748, 0.01368637,\n",
      "       0.01339486, 0.01553821, 0.01521467, 0.01207018, 0.01319715],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(fisher_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_fisher = copy.deepcopy(model_fisher_on_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate of Fine-tuning on Retain Set: 0.1011\n",
      "Error rate of Fine-tuning on Forget Set: 0.0967\n",
      "Error rate of Fine-tuning on Test Set: 0.0962\n",
      "Error rate of Negative Gradients on Retain Set: 0.1037\n",
      "Error rate of Negative Gradients on Forget Set: 0.1233\n",
      "Error rate of Negative Gradients on Test Set: 0.0970\n",
      "Error rate of Fisher Forgetting on Retain Set: 0.1446\n",
      "Error rate of Fisher Forgetting on Forget Set: 0.1533\n",
      "Error rate of Fisher Forgetting on Test Set: 0.1324\n",
      "{'Fine-tuning': {'Retain Set': 0.10114455810833473, 'Forget Set': 0.09666666666666666, 'Test Set': 0.0962}, 'Negative Gradients': {'Retain Set': 0.10370199563977864, 'Forget Set': 0.12333333333333334, 'Test Set': 0.097}, 'Fisher Forgetting': {'Retain Set': 0.14455810833473084, 'Forget Set': 0.15333333333333332, 'Test Set': 0.1324}}\n"
     ]
    }
   ],
   "source": [
    "# 定义测试函数\n",
    "def evaluate(model: nn.Module, data_loader: torch.utils.data.DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    返回模型在给定的数据加载器上的错误率\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_samples = 0\n",
    "    total_mistakes = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(next(model.parameters()).device), target.to(next(model.parameters()).device)\n",
    "            output = model(data)\n",
    "            prediction = torch.argmax(output, dim=1)\n",
    "            mistakes = torch.sum(prediction != target).item()\n",
    "            total_mistakes += mistakes\n",
    "            total_samples += data.size(0)\n",
    "\n",
    "    error_rate = total_mistakes / total_samples\n",
    "    return error_rate\n",
    "\n",
    "# 计算error\n",
    "models = {\n",
    "    \"Fine-tuning\": model_ft,\n",
    "    \"Negative Gradients\": model_negative_gradient,\n",
    "    \"Fisher Forgetting\": model_fisher\n",
    "}\n",
    "\n",
    "data_loaders = {\n",
    "    \"Retain Set\": retain_loader,\n",
    "    \"Forget Set\": forget_loader,\n",
    "    \"Test Set\": test_loader_full\n",
    "}\n",
    "\n",
    "# 存储结果\n",
    "results = {}\n",
    "\n",
    "for method, model in models.items():\n",
    "    results[method] = {}\n",
    "    for dataset_name, loader in data_loaders.items():\n",
    "        error_rate = evaluate(model, loader)\n",
    "        results[method][dataset_name] = error_rate\n",
    "        print(f\"Error rate of {method} on {dataset_name}: {error_rate:.4f}\")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MIA on Original Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shr91\\AppData\\Local\\Temp\\ipykernel_16804\\485575046.py:103: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(test_losses, kde=False, label='test-loss')\n",
      "C:\\Users\\shr91\\AppData\\Local\\Temp\\ipykernel_16804\\485575046.py:104: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(forget_losses, kde=False, label='forget-loss')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR:0.44, FNR:0.48, FP8.00, TN10.00, TP22.00, FN20.00\n",
      "FPR:0.48, FNR:0.49, FP10.00, TN11.00, TP20.00, FN19.00\n",
      "FPR:0.44, FNR:0.48, FP7.00, TN9.00, TP23.00, FN21.00\n",
      "FPR:0.67, FNR:0.59, FP14.00, TN7.00, TP16.00, FN23.00\n",
      "FPR:0.53, FNR:0.51, FP10.00, TN9.00, TP20.00, FN21.00\n",
      "Attack accuracy for Original Model: 0.49000000000000005\n",
      "\n",
      "Evaluating MIA on Negative Gradients Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shr91\\AppData\\Local\\Temp\\ipykernel_16804\\485575046.py:103: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(test_losses, kde=False, label='test-loss')\n",
      "C:\\Users\\shr91\\AppData\\Local\\Temp\\ipykernel_16804\\485575046.py:104: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(forget_losses, kde=False, label='forget-loss')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR:0.44, FNR:0.48, FP8.00, TN10.00, TP22.00, FN20.00\n",
      "FPR:0.53, FNR:0.55, FP21.00, TN19.00, TP9.00, FN11.00\n",
      "FPR:0.58, FNR:0.56, FP14.00, TN10.00, TP16.00, FN20.00\n",
      "FPR:0.29, FNR:0.43, FP4.00, TN10.00, TP26.00, FN20.00\n",
      "FPR:0.38, FNR:0.45, FP6.00, TN10.00, TP24.00, FN20.00\n",
      "Attack accuracy for Negative Gradients Model: 0.5199999999999999\n",
      "\n",
      "Evaluating MIA on Fisher Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shr91\\AppData\\Local\\Temp\\ipykernel_16804\\485575046.py:103: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(test_losses, kde=False, label='test-loss')\n",
      "C:\\Users\\shr91\\AppData\\Local\\Temp\\ipykernel_16804\\485575046.py:104: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(forget_losses, kde=False, label='forget-loss')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR:0.45, FNR:0.47, FP10.00, TN12.00, TP20.00, FN18.00\n",
      "FPR:0.41, FNR:0.45, FP9.00, TN13.00, TP21.00, FN17.00\n",
      "FPR:0.50, FNR:0.50, FP12.00, TN12.00, TP18.00, FN18.00\n",
      "FPR:0.43, FNR:0.46, FP10.00, TN13.00, TP20.00, FN17.00\n",
      "FPR:0.38, FNR:0.44, FP8.00, TN13.00, TP22.00, FN17.00\n",
      "Attack accuracy for Fisher Model: 0.5466666666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def cm_score(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    cnf_matrix = confusion_matrix(y, y_pred)\n",
    "    \n",
    "    FP = cnf_matrix[0][1] \n",
    "    FN = cnf_matrix[1][0] \n",
    "    TP = cnf_matrix[0][0] \n",
    "    TN = cnf_matrix[1][1]\n",
    "\n",
    "\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP/(TP+FN)\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN/(TN+FP) \n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP/(TP+FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP/(TP+FP)\n",
    "\n",
    "    # Overall accuracy\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "    print (f\"FPR:{FPR:.2f}, FNR:{FNR:.2f}, FP{FP:.2f}, TN{TN:.2f}, TP{TP:.2f}, FN{FN:.2f}\")\n",
    "    return ACC\n",
    "\n",
    "\n",
    "def evaluate_attack_model(sample_loss,\n",
    "                          members,\n",
    "                          n_splits = 5,\n",
    "                          random_state = None):\n",
    "  \"\"\"Computes the cross-validation score of a membership inference attack.\n",
    "  Args:\n",
    "    sample_loss : array_like of shape (n,).\n",
    "      objective function evaluated on n samples.\n",
    "    members : array_like of shape (n,),\n",
    "      whether a sample was used for training.\n",
    "    n_splits: int\n",
    "      number of splits to use in the cross-validation.\n",
    "    random_state: int, RandomState instance or None, default=None\n",
    "      random state to use in cross-validation splitting.\n",
    "  Returns:\n",
    "    score : array_like of size (n_splits,)\n",
    "  \"\"\"\n",
    "\n",
    "  unique_members = np.unique(members)\n",
    "  if not np.all(unique_members == np.array([0, 1])):\n",
    "    raise ValueError(\"members should only have 0 and 1s\")\n",
    "\n",
    "  attack_model = LogisticRegression()\n",
    "  cv = StratifiedShuffleSplit(\n",
    "      n_splits=n_splits, random_state=random_state)\n",
    "  return cross_val_score(attack_model, sample_loss, members, cv=cv, scoring=cm_score)\n",
    "\n",
    "def membership_inference_attack(model, t_loader, f_loader, seed):\n",
    "    cr = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    test_losses = []\n",
    "    forget_losses = []\n",
    "    model.eval()\n",
    "\n",
    "    mult = 0.5 if args.lossfn == 'mse' else 1\n",
    "\n",
    "    # 获取测试集的losses\n",
    "    for data, target in t_loader:\n",
    "        data, target = data.to(args.device), target.to(args.device)\n",
    "        if args.lossfn == 'mse':\n",
    "            target = (2 * target - 1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        output = model(data)\n",
    "        loss = mult * cr(output, target)\n",
    "        test_losses.extend(loss.cpu().detach().numpy())\n",
    "\n",
    "    # 获取forget set的losses\n",
    "    for data, target in f_loader:\n",
    "        data, target = data.to(args.device), target.to(args.device)\n",
    "        if args.lossfn == 'mse':\n",
    "            target = (2 * target - 1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        output = model(data)\n",
    "        loss = mult * cr(output, target)\n",
    "        forget_losses.extend(loss.cpu().detach().numpy())\n",
    "\n",
    "    # 为了确保两者大小相同，进行采样\n",
    "    min_len = min(len(test_losses), len(forget_losses))\n",
    "    test_losses = np.random.choice(test_losses, min_len, replace=False)\n",
    "    forget_losses = np.random.choice(forget_losses, min_len, replace=False)\n",
    "\n",
    "    # 可视化\n",
    "    sns.distplot(test_losses, kde=False, label='test-loss')\n",
    "    sns.distplot(forget_losses, kde=False, label='forget-loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # 准备数据\n",
    "    all_losses = np.concatenate((test_losses, forget_losses)).reshape(-1, 1)\n",
    "    labels = np.array([0] * len(test_losses) + [1] * len(forget_losses))\n",
    "\n",
    "    # 评估\n",
    "    score = evaluate_attack_model(all_losses, labels, n_splits=5, random_state=seed)\n",
    "    return score\n",
    "\n",
    "# 进行模型的MIA评估\n",
    "for model_name, model_instance in zip(\n",
    "    [\"Original Model\", \"Negative Gradients Model\", \"Fisher Model\"],\n",
    "    [model_original, model_negative_gradient, model_fisher]\n",
    "):\n",
    "    print(f\"Evaluating MIA on {model_name}...\")\n",
    "    score = membership_inference_attack(model_instance, retain_loader, forget_loader, seed=42)\n",
    "    print(f\"Attack accuracy for {model_name}: {np.mean(score)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IC Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: mnist_mlp_1_0_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in mnist_mlp_1_0_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "confuse mode: True\n",
      "split mode: train\n",
      "[ 3986  4154  2840 30451 14265  8292 25600  1288 34049 35304 21351 12196\n",
      " 40669  2058 35787 45656 21336 42720 30978 13047  5133 29183 23030 15426\n",
      " 38880 41122 25965 44928 31896 21090  8439 24290 33564  9927 30135 18138\n",
      " 24758 20586 47548 35323 44096 12592 42739 16633 46931 31085 47894 43845\n",
      " 21884 39303 27099 34534 47955 25777  3350 36102  8575 16033 31756 12640\n",
      " 44326 20162 26729 16173 19616   907 20929  2479 33973  8169 14941 16983\n",
      " 19510 40484  8108 35470 36336  1289 11265  9524 21827 23144 16596 32410\n",
      "  8130 16407 15630 31796 31827 45221 30043 33736  2716 29201 38600 29204\n",
      " 46209  1268 30420  7989 36173  1375 40322  6569 45757  4529 35584 22202\n",
      " 20906  4268 37710  6079 17458 20178  7372 23555  9515 30129 41919 17180\n",
      " 17353 38538  7118 29440 14006 18383 37138 30358   698 33221  7383 24972\n",
      " 18463 45429 47227  7324  2837  7984 17817  5977  5323 21641 26046 20405\n",
      "  4224 20691  7898 42488 21131   412  2084 39607 32079 40810  6302  8679\n",
      "   272  5948 20066 33839 43985 11351 22757 19649  1671 22803 46894 32460\n",
      " 24277 13268 36490  3904 15963  9697 28565  1939 28441  5567  4596 20718\n",
      "  5619  7487 37910 45172 41334 15120  9199 18569 24151 16864 15954 22504\n",
      "  4934 25350 10866 47693 43629 27260 37655 21406   892 14018 25656 27394\n",
      " 19787 22062 22439 34329  7000 41566 13545 35663 30184 23932 18518 11111\n",
      " 19975 14423 26278 27866 17864 27346 29518 24527 19036 35907  8423 37647\n",
      " 40731  6539  5190 44342 11892 37578 29206 11885 39733 35274 13556 23923\n",
      " 23944 21530 26260 33534 29354 25696  1711 18582 30111  3305 43246  3104\n",
      " 41465  7208 28532 21742 35094 37886  2143 37076 27442 45018 17925 24470\n",
      " 35370 31711 34435 37837 47889  1349 10296 30119  4401 25201  8452  4022\n",
      "  9751 44968  1731 29438 20035 42381 31081 34629 36946 30869 21028 13509\n",
      " 24382 39498 31002 27357 30267 39458  3792 27570 20320 36226  2888 25479]\n",
      "Number of Classes: 10\n",
      "State OrderedDict([('layers.0.weight', tensor([[-0.0832, -0.0317,  0.0080,  ...,  0.0650, -0.0346, -0.0338],\n",
      "        [-0.0027, -0.0888, -0.0519,  ..., -0.0379,  0.0615,  0.0149],\n",
      "        [-0.1416, -0.0336,  0.0156,  ...,  0.0427,  0.0694,  0.0308],\n",
      "        ...,\n",
      "        [-0.0182, -0.0285,  0.0117,  ...,  0.0094, -0.0225, -0.0526],\n",
      "        [-0.0159,  0.0288, -0.0066,  ...,  0.0381,  0.0633,  0.0451],\n",
      "        [ 0.0131, -0.0608,  0.0516,  ..., -0.0093,  0.0415, -0.0130]])), ('layers.0.bias', tensor([ 0.2570,  0.5360, -0.9828, -0.2736, -0.2282,  0.3082,  0.4359, -0.1057,\n",
      "        -0.0857,  0.2738,  0.0112, -0.4476,  0.2455,  0.2458, -0.0411, -0.3458,\n",
      "        -0.4463,  0.1637, -0.5212,  0.0843,  0.1513, -0.1780, -0.6362,  0.2063,\n",
      "         0.4134,  0.2404, -0.3497, -0.2096, -0.0949, -0.2470,  0.0197,  0.4874])), ('layers.2.weight', tensor([[-0.3519, -0.2154, -0.3365, -0.1412,  0.4156,  0.1818,  0.0200, -0.0604,\n",
      "          0.2205,  0.2322, -0.0053, -0.1393, -0.2035, -0.0579,  0.0095, -0.1644,\n",
      "          0.2235,  0.3024,  0.2238, -0.7361,  0.2985,  0.1393,  0.1623, -0.0921,\n",
      "         -0.0720, -0.1394,  0.2537, -0.1786,  0.1402, -0.0607,  0.0045,  0.0832],\n",
      "        [ 0.3928, -0.1737, -0.1759, -0.4551,  0.2481, -0.4473, -0.0411, -0.2373,\n",
      "         -0.0809,  0.0645,  0.0849,  0.2062, -0.1421, -0.0616,  0.1801, -0.0840,\n",
      "         -0.2335, -0.0692,  0.1498, -0.0470, -0.1586, -0.1081,  0.0933, -0.2024,\n",
      "         -0.1436, -0.1707, -0.0688, -0.5912,  0.1219, -0.1256,  0.2843, -0.1910],\n",
      "        [ 0.4523, -0.3588,  0.0086,  0.0764,  0.1664,  0.4396,  0.0665,  0.4759,\n",
      "         -0.1288, -0.1356, -0.1838,  0.0269, -0.1764, -0.1239,  0.3496, -0.3435,\n",
      "          0.2170, -0.0702,  0.2358,  0.0587, -0.4519, -0.0437,  0.2893, -0.2968,\n",
      "          0.0510,  0.1216,  0.3035,  0.1648, -0.0432,  0.4584,  0.1138, -0.1609],\n",
      "        [-0.2260, -0.0343, -0.1534, -0.0083, -0.2569,  0.0590,  0.2407,  0.2322,\n",
      "         -0.0340,  0.2132,  0.1880,  0.2921, -0.0644, -0.0330,  0.4186,  0.4952,\n",
      "          0.0323,  0.1553, -0.1318, -0.1697,  0.2046, -0.2184,  0.0321, -0.3517,\n",
      "          0.3038, -0.0949, -0.1500,  0.1053, -0.0209, -0.1221, -0.1718, -0.0792],\n",
      "        [ 0.3664,  0.1804, -0.0120,  0.2229, -0.2088,  0.1287, -0.2451, -0.3353,\n",
      "          0.0897, -0.2442,  0.0131, -0.2590, -0.1019,  0.1473,  0.3632,  0.2329,\n",
      "          0.5305,  0.1337, -0.1440,  0.1620,  0.3382, -0.3171,  0.0806,  0.1881,\n",
      "          0.0109,  0.5281, -0.3445,  0.2525, -0.2200, -0.4710,  0.2022, -0.2743],\n",
      "        [ 0.1823, -0.0562, -0.2757, -0.0546, -0.1238, -0.1061,  0.0172, -0.0347,\n",
      "          0.4565,  0.1192, -0.0601,  0.2493, -0.2201,  0.0361,  0.0090,  0.2869,\n",
      "          0.0090, -0.2832, -0.0779, -0.2768, -0.3961, -0.1289,  0.1251, -0.0015,\n",
      "          0.2930,  0.1101,  0.0197,  0.2491,  0.0698,  0.3177,  0.0416,  0.0137],\n",
      "        [-0.3832,  0.2309, -0.1809,  0.1982,  0.0751,  0.0873, -0.1564,  0.0144,\n",
      "         -0.0449, -0.3521,  0.0023, -0.1973, -0.3833,  0.2016, -0.5147, -0.0191,\n",
      "          0.0122, -0.0809,  0.0038,  0.1643,  0.2994,  0.0281, -0.0899, -0.0444,\n",
      "          0.2220,  0.0072, -0.0604, -0.2090, -0.1963, -0.1330, -0.2076,  0.0046],\n",
      "        [-0.2165, -0.1100,  0.0688, -0.3747,  0.4035, -0.0398, -0.0595, -0.1467,\n",
      "         -0.0216,  0.1590, -0.0791, -0.5878,  0.2140,  0.0116,  0.0842,  0.0181,\n",
      "         -0.2411,  0.1744, -0.0815,  0.3069, -0.1610,  0.1093,  0.1149, -0.2111,\n",
      "         -0.4566, -0.0443,  0.1989, -0.1942, -0.2622, -0.1263,  0.3670, -0.2812],\n",
      "        [-0.1144,  0.1658, -0.1994,  0.2232, -0.0683,  0.2219,  0.2655,  0.4258,\n",
      "         -0.0326, -0.1462, -0.0584,  0.2980, -0.0663,  0.1896, -0.0486,  0.3528,\n",
      "         -0.2598,  0.0683,  0.2263, -0.3286,  0.0065, -0.0126, -0.1758,  0.2649,\n",
      "         -0.1740,  0.2994, -0.0692, -0.3554,  0.0484, -0.0219, -0.4493,  0.4110],\n",
      "        [ 0.4281, -0.0311, -0.3707,  0.5601, -0.2563, -0.1194, -0.2108, -0.0102,\n",
      "         -0.1516, -0.0340, -0.2634,  0.4547, -0.1523,  0.6067, -0.4058,  0.1322,\n",
      "         -0.3815,  0.2264, -0.0823, -0.2132, -0.0130,  0.1347,  0.1836,  0.1278,\n",
      "         -0.0120, -0.0764, -0.3119, -0.1457,  0.3770, -0.0323, -0.4128, -0.1316]])), ('layers.2.bias', tensor([ 0.2496,  0.6326,  0.5569,  0.1073,  0.2888, -0.0639,  0.1895,  0.2966,\n",
      "        -0.0117, -0.5746]))])\n",
      "Args checkpoints/standard_model_for_pilot.pt\n",
      "[0] train metrics:{\"loss\": 1.7112969087368348, \"error\": 0.4412548954253812}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 1.230843968963623, \"error\": 0.2177}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.87 sec\n",
      "[1] train metrics:{\"loss\": 1.1652259757027865, \"error\": 0.19196316973585534}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.72 sec\n",
      "[2] train metrics:{\"loss\": 1.0869950523238194, \"error\": 0.1581743188067661}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.7 sec\n",
      "[3] train metrics:{\"loss\": 1.0671998204643374, \"error\": 0.14673777185234563}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.71 sec\n",
      "[4] train metrics:{\"loss\": 1.0584365974395595, \"error\": 0.14023831347387716}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.82 sec\n",
      "[5] train metrics:{\"loss\": 1.0537598108154627, \"error\": 0.13667611032413965}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.7 sec\n",
      "[6] train metrics:{\"loss\": 1.0511841749699948, \"error\": 0.1345304557953504}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.67 sec\n",
      "[7] train metrics:{\"loss\": 1.0494198576081506, \"error\": 0.134051329055912}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.75 sec\n",
      "[8] train metrics:{\"loss\": 1.048275300805027, \"error\": 0.13275977001916506}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.81 sec\n",
      "[9] train metrics:{\"loss\": 1.0476790525527628, \"error\": 0.1321348220981585}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.76 sec\n",
      "[10] train metrics:{\"loss\": 1.047345990926442, \"error\": 0.1313432213982168}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.72 sec\n",
      "[11] train metrics:{\"loss\": 1.0468714847414506, \"error\": 0.13042663111407382}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.68 sec\n",
      "[12] train metrics:{\"loss\": 1.0465393338876112, \"error\": 0.1311974002166486}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.7 sec\n",
      "[13] train metrics:{\"loss\": 1.0462052244691489, \"error\": 0.13140571619031746}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.68 sec\n",
      "[14] train metrics:{\"loss\": 1.0462562588987405, \"error\": 0.1303016415298725}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.74 sec\n",
      "[15] train metrics:{\"loss\": 1.0458662120632982, \"error\": 0.1309265894508791}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.69 sec\n",
      "[16] train metrics:{\"loss\": 1.0458272729711864, \"error\": 0.13090575785351222}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.7 sec\n",
      "[17] train metrics:{\"loss\": 1.0457867211614507, \"error\": 0.1306141154903758}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.6 sec\n",
      "[18] train metrics:{\"loss\": 1.045779175822332, \"error\": 0.12992667277726855}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.67 sec\n",
      "[19] train metrics:{\"loss\": 1.0455922678444507, \"error\": 0.13038496791934007}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.74 sec\n",
      "[20] train metrics:{\"loss\": 1.0455267792313847, \"error\": 0.1300099991667361}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.73 sec\n",
      "[21] train metrics:{\"loss\": 1.0456099886713837, \"error\": 0.1306141154903758}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.61 sec\n",
      "[22] train metrics:{\"loss\": 1.0455496996981375, \"error\": 0.13040579951670694}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.76 sec\n",
      "[23] train metrics:{\"loss\": 1.045603650260514, \"error\": 0.1309265894508791}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.74 sec\n",
      "[24] train metrics:{\"loss\": 1.045598630383257, \"error\": 0.13007249395883677}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.87 sec\n",
      "[25] train metrics:{\"loss\": 1.0454931952100865, \"error\": 0.1304682943088076}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.84 sec\n",
      "[26] train metrics:{\"loss\": 1.0456488081022417, \"error\": 0.12992667277726855}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.68 sec\n",
      "[27] train metrics:{\"loss\": 1.0455399555679599, \"error\": 0.13013498875093743}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.61 sec\n",
      "[28] train metrics:{\"loss\": 1.0456268656264742, \"error\": 0.13059328389300892}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.69 sec\n",
      "[29] train metrics:{\"loss\": 1.0454424160161164, \"error\": 0.13040579951670694}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.64 sec\n",
      "[30] train metrics:{\"loss\": 1.0452252618571776, \"error\": 0.12948920923256396}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.71 sec\n",
      "Pure training time: 177.21 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run main.py --dataset mnist --model mlp --dataroot=data/MNIST/ --filters 1.0 --lr 0.001 \\\n",
    "--resume checkpoints/standard_model_for_pilot.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 --seed 3 \\\n",
    "--split train --confuse-mode --forget-class 0,1,2,3,4,5 --num-to-forget 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: mnist_mlp_1_0_forget_[0, 1, 2, 3, 4, 5]_num_300_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in mnist_mlp_1_0_forget_[0, 1, 2, 3, 4, 5]_num_300_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "confuse mode: True\n",
      "split mode: forget\n",
      "[ 3986  4154  2840 30451 14265  8292 25600  1288 34049 35304 21351 12196\n",
      " 40669  2058 35787 45656 21336 42720 30978 13047  5133 29183 23030 15426\n",
      " 38880 41122 25965 44928 31896 21090  8439 24290 33564  9927 30135 18138\n",
      " 24758 20586 47548 35323 44096 12592 42739 16633 46931 31085 47894 43845\n",
      " 21884 39303 27099 34534 47955 25777  3350 36102  8575 16033 31756 12640\n",
      " 44326 20162 26729 16173 19616   907 20929  2479 33973  8169 14941 16983\n",
      " 19510 40484  8108 35470 36336  1289 11265  9524 21827 23144 16596 32410\n",
      "  8130 16407 15630 31796 31827 45221 30043 33736  2716 29201 38600 29204\n",
      " 46209  1268 30420  7989 36173  1375 40322  6569 45757  4529 35584 22202\n",
      " 20906  4268 37710  6079 17458 20178  7372 23555  9515 30129 41919 17180\n",
      " 17353 38538  7118 29440 14006 18383 37138 30358   698 33221  7383 24972\n",
      " 18463 45429 47227  7324  2837  7984 17817  5977  5323 21641 26046 20405\n",
      "  4224 20691  7898 42488 21131   412  2084 39607 32079 40810  6302  8679\n",
      "   272  5948 20066 33839 43985 11351 22757 19649  1671 22803 46894 32460\n",
      " 24277 13268 36490  3904 15963  9697 28565  1939 28441  5567  4596 20718\n",
      "  5619  7487 37910 45172 41334 15120  9199 18569 24151 16864 15954 22504\n",
      "  4934 25350 10866 47693 43629 27260 37655 21406   892 14018 25656 27394\n",
      " 19787 22062 22439 34329  7000 41566 13545 35663 30184 23932 18518 11111\n",
      " 19975 14423 26278 27866 17864 27346 29518 24527 19036 35907  8423 37647\n",
      " 40731  6539  5190 44342 11892 37578 29206 11885 39733 35274 13556 23923\n",
      " 23944 21530 26260 33534 29354 25696  1711 18582 30111  3305 43246  3104\n",
      " 41465  7208 28532 21742 35094 37886  2143 37076 27442 45018 17925 24470\n",
      " 35370 31711 34435 37837 47889  1349 10296 30119  4401 25201  8452  4022\n",
      "  9751 44968  1731 29438 20035 42381 31081 34629 36946 30869 21028 13509\n",
      " 24382 39498 31002 27357 30267 39458  3792 27570 20320 36226  2888 25479]\n",
      "Number of Classes: 10\n",
      "State OrderedDict([('layers.0.weight', tensor([[-0.0832, -0.0317,  0.0080,  ...,  0.0650, -0.0346, -0.0338],\n",
      "        [-0.0027, -0.0888, -0.0519,  ..., -0.0379,  0.0615,  0.0149],\n",
      "        [-0.1416, -0.0336,  0.0156,  ...,  0.0427,  0.0694,  0.0308],\n",
      "        ...,\n",
      "        [-0.0182, -0.0285,  0.0117,  ...,  0.0094, -0.0225, -0.0526],\n",
      "        [-0.0159,  0.0288, -0.0066,  ...,  0.0381,  0.0633,  0.0451],\n",
      "        [ 0.0131, -0.0608,  0.0516,  ..., -0.0093,  0.0415, -0.0130]])), ('layers.0.bias', tensor([ 0.2570,  0.5360, -0.9828, -0.2736, -0.2282,  0.3082,  0.4359, -0.1057,\n",
      "        -0.0857,  0.2738,  0.0112, -0.4476,  0.2455,  0.2458, -0.0411, -0.3458,\n",
      "        -0.4463,  0.1637, -0.5212,  0.0843,  0.1513, -0.1780, -0.6362,  0.2063,\n",
      "         0.4134,  0.2404, -0.3497, -0.2096, -0.0949, -0.2470,  0.0197,  0.4874])), ('layers.2.weight', tensor([[-0.3519, -0.2154, -0.3365, -0.1412,  0.4156,  0.1818,  0.0200, -0.0604,\n",
      "          0.2205,  0.2322, -0.0053, -0.1393, -0.2035, -0.0579,  0.0095, -0.1644,\n",
      "          0.2235,  0.3024,  0.2238, -0.7361,  0.2985,  0.1393,  0.1623, -0.0921,\n",
      "         -0.0720, -0.1394,  0.2537, -0.1786,  0.1402, -0.0607,  0.0045,  0.0832],\n",
      "        [ 0.3928, -0.1737, -0.1759, -0.4551,  0.2481, -0.4473, -0.0411, -0.2373,\n",
      "         -0.0809,  0.0645,  0.0849,  0.2062, -0.1421, -0.0616,  0.1801, -0.0840,\n",
      "         -0.2335, -0.0692,  0.1498, -0.0470, -0.1586, -0.1081,  0.0933, -0.2024,\n",
      "         -0.1436, -0.1707, -0.0688, -0.5912,  0.1219, -0.1256,  0.2843, -0.1910],\n",
      "        [ 0.4523, -0.3588,  0.0086,  0.0764,  0.1664,  0.4396,  0.0665,  0.4759,\n",
      "         -0.1288, -0.1356, -0.1838,  0.0269, -0.1764, -0.1239,  0.3496, -0.3435,\n",
      "          0.2170, -0.0702,  0.2358,  0.0587, -0.4519, -0.0437,  0.2893, -0.2968,\n",
      "          0.0510,  0.1216,  0.3035,  0.1648, -0.0432,  0.4584,  0.1138, -0.1609],\n",
      "        [-0.2260, -0.0343, -0.1534, -0.0083, -0.2569,  0.0590,  0.2407,  0.2322,\n",
      "         -0.0340,  0.2132,  0.1880,  0.2921, -0.0644, -0.0330,  0.4186,  0.4952,\n",
      "          0.0323,  0.1553, -0.1318, -0.1697,  0.2046, -0.2184,  0.0321, -0.3517,\n",
      "          0.3038, -0.0949, -0.1500,  0.1053, -0.0209, -0.1221, -0.1718, -0.0792],\n",
      "        [ 0.3664,  0.1804, -0.0120,  0.2229, -0.2088,  0.1287, -0.2451, -0.3353,\n",
      "          0.0897, -0.2442,  0.0131, -0.2590, -0.1019,  0.1473,  0.3632,  0.2329,\n",
      "          0.5305,  0.1337, -0.1440,  0.1620,  0.3382, -0.3171,  0.0806,  0.1881,\n",
      "          0.0109,  0.5281, -0.3445,  0.2525, -0.2200, -0.4710,  0.2022, -0.2743],\n",
      "        [ 0.1823, -0.0562, -0.2757, -0.0546, -0.1238, -0.1061,  0.0172, -0.0347,\n",
      "          0.4565,  0.1192, -0.0601,  0.2493, -0.2201,  0.0361,  0.0090,  0.2869,\n",
      "          0.0090, -0.2832, -0.0779, -0.2768, -0.3961, -0.1289,  0.1251, -0.0015,\n",
      "          0.2930,  0.1101,  0.0197,  0.2491,  0.0698,  0.3177,  0.0416,  0.0137],\n",
      "        [-0.3832,  0.2309, -0.1809,  0.1982,  0.0751,  0.0873, -0.1564,  0.0144,\n",
      "         -0.0449, -0.3521,  0.0023, -0.1973, -0.3833,  0.2016, -0.5147, -0.0191,\n",
      "          0.0122, -0.0809,  0.0038,  0.1643,  0.2994,  0.0281, -0.0899, -0.0444,\n",
      "          0.2220,  0.0072, -0.0604, -0.2090, -0.1963, -0.1330, -0.2076,  0.0046],\n",
      "        [-0.2165, -0.1100,  0.0688, -0.3747,  0.4035, -0.0398, -0.0595, -0.1467,\n",
      "         -0.0216,  0.1590, -0.0791, -0.5878,  0.2140,  0.0116,  0.0842,  0.0181,\n",
      "         -0.2411,  0.1744, -0.0815,  0.3069, -0.1610,  0.1093,  0.1149, -0.2111,\n",
      "         -0.4566, -0.0443,  0.1989, -0.1942, -0.2622, -0.1263,  0.3670, -0.2812],\n",
      "        [-0.1144,  0.1658, -0.1994,  0.2232, -0.0683,  0.2219,  0.2655,  0.4258,\n",
      "         -0.0326, -0.1462, -0.0584,  0.2980, -0.0663,  0.1896, -0.0486,  0.3528,\n",
      "         -0.2598,  0.0683,  0.2263, -0.3286,  0.0065, -0.0126, -0.1758,  0.2649,\n",
      "         -0.1740,  0.2994, -0.0692, -0.3554,  0.0484, -0.0219, -0.4493,  0.4110],\n",
      "        [ 0.4281, -0.0311, -0.3707,  0.5601, -0.2563, -0.1194, -0.2108, -0.0102,\n",
      "         -0.1516, -0.0340, -0.2634,  0.4547, -0.1523,  0.6067, -0.4058,  0.1322,\n",
      "         -0.3815,  0.2264, -0.0823, -0.2132, -0.0130,  0.1347,  0.1836,  0.1278,\n",
      "         -0.0120, -0.0764, -0.3119, -0.1457,  0.3770, -0.0323, -0.4128, -0.1316]])), ('layers.2.bias', tensor([ 0.2496,  0.6326,  0.5569,  0.1073,  0.2888, -0.0639,  0.1895,  0.2966,\n",
      "        -0.0117, -0.5746]))])\n",
      "Args checkpoints/standard_model_for_pilot.pt\n",
      "[0] train metrics:{\"loss\": 1.6960258417234413, \"error\": 0.4373802183151404}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 1.221060030555725, \"error\": 0.216}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.84 sec\n",
      "[1] train metrics:{\"loss\": 1.1363842140067748, \"error\": 0.18523456378635114}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.83 sec\n",
      "[2] train metrics:{\"loss\": 1.0571164388317296, \"error\": 0.15217481876510291}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.69 sec\n",
      "[3] train metrics:{\"loss\": 1.0374555590947203, \"error\": 0.14044662944754605}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.75 sec\n",
      "[4] train metrics:{\"loss\": 1.0290112941823952, \"error\": 0.1337596866927756}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.76 sec\n",
      "[5] train metrics:{\"loss\": 1.0245170708289097, \"error\": 0.13034330472460628}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.74 sec\n",
      "[6] train metrics:{\"loss\": 1.021976387234551, \"error\": 0.1277601866511124}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.68 sec\n",
      "[7] train metrics:{\"loss\": 1.0202282388233541, \"error\": 0.1276351970669111}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.71 sec\n",
      "[8] train metrics:{\"loss\": 1.0191392157139574, \"error\": 0.12617698525122906}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.76 sec\n",
      "[9] train metrics:{\"loss\": 1.0184805124027114, \"error\": 0.1261353220564953}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.67 sec\n",
      "[10] train metrics:{\"loss\": 1.0181539320228556, \"error\": 0.12480209982501458}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.67 sec\n",
      "[11] train metrics:{\"loss\": 1.0176601340875815, \"error\": 0.12482293142238148}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.81 sec\n",
      "[12] train metrics:{\"loss\": 1.0172825151458897, \"error\": 0.12528122656445295}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.74 sec\n",
      "[13] train metrics:{\"loss\": 1.0169745795816771, \"error\": 0.12509374218815097}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.69 sec\n",
      "[14] train metrics:{\"loss\": 1.0170251586299948, \"error\": 0.12378135155403716}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.75 sec\n",
      "[15] train metrics:{\"loss\": 1.01663390371901, \"error\": 0.12455212065661195}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.7 sec\n",
      "[16] train metrics:{\"loss\": 1.0165768179056316, \"error\": 0.12459378385134572}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.74 sec\n",
      "[17] train metrics:{\"loss\": 1.016568478222003, \"error\": 0.12448962586451129}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.69 sec\n",
      "[18] train metrics:{\"loss\": 1.01652554520646, \"error\": 0.12380218315140405}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.69 sec\n",
      "[19] train metrics:{\"loss\": 1.0163531688320349, \"error\": 0.1247812682276477}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.73 sec\n",
      "[20] train metrics:{\"loss\": 1.0162719692173485, \"error\": 0.12411465711190735}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.76 sec\n",
      "[21] train metrics:{\"loss\": 1.0163686158617142, \"error\": 0.1246146154487126}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.74 sec\n",
      "[22] train metrics:{\"loss\": 1.016340909088843, \"error\": 0.12415632030664112}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.76 sec\n",
      "[23] train metrics:{\"loss\": 1.0163707015574173, \"error\": 0.12469794183818016}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.74 sec\n",
      "[24] train metrics:{\"loss\": 1.016346647812718, \"error\": 0.12436463628030997}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.72 sec\n",
      "[25] train metrics:{\"loss\": 1.0162441408661005, \"error\": 0.12378135155403716}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.82 sec\n",
      "[26] train metrics:{\"loss\": 1.0163302882970666, \"error\": 0.1240104991250729}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.72 sec\n",
      "[27] train metrics:{\"loss\": 1.0162988683003484, \"error\": 0.12378135155403716}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.76 sec\n",
      "[28] train metrics:{\"loss\": 1.016409379503606, \"error\": 0.12442713107241063}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.73 sec\n",
      "[29] train metrics:{\"loss\": 1.0162812492895321, \"error\": 0.12448962586451129}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.82 sec\n",
      "[30] train metrics:{\"loss\": 1.0161367770076126, \"error\": 0.12317723523039746}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.71 sec\n",
      "Pure training time: 177.81999999999996 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset mnist --model mlp --dataroot=data/MNIST/ --filters 1.0 --lr 0.001 \\\n",
    "--resume checkpoints/standard_model_for_pilot.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 --seed 3 \\\n",
    "--split forget --confuse-mode --forget-class 0,1,2,3,4,5 --num-to-forget 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 33130\n"
     ]
    }
   ],
   "source": [
    "parameter_count(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "confued_model_original = copy.deepcopy(model)\n",
    "confued_model_retain = copy.deepcopy(model)\n",
    "confued_model_pretrain = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntk_init(resume,seed=1):\n",
    "    manual_seed(seed)\n",
    "    model_init = models.get_model(arch, num_classes=num_classes, filters_percentage=filters).to(args.device)\n",
    "    model_init.load_state_dict(torch.load(resume))\n",
    "    return model_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict={}\n",
    "training_epochs=30\n",
    "\n",
    "log_dict['epoch']=training_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model0 = copy.deepcopy(model)\n",
    "model_initial = copy.deepcopy(model)\n",
    "\n",
    "arch = args.model \n",
    "filters=args.filters\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "augment = False\n",
    "dataset = args.dataset\n",
    "class_to_forget = args.forget_class\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "num_classes=args.num_classes\n",
    "num_to_forget = args.num_to_forget\n",
    "num_total = len(train_loader.dataset)\n",
    "num_to_retain = num_total - num_to_forget\n",
    "seed = args.seed\n",
    "unfreeze_start = None\n",
    "\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "\n",
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(m_name))\n",
    "model0.load_state_dict(torch.load(m0_name))\n",
    "model_initial.load_state_dict(torch.load(init_checkpoint))\n",
    "\n",
    "teacher = copy.deepcopy(model)\n",
    "student = copy.deepcopy(model)\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "for p in model0.parameters():\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['args']=args\n",
    "\n",
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "for p in model_init.parameters():\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confuse mode: True\n",
      "split mode: train\n",
      "[ 3986  4154  2840 30451 14265  8292 25600  1288 34049 35304 21351 12196\n",
      " 40669  2058 35787 45656 21336 42720 30978 13047  5133 29183 23030 15426\n",
      " 38880 41122 25965 44928 31896 21090  8439 24290 33564  9927 30135 18138\n",
      " 24758 20586 47548 35323 44096 12592 42739 16633 46931 31085 47894 43845\n",
      " 21884 39303 27099 34534 47955 25777  3350 36102  8575 16033 31756 12640\n",
      " 44326 20162 26729 16173 19616   907 20929  2479 33973  8169 14941 16983\n",
      " 19510 40484  8108 35470 36336  1289 11265  9524 21827 23144 16596 32410\n",
      "  8130 16407 15630 31796 31827 45221 30043 33736  2716 29201 38600 29204\n",
      " 46209  1268 30420  7989 36173  1375 40322  6569 45757  4529 35584 22202\n",
      " 20906  4268 37710  6079 17458 20178  7372 23555  9515 30129 41919 17180\n",
      " 17353 38538  7118 29440 14006 18383 37138 30358   698 33221  7383 24972\n",
      " 18463 45429 47227  7324  2837  7984 17817  5977  5323 21641 26046 20405\n",
      "  4224 20691  7898 42488 21131   412  2084 39607 32079 40810  6302  8679\n",
      "   272  5948 20066 33839 43985 11351 22757 19649  1671 22803 46894 32460\n",
      " 24277 13268 36490  3904 15963  9697 28565  1939 28441  5567  4596 20718\n",
      "  5619  7487 37910 45172 41334 15120  9199 18569 24151 16864 15954 22504\n",
      "  4934 25350 10866 47693 43629 27260 37655 21406   892 14018 25656 27394\n",
      " 19787 22062 22439 34329  7000 41566 13545 35663 30184 23932 18518 11111\n",
      " 19975 14423 26278 27866 17864 27346 29518 24527 19036 35907  8423 37647\n",
      " 40731  6539  5190 44342 11892 37578 29206 11885 39733 35274 13556 23923\n",
      " 23944 21530 26260 33534 29354 25696  1711 18582 30111  3305 43246  3104\n",
      " 41465  7208 28532 21742 35094 37886  2143 37076 27442 45018 17925 24470\n",
      " 35370 31711 34435 37837 47889  1349 10296 30119  4401 25201  8452  4022\n",
      "  9751 44968  1731 29438 20035 42381 31081 34629 36946 30869 21028 13509\n",
      " 24382 39498 31002 27357 30267 39458  3792 27570 20320 36226  2888 25479]\n",
      "confuse mode: True\n",
      "split mode: forget\n",
      "[ 3986  4154  2840 30451 14265  8292 25600  1288 34049 35304 21351 12196\n",
      " 40669  2058 35787 45656 21336 42720 30978 13047  5133 29183 23030 15426\n",
      " 38880 41122 25965 44928 31896 21090  8439 24290 33564  9927 30135 18138\n",
      " 24758 20586 47548 35323 44096 12592 42739 16633 46931 31085 47894 43845\n",
      " 21884 39303 27099 34534 47955 25777  3350 36102  8575 16033 31756 12640\n",
      " 44326 20162 26729 16173 19616   907 20929  2479 33973  8169 14941 16983\n",
      " 19510 40484  8108 35470 36336  1289 11265  9524 21827 23144 16596 32410\n",
      "  8130 16407 15630 31796 31827 45221 30043 33736  2716 29201 38600 29204\n",
      " 46209  1268 30420  7989 36173  1375 40322  6569 45757  4529 35584 22202\n",
      " 20906  4268 37710  6079 17458 20178  7372 23555  9515 30129 41919 17180\n",
      " 17353 38538  7118 29440 14006 18383 37138 30358   698 33221  7383 24972\n",
      " 18463 45429 47227  7324  2837  7984 17817  5977  5323 21641 26046 20405\n",
      "  4224 20691  7898 42488 21131   412  2084 39607 32079 40810  6302  8679\n",
      "   272  5948 20066 33839 43985 11351 22757 19649  1671 22803 46894 32460\n",
      " 24277 13268 36490  3904 15963  9697 28565  1939 28441  5567  4596 20718\n",
      "  5619  7487 37910 45172 41334 15120  9199 18569 24151 16864 15954 22504\n",
      "  4934 25350 10866 47693 43629 27260 37655 21406   892 14018 25656 27394\n",
      " 19787 22062 22439 34329  7000 41566 13545 35663 30184 23932 18518 11111\n",
      " 19975 14423 26278 27866 17864 27346 29518 24527 19036 35907  8423 37647\n",
      " 40731  6539  5190 44342 11892 37578 29206 11885 39733 35274 13556 23923\n",
      " 23944 21530 26260 33534 29354 25696  1711 18582 30111  3305 43246  3104\n",
      " 41465  7208 28532 21742 35094 37886  2143 37076 27442 45018 17925 24470\n",
      " 35370 31711 34435 37837 47889  1349 10296 30119  4401 25201  8452  4022\n",
      "  9751 44968  1731 29438 20035 42381 31081 34629 36946 30869 21028 13509\n",
      " 24382 39498 31002 27357 30267 39458  3792 27570 20320 36226  2888 25479]\n"
     ]
    }
   ],
   "source": [
    "args.retain_bs = 32\n",
    "args.forget_bs = 32\n",
    "\n",
    "train_loader_full, valid_loader_full, test_loader_full = datasets.get_loaders(dataset, split=\"train\",confuse_mode=True,class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, batch_size=args.batch_size, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "marked_loader, _, _ = datasets.get_loaders(dataset, split=\"forget\", confuse_mode=True,class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "\n",
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "    manual_seed(seed)\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "    def _init_fn(worker_id):\n",
    "        np.random.seed(int(seed))\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)\n",
    "    \n",
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = forget_dataset.targets < 0\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, batch_size=args.forget_bs, seed=seed, shuffle=True)\n",
    "\n",
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = retain_dataset.targets >= 0\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, batch_size=args.retain_bs, seed=seed, shuffle=True)\n",
    "\n",
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "47704\n",
      "10000\n",
      "48004\n",
      "{5: 4337, 4: 4674, 9: 4760, 2: 4767, 1: 5394, 3: 4905, 6: 4735, 7: 5012, 8: 4681, 0: 4739}\n"
     ]
    }
   ],
   "source": [
    "print (len(forget_loader.dataset))\n",
    "print (len(retain_loader.dataset))\n",
    "print (len(test_loader_full.dataset))\n",
    "print (len(train_loader_full.dataset))\n",
    "from collections import Counter\n",
    "print(dict(Counter(train_loader_full.dataset.targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_errors(model, test_loader):\n",
    "    IC_err_count = 0\n",
    "    FGT_err_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                if labels[i] in [0, 1]:\n",
    "                    total_count += 1\n",
    "                    if labels[i] == 0 and predicted[i] == 1:\n",
    "                        FGT_err_count += 1\n",
    "                    elif labels[i] == 1 and predicted[i] == 0:\n",
    "                        FGT_err_count += 1\n",
    "                    elif predicted[i] not in [0, 1]:\n",
    "                        IC_err_count += 1\n",
    "\n",
    "    IC_error = IC_err_count / total_count\n",
    "    FGT_error = FGT_err_count / total_count\n",
    "\n",
    "    return IC_error, FGT_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning ICtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c024850c4fe945dc82aeccd77f0b56f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb663b67af64bd6a57260bafbda83f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c739724724294676a2cebba15c52395b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911856931a804bf2a407dfd5c2e9c8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec0d386589247f4b39f81efe04f07b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591cac66cd3e479396b50f8d6b3eddec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ceb17832ce8460a89be4fb82967c274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c201db453d748ea83aa8911fd6e7dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf35ef5e4a8c43a6ab329f722a78fcba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378ac0970f7447b89262c8b4915a923f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_ft = copy.deepcopy(model)\n",
    "retain_loader = replace_loader_dataset(train_loader_full,retain_dataset, seed=seed, batch_size=args.batch_size, shuffle=True)\n",
    "finetune(model_ft, retain_loader, epochs=10, quiet=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IC error: 0.03\n",
      "FGT error: 0.00\n"
     ]
    }
   ],
   "source": [
    "model_ft.eval()  # Set model to evaluation mode\n",
    "IC_error, FGT_error = evaluate_errors(model_ft, test_loader_full)\n",
    "print(f'IC error: {IC_error:.2f}')\n",
    "print(f'FGT error: {FGT_error:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Gradients IC test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099704f275a2454190b9f2869028fa6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529210286c6348aba6b7ed78b72c1e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ff18efc68749f39ad300ec5ab05a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d946bae85cee492080f2c22d882bc87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea32fb58c664be194edae46d4efa8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2650ee346a1451ea31fc33fa92b8daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e9ddb1a3a74be59cb1724b7e51ed82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3dca97453a423cbf1ce049de07d6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c85af6aebb4d58bcdff03586f456fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5256a76622408d815f0836e004035a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args.ng_alpha = 0.95\n",
    "args.ng_epochs = 10\n",
    "args.ng_lr = 0.01\n",
    "model_negative_gradient = copy.deepcopy(model)\n",
    "negative_grad(model_negative_gradient, retain_loader, forget_loader, alpha=args.ng_alpha, epochs=args.ng_epochs, lr=args.ng_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IC error: 0.07\n",
      "FGT error: 0.00\n"
     ]
    }
   ],
   "source": [
    "model_negative_gradient.eval()  # Set model to evaluation mode\n",
    "IC_error, FGT_error = evaluate_errors(model_negative_gradient, test_loader_full)\n",
    "print(f'IC error: {IC_error:.2f}')\n",
    "print(f'FGT error: {FGT_error:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher IC test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fisher_on_original = copy.deepcopy(confued_model_original)\n",
    "model_fisher_on_pretrained = copy.deepcopy(confued_model_pretrain)\n",
    "for p in itertools.chain(model_fisher_on_original.parameters(), model_fisher_on_pretrained.parameters()):\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa2658f85e54d5ea9314120917e2587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03226edf652a4c139ed7cad1d15eeec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hessian(retain_loader.dataset, model_fisher_on_original)\n",
    "hessian(retain_loader.dataset, model_fisher_on_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.weight 0.0\n",
      "layers.0.bias 0.0\n",
      "layers.2.weight 0.0\n",
      "layers.2.bias 0.0\n",
      "Total: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computes the amount of information not forgotten at all layers using the given alpha\n",
    "\n",
    "alpha = 1e-7\n",
    "total_kl = 0\n",
    "torch.manual_seed(seed)\n",
    "for (k, p), (k0, p0) in zip(model_fisher_on_original.named_parameters(), model_fisher_on_pretrained.named_parameters()):\n",
    "    mu0, var0 = get_mean_var(p, False, alpha=alpha)\n",
    "    mu1, var1 = get_mean_var(p0, True, alpha=alpha)\n",
    "    kl = kl_divergence_fisher(mu0, var0, mu1, var1).item()\n",
    "    total_kl += kl\n",
    "    print(k, f'{kl:.1f}')\n",
    "print(\"Total:\", total_kl)\n",
    "log_dict['fisher_info']=total_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_dir = []\n",
    "alpha = 1e-6\n",
    "torch.manual_seed(seed)\n",
    "for i, p in enumerate(model_fisher_on_original.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "    fisher_dir.append(var.sqrt().view(-1).cpu().detach().numpy())\n",
    "for i, p in enumerate(model_fisher_on_pretrained.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.02581587, 0.02581587, 0.02581587, ..., 0.02661954, 0.02661954,\n",
      "       0.02661954], dtype=float32), array([0.01122092, 0.01923348, 0.1       , 0.01317423, 0.01423648,\n",
      "       0.01723084, 0.01900835, 0.01376374, 0.02644168, 0.01514601,\n",
      "       0.1       , 0.1       , 0.02217704, 0.01600816, 0.01258303,\n",
      "       0.01440648, 0.01446286, 0.01838717, 0.1       , 0.01329374,\n",
      "       0.01358323, 0.1       , 0.05454868, 0.01591179, 0.01379216,\n",
      "       0.01614189, 0.01870186, 0.01820244, 0.03445135, 0.01571004,\n",
      "       0.0147932 , 0.01502657], dtype=float32), array([0.02056376, 0.02056376, 0.02056376, 0.02056376, 0.02056376,\n",
      "       0.02056376, 0.02056376, 0.02056376, 0.02056376, 0.02056376,\n",
      "       0.02056376, 0.02056376, 0.02056376, 0.02056376, 0.02056376,\n",
      "       0.02056376, 0.02056376, 0.02056376, 0.02056376, 0.02056376,\n",
      "       0.02056376, 0.02056376, 0.02056376, 0.02056376, 0.02056376,\n",
      "       0.02056376, 0.02056376, 0.02056376, 0.02056376, 0.02056376,\n",
      "       0.02056376, 0.02056376, 0.02426456, 0.02426456, 0.02426456,\n",
      "       0.02426456, 0.02426456, 0.02426456, 0.02426456, 0.02426456,\n",
      "       0.02426456, 0.02426456, 0.02426456, 0.02426456, 0.02426456,\n",
      "       0.02426456, 0.02426456, 0.02426456, 0.02426456, 0.02426456,\n",
      "       0.02426456, 0.02426456, 0.02426456, 0.02426456, 0.02426456,\n",
      "       0.02426456, 0.02426456, 0.02426456, 0.02426456, 0.02426456,\n",
      "       0.02426456, 0.02426456, 0.02426456, 0.02426456, 0.02051161,\n",
      "       0.02051161, 0.02051161, 0.02051161, 0.02051161, 0.02051161,\n",
      "       0.02051161, 0.02051161, 0.02051161, 0.02051161, 0.02051161,\n",
      "       0.02051161, 0.02051161, 0.02051161, 0.02051161, 0.02051161,\n",
      "       0.02051161, 0.02051161, 0.02051161, 0.02051161, 0.02051161,\n",
      "       0.02051161, 0.02051161, 0.02051161, 0.02051161, 0.02051161,\n",
      "       0.02051161, 0.02051161, 0.02051161, 0.02051161, 0.02051161,\n",
      "       0.02051161, 0.01929601, 0.01929601, 0.01929601, 0.01929601,\n",
      "       0.01929601, 0.01929601, 0.01929601, 0.01929601, 0.01929601,\n",
      "       0.01929601, 0.01929601, 0.01929601, 0.01929601, 0.01929601,\n",
      "       0.01929601, 0.01929601, 0.01929601, 0.01929601, 0.01929601,\n",
      "       0.01929601, 0.01929601, 0.01929601, 0.01929601, 0.01929601,\n",
      "       0.01929601, 0.01929601, 0.01929601, 0.01929601, 0.01929601,\n",
      "       0.01929601, 0.01929601, 0.01929601, 0.02008264, 0.02008264,\n",
      "       0.02008264, 0.02008264, 0.02008264, 0.02008264, 0.02008264,\n",
      "       0.02008264, 0.02008264, 0.02008264, 0.02008264, 0.02008264,\n",
      "       0.02008264, 0.02008264, 0.02008264, 0.02008264, 0.02008264,\n",
      "       0.02008264, 0.02008264, 0.02008264, 0.02008264, 0.02008264,\n",
      "       0.02008264, 0.02008264, 0.02008264, 0.02008264, 0.02008264,\n",
      "       0.02008264, 0.02008264, 0.02008264, 0.02008264, 0.02008264,\n",
      "       0.01840107, 0.01840107, 0.01840107, 0.01840107, 0.01840107,\n",
      "       0.01840107, 0.01840107, 0.01840107, 0.01840107, 0.01840107,\n",
      "       0.01840107, 0.01840107, 0.01840107, 0.01840107, 0.01840107,\n",
      "       0.01840107, 0.01840107, 0.01840107, 0.01840107, 0.01840107,\n",
      "       0.01840107, 0.01840107, 0.01840107, 0.01840107, 0.01840107,\n",
      "       0.01840107, 0.01840107, 0.01840107, 0.01840107, 0.01840107,\n",
      "       0.01840107, 0.01840107, 0.02129888, 0.02129888, 0.02129888,\n",
      "       0.02129888, 0.02129888, 0.02129888, 0.02129888, 0.02129888,\n",
      "       0.02129888, 0.02129888, 0.02129888, 0.02129888, 0.02129888,\n",
      "       0.02129888, 0.02129888, 0.02129888, 0.02129888, 0.02129888,\n",
      "       0.02129888, 0.02129888, 0.02129888, 0.02129888, 0.02129888,\n",
      "       0.02129888, 0.02129888, 0.02129888, 0.02129888, 0.02129888,\n",
      "       0.02129888, 0.02129888, 0.02129888, 0.02129888, 0.021647  ,\n",
      "       0.021647  , 0.021647  , 0.021647  , 0.021647  , 0.021647  ,\n",
      "       0.021647  , 0.021647  , 0.021647  , 0.021647  , 0.021647  ,\n",
      "       0.021647  , 0.021647  , 0.021647  , 0.021647  , 0.021647  ,\n",
      "       0.021647  , 0.021647  , 0.021647  , 0.021647  , 0.021647  ,\n",
      "       0.021647  , 0.021647  , 0.021647  , 0.021647  , 0.021647  ,\n",
      "       0.021647  , 0.021647  , 0.021647  , 0.021647  , 0.021647  ,\n",
      "       0.021647  , 0.01868634, 0.01868634, 0.01868634, 0.01868634,\n",
      "       0.01868634, 0.01868634, 0.01868634, 0.01868634, 0.01868634,\n",
      "       0.01868634, 0.01868634, 0.01868634, 0.01868634, 0.01868634,\n",
      "       0.01868634, 0.01868634, 0.01868634, 0.01868634, 0.01868634,\n",
      "       0.01868634, 0.01868634, 0.01868634, 0.01868634, 0.01868634,\n",
      "       0.01868634, 0.01868634, 0.01868634, 0.01868634, 0.01868634,\n",
      "       0.01868634, 0.01868634, 0.01868634, 0.01989676, 0.01989676,\n",
      "       0.01989676, 0.01989676, 0.01989676, 0.01989676, 0.01989676,\n",
      "       0.01989676, 0.01989676, 0.01989676, 0.01989676, 0.01989676,\n",
      "       0.01989676, 0.01989676, 0.01989676, 0.01989676, 0.01989676,\n",
      "       0.01989676, 0.01989676, 0.01989676, 0.01989676, 0.01989676,\n",
      "       0.01989676, 0.01989676, 0.01989676, 0.01989676, 0.01989676,\n",
      "       0.01989676, 0.01989676, 0.01989676, 0.01989676, 0.01989676],\n",
      "      dtype=float32), array([0.01696932, 0.01545449, 0.01406354, 0.01354938, 0.01366788,\n",
      "       0.01337327, 0.0155437 , 0.01522121, 0.01201078, 0.01318083],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(fisher_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fisher = copy.deepcopy(model_fisher_on_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IC error: 0.05\n",
      "FGT error: 0.00\n"
     ]
    }
   ],
   "source": [
    "model_fisher.eval()  # Set model to evaluation mode\n",
    "IC_error, FGT_error = evaluate_errors(model_fisher, test_loader_full)\n",
    "print(f'IC error: {IC_error:.2f}')\n",
    "print(f'FGT error: {FGT_error:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## CF-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CF_K(model: nn.Module, data_loader: torch.utils.data.DataLoader, args, lr=0.01, epochs=10):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "        run_CF_K_epoch(model, model_init, data_loader, loss_fn, optimizer, epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_CF_K_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader,\n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, epoch: int):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    with torch.set_grad_enabled(True):\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF-K is not applicable\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[66], line 19\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m     18\u001B[0m     \u001B[39mprint\u001B[39m(\u001B[39m\"\u001B[39m\u001B[39mCF-K is not applicable\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[1;32m---> 19\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mNotImplementedError\u001B[39;00m\n\u001B[0;32m     21\u001B[0m CF_K(model_cfk, retain_loader, args\u001B[39m=\u001B[39margs, epochs\u001B[39m=\u001B[39margs\u001B[39m.\u001B[39mcfk_epochs, lr\u001B[39m=\u001B[39margs\u001B[39m.\u001B[39mcfk_lr)\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "args.lr_decay_epochs = [10,15,20]\n",
    "args.cfk_lr = 0.01\n",
    "args.cfk_epochs = 10\n",
    "model_cfk = copy.deepcopy(model)\n",
    "\n",
    "for param in model_cfk.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "if args.model == 'allcnn':\n",
    "    layers = [9]\n",
    "    for k in layers:\n",
    "        for param in model_cfk.features[k].parameters():\n",
    "            param.requires_grad_(True)\n",
    "elif args.model == \"resnet\":\n",
    "    for param in model_cfk.layer4.parameters():\n",
    "        param.requires_grad_(True)\n",
    "else:\n",
    "    print(\"CF-K is not applicable\")\n",
    "    raise NotImplementedError\n",
    "\n",
    "CF_K(model_cfk, retain_loader, args=args, epochs=args.cfk_epochs, lr=args.cfk_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## EU-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_eu_k_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader,\n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    negative_gradient=False, negative_multiplier=-1, random_labels=False,\n",
    "                    quiet=False,delta_w=None,scrub_act=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            if split=='test' and scrub_act:\n",
    "                G = []\n",
    "                for cls in range(num_classes):\n",
    "                    grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                    grads = torch.cat([g.view(-1) for g in grads])\n",
    "                    G.append(grads)\n",
    "                grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "                G = torch.stack(G).pow(2)\n",
    "                delta_f = torch.matmul(G,delta_w)\n",
    "                output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "\n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def EU_K(model: nn.Module, data_loader: torch.utils.data.DataLoader, args, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "        run_eu_k_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args.lr_decay_epochs = [10,15,20]\n",
    "args.euk_lr = 0.01\n",
    "args.euk_epochs = training_epochs\n",
    "model_euk = copy.deepcopy(model)\n",
    "\n",
    "for param in model_euk.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "if args.model == 'allcnn':\n",
    "    with torch.no_grad():\n",
    "        for k in layers:\n",
    "            for i in range(0,3):\n",
    "                try:\n",
    "                    model_euk.features[k][i].weight.copy_(model_initial.features[k][i].weight)\n",
    "                except:\n",
    "                    print (\"block {}, layer {} does not have weights\".format(k,i))\n",
    "                try:\n",
    "                    model_euk.features[k][i].bias.copy_(model_initial.features[k][i].bias)\n",
    "                except:\n",
    "                    print (\"block {}, layer {} does not have bias\".format(k,i))\n",
    "        model_euk.classifier[0].weight.copy_(model_initial.classifier[0].weight)\n",
    "        model_euk.classifier[0].bias.copy_(model_initial.classifier[0].bias)\n",
    "\n",
    "    for k in layers:\n",
    "        for param in model_euk.features[k].parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "elif args.model == \"resnet\":\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,2):\n",
    "            try:\n",
    "                model_euk.layer4[i].bn1.weight.copy_(model_initial.layer4[i].bn1.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].bn1.bias.copy_(model_initial.layer4[i].bn1.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv1.weight.copy_(model_initial.layer4[i].conv1.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv1.bias.copy_(model_initial.layer4[i].conv1.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "\n",
    "            try:\n",
    "                model_euk.layer4[i].bn2.weight.copy_(model_initial.layer4[i].bn2.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].bn2.bias.copy_(model_initial.layer4[i].bn2.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv2.weight.copy_(model_initial.layer4[i].conv2.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv2.bias.copy_(model_initial.layer4[i].conv2.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "\n",
    "        model_euk.layer4[0].shortcut[0].weight.copy_(model_initial.layer4[0].shortcut[0].weight)\n",
    "\n",
    "    for param in model_euk.layer4.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "else:\n",
    "    print(\"EU_K is not applicable\")\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EU_K(model_euk, retain_loader, epochs=args.euk_epochs, quiet=True, lr=args.euk_lr, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## SCRUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args.optim = 'adam'\n",
    "args.gamma = 1\n",
    "args.alpha = 0.5\n",
    "args.beta = 0\n",
    "args.smoothing = 0.5\n",
    "args.msteps = 3\n",
    "args.clip = 0.2\n",
    "args.sstart = 10\n",
    "args.kd_T = 2\n",
    "args.distill = 'kd'\n",
    "\n",
    "args.sgda_epochs = 10\n",
    "args.sgda_learning_rate = 0.0005\n",
    "args.lr_decay_epochs = [5,8,9]\n",
    "args.lr_decay_rate = 0.1\n",
    "args.sgda_weight_decay = 0.1#5e-4\n",
    "args.sgda_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "teacher = copy.deepcopy(model)\n",
    "student = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_t = copy.deepcopy(teacher)\n",
    "model_s = copy.deepcopy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta = 0.1\n",
    "def avg_fn(averaged_model_parameter, model_parameter, num_averaged): return (\n",
    "    1 - beta) * averaged_model_parameter + beta * model_parameter\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(\n",
    "    model_s, avg_fn=avg_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "module_list = nn.ModuleList([])\n",
    "module_list.append(model_s)\n",
    "trainable_list = nn.ModuleList([])\n",
    "trainable_list.append(model_s)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_div = DistillKL(args.kd_T)\n",
    "criterion_kd = DistillKL(args.kd_T)\n",
    "\n",
    "\n",
    "criterion_list = nn.ModuleList([])\n",
    "criterion_list.append(criterion_cls)    # classification loss\n",
    "criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
    "criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "\n",
    "# optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = optim.SGD(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"adam\":\n",
    "    optimizer = optim.Adam(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"rmsp\":\n",
    "    optimizer = optim.RMSprop(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "module_list.append(model_t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    module_list.cuda()\n",
    "    criterion_list.cuda()\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True\n",
    "    swa_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_rs = []\n",
    "acc_fs = []\n",
    "acc_ts = []\n",
    "for epoch in range(1, args.sgda_epochs + 1):\n",
    "\n",
    "    lr = sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "\n",
    "    print(\"==> scrub unlearning ...\")\n",
    "\n",
    "    acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True)\n",
    "    acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True)\n",
    "    acc_rs.append(100-acc_r.item())\n",
    "    acc_fs.append(100-acc_f.item())\n",
    "\n",
    "    maximize_loss = 0\n",
    "    if epoch <= args.msteps:\n",
    "        maximize_loss = train_distill(epoch, forget_loader, module_list, swa_model, criterion_list, optimizer, args, \"maximize\")\n",
    "    train_acc, train_loss = train_distill(epoch, retain_loader, module_list, swa_model, criterion_list, optimizer, args, \"minimize\",)\n",
    "    if epoch >= args.sstart:\n",
    "        swa_model.update_parameters(model_s)\n",
    "\n",
    "    print (\"maximize loss: {:.2f}\\t minimize loss: {:.2f}\\t train_acc: {}\".format(maximize_loss, train_loss, train_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "indices = list(range(0,len(acc_rs)))\n",
    "plt.plot(indices, acc_rs, marker='*', alpha=1, label='retain-set')\n",
    "plt.plot(indices, acc_fs, marker='o', alpha=1, label='forget-set')\n",
    "plt.legend(prop={'size': 14})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.title('scrub retain- and forget- set error',size=18)\n",
    "plt.xlabel('epoch',size=14)\n",
    "plt.ylabel('error',size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
