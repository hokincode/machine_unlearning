{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mw/2gycpkq57p97cpwvmwvm69m00000gn/T/ipykernel_5756/1595774001.py:24: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import variational\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from itertools import cycle\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from typing import List\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "from models import *\n",
    "import models\n",
    "from logger import *\n",
    "import wandb\n",
    "\n",
    "from thirdparty.repdistiller.helper.util import adjust_learning_rate as sgda_adjust_learning_rate\n",
    "from thirdparty.repdistiller.distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss\n",
    "from thirdparty.repdistiller.distiller_zoo import PKT, ABLoss, FactorTransfer, KDSVD, FSP, NSTLoss\n",
    "\n",
    "from thirdparty.repdistiller.helper.loops import train_distill, train_distill_hide, train_distill_linear, train_vanilla, train_negrad, train_bcu, train_bcu_distill, validate\n",
    "from thirdparty.repdistiller.helper.pretrain import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb():\n",
    "    import pdb\n",
    "    pdb.set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_count(model):\n",
    "    count=0\n",
    "    for p in model.parameters():\n",
    "        count+=np.prod(np.array(list(p.shape)))\n",
    "    print(f'Total Number of Parameters: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_params(model):\n",
    "    param = []\n",
    "    for p in model.parameters():\n",
    "        param.append(p.data.view(-1).cpu().numpy())\n",
    "    return np.concatenate(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_shape(model):\n",
    "    for k,p in model.named_parameters():\n",
    "        print(k,p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the original model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: small_binary_mnist_mlp_1_0_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in small_binary_mnist_mlp_1_0_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 2\n",
      "State OrderedDict([('layers.0.weight', tensor([[-0.0832, -0.0317,  0.0080,  ...,  0.0650, -0.0346, -0.0338],\n",
      "        [-0.0027, -0.0888, -0.0519,  ..., -0.0379,  0.0615,  0.0149],\n",
      "        [-0.1416, -0.0336,  0.0156,  ...,  0.0427,  0.0694,  0.0308],\n",
      "        ...,\n",
      "        [-0.0182, -0.0285,  0.0117,  ...,  0.0094, -0.0225, -0.0526],\n",
      "        [-0.0159,  0.0288, -0.0066,  ...,  0.0381,  0.0633,  0.0451],\n",
      "        [ 0.0131, -0.0608,  0.0516,  ..., -0.0093,  0.0415, -0.0130]])), ('layers.0.bias', tensor([ 0.2570,  0.5360, -0.9828, -0.2736, -0.2282,  0.3082,  0.4359, -0.1057,\n",
      "        -0.0857,  0.2738,  0.0112, -0.4476,  0.2455,  0.2458, -0.0411, -0.3458,\n",
      "        -0.4463,  0.1637, -0.5212,  0.0843,  0.1513, -0.1780, -0.6362,  0.2063,\n",
      "         0.4134,  0.2404, -0.3497, -0.2096, -0.0949, -0.2470,  0.0197,  0.4874])), ('layers.2.weight', tensor([[ 0.4165,  0.1779,  0.1134,  0.2528, -0.2743, -0.2084, -0.2076,  0.1134,\n",
      "          0.4338,  0.3145, -0.2518,  0.0770,  0.2922, -0.0379,  0.0544, -0.1496,\n",
      "          0.1223,  0.0839,  0.2992, -0.0547, -0.1302,  0.0199,  0.0938, -0.4739,\n",
      "          0.1300, -0.1818,  0.0711,  0.0949,  0.0130, -0.2929, -0.0437,  0.0628],\n",
      "        [ 0.4588,  0.1392,  0.0478, -0.1891,  0.1087, -0.1973,  0.0503,  0.2438,\n",
      "          0.2012,  0.0677, -0.3998, -0.2727,  0.2041,  0.5477, -0.2385,  0.1783,\n",
      "         -0.0714, -0.0116, -0.1500, -0.1541,  0.3122,  0.2019,  0.0899,  0.0779,\n",
      "          0.1333,  0.3276, -0.1887, -0.1601,  0.1590,  0.1680,  0.3795, -0.0016]])), ('layers.2.bias', tensor([0.8454, 0.3357]))])\n",
      "Args checkpoints/standard_model_for_small_binary_mnist.pt\n",
      "[0] train metrics:{\"loss\": 0.7348050475120544, \"error\": 0.5625}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 0.7059895572905196, \"error\": 0.5166995656382082}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 1.29 sec\n",
      "[1] train metrics:{\"loss\": 0.6976857542991638, \"error\": 0.46875}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[2] train metrics:{\"loss\": 0.6434240579605103, \"error\": 0.278125}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[3] train metrics:{\"loss\": 0.5851557970046997, \"error\": 0.125}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[4] train metrics:{\"loss\": 0.5324300050735473, \"error\": 0.059375}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[5] train metrics:{\"loss\": 0.48600354194641116, \"error\": 0.040625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[6] train metrics:{\"loss\": 0.44455034732818605, \"error\": 0.01875}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[7] train metrics:{\"loss\": 0.4067540466785431, \"error\": 0.009375}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[8] train metrics:{\"loss\": 0.3712169885635376, \"error\": 0.00625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[9] train metrics:{\"loss\": 0.3381021022796631, \"error\": 0.003125}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[10] train metrics:{\"loss\": 0.30804163813591, \"error\": 0.003125}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[11] train metrics:{\"loss\": 0.28126220107078553, \"error\": 0.003125}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[12] train metrics:{\"loss\": 0.25700636208057404, \"error\": 0.003125}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[13] train metrics:{\"loss\": 0.2357402354478836, \"error\": 0.003125}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[14] train metrics:{\"loss\": 0.21733598709106444, \"error\": 0.00625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[15] train metrics:{\"loss\": 0.20113636255264283, \"error\": 0.00625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[16] train metrics:{\"loss\": 0.18722088038921356, \"error\": 0.00625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[17] train metrics:{\"loss\": 0.1751156359910965, \"error\": 0.00625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[18] train metrics:{\"loss\": 0.16444210410118104, \"error\": 0.00625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[19] train metrics:{\"loss\": 0.15532595813274383, \"error\": 0.00625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[20] train metrics:{\"loss\": 0.14715788066387175, \"error\": 0.00625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[21] train metrics:{\"loss\": 0.1401567727327347, \"error\": 0.00625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[22] train metrics:{\"loss\": 0.13383355587720872, \"error\": 0.00625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.03 sec\n",
      "[23] train metrics:{\"loss\": 0.12822587192058563, \"error\": 0.00625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[24] train metrics:{\"loss\": 0.12330527901649475, \"error\": 0.00625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[25] train metrics:{\"loss\": 0.11894703358411789, \"error\": 0.00625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[26] train metrics:{\"loss\": 0.11494856476783752, \"error\": 0.00625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[27] train metrics:{\"loss\": 0.11140622198581696, \"error\": 0.00625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[28] train metrics:{\"loss\": 0.10821071714162826, \"error\": 0.00625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[29] train metrics:{\"loss\": 0.10528613924980164, \"error\": 0.00625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[30] train metrics:{\"loss\": 0.10267640203237534, \"error\": 0.00625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "Pure training time: 1.2300000000000004 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset small_binary_mnist --model mlp --dataroot=data/small_binary_mnist/ --filters 1.0 --lr 0.001 \\\n",
    "--resume checkpoints/standard_model_for_small_binary_mnist.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 --seed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: small_binary_mnist_mlp_1_0_forget_[0, 1, 2, 3, 4, 5]_num_300_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in small_binary_mnist_mlp_1_0_forget_[0, 1, 2, 3, 4, 5]_num_300_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Replacing indexes [166 190   7  65  98 197 312 128 160  84 240  67  99 180 231  29  94 282\n",
      " 302 174 256 292 286  13   3 141 315 169  30 244  68 193 106 304 175 194\n",
      "  35 151 154  10  60 214 241 118 242 289 109 192 257  66 183  12 308  89\n",
      " 127  77 215  74  41  11 179 274 114  53 226 152  91 172   2 178 258 313\n",
      " 155 176  23 216 153 266 173 237 222 189 268 161  54   4 290 170 140 264\n",
      " 184 163 235 104  24 297   6 273 147  20  28 142 112 204 279  71 144 296\n",
      " 250 126  64  14 131 134  18  85 318 253  25 284 103 220 223 309 120 277\n",
      "  69 188 133  44 113  93 101   5 157 130  79 115 181 199 117 305 280 156\n",
      " 300 198 225  90 272 301   9  37 317  45 164 191 261 316 209  48  42 265\n",
      " 159  57 145 122 182 105 288 203 278 228 291   1 139 111 303 262 107   8\n",
      " 267 298 210 230  21 295 171 236 239 119  32 311 206  78 275 243  46 108\n",
      "  40 281 129  82 217 135 201 285 110 310 234  36  92 186 232  62   0  27\n",
      " 100 177 187 249 148  63  39 269 205 158 143  17  59 165 245  70 259 208\n",
      " 221 276 149 123  55  16 224 219 227 251 200 202 294 307 270 150  87 283\n",
      " 212 146  97 229 125  61  81 319  56  19  86 246 137  34 138 254  76  26\n",
      "  43  96  83 252 185  73 248 247 233 271 136  49 207 306 132  50 238  52\n",
      "  80  31 121 211 196  88 255  58  33  38  51 260]\n",
      "Number of Classes: 2\n",
      "State OrderedDict([('layers.0.weight', tensor([[-0.0832, -0.0317,  0.0080,  ...,  0.0650, -0.0346, -0.0338],\n",
      "        [-0.0027, -0.0888, -0.0519,  ..., -0.0379,  0.0615,  0.0149],\n",
      "        [-0.1416, -0.0336,  0.0156,  ...,  0.0427,  0.0694,  0.0308],\n",
      "        ...,\n",
      "        [-0.0182, -0.0285,  0.0117,  ...,  0.0094, -0.0225, -0.0526],\n",
      "        [-0.0159,  0.0288, -0.0066,  ...,  0.0381,  0.0633,  0.0451],\n",
      "        [ 0.0131, -0.0608,  0.0516,  ..., -0.0093,  0.0415, -0.0130]])), ('layers.0.bias', tensor([ 0.2570,  0.5360, -0.9828, -0.2736, -0.2282,  0.3082,  0.4359, -0.1057,\n",
      "        -0.0857,  0.2738,  0.0112, -0.4476,  0.2455,  0.2458, -0.0411, -0.3458,\n",
      "        -0.4463,  0.1637, -0.5212,  0.0843,  0.1513, -0.1780, -0.6362,  0.2063,\n",
      "         0.4134,  0.2404, -0.3497, -0.2096, -0.0949, -0.2470,  0.0197,  0.4874])), ('layers.2.weight', tensor([[ 0.4165,  0.1779,  0.1134,  0.2528, -0.2743, -0.2084, -0.2076,  0.1134,\n",
      "          0.4338,  0.3145, -0.2518,  0.0770,  0.2922, -0.0379,  0.0544, -0.1496,\n",
      "          0.1223,  0.0839,  0.2992, -0.0547, -0.1302,  0.0199,  0.0938, -0.4739,\n",
      "          0.1300, -0.1818,  0.0711,  0.0949,  0.0130, -0.2929, -0.0437,  0.0628],\n",
      "        [ 0.4588,  0.1392,  0.0478, -0.1891,  0.1087, -0.1973,  0.0503,  0.2438,\n",
      "          0.2012,  0.0677, -0.3998, -0.2727,  0.2041,  0.5477, -0.2385,  0.1783,\n",
      "         -0.0714, -0.0116, -0.1500, -0.1541,  0.3122,  0.2019,  0.0899,  0.0779,\n",
      "          0.1333,  0.3276, -0.1887, -0.1601,  0.1590,  0.1680,  0.3795, -0.0016]])), ('layers.2.bias', tensor([0.8454, 0.3357]))])\n",
      "Args checkpoints/standard_model_for_small_binary_mnist.pt\n",
      "[0] train metrics:{\"loss\": 0.764635968208313, \"error\": 0.853125}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 0.7086560794453446, \"error\": 0.5121989732714826}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 1.34 sec\n",
      "[1] train metrics:{\"loss\": 0.7314006209373474, \"error\": 0.71875}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.03 sec\n",
      "[2] train metrics:{\"loss\": 0.6761159896850586, \"error\": 0.4375}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[3] train metrics:{\"loss\": 0.6149368166923523, \"error\": 0.125}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[4] train metrics:{\"loss\": 0.5548787236213684, \"error\": 0.040625}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[5] train metrics:{\"loss\": 0.5010664284229278, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[6] train metrics:{\"loss\": 0.452588415145874, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[7] train metrics:{\"loss\": 0.4067587792873383, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[8] train metrics:{\"loss\": 0.3658595681190491, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[9] train metrics:{\"loss\": 0.33003453612327577, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[10] train metrics:{\"loss\": 0.2985000491142273, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[11] train metrics:{\"loss\": 0.27125226259231566, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[12] train metrics:{\"loss\": 0.24813926219940186, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[13] train metrics:{\"loss\": 0.22868556082248687, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[14] train metrics:{\"loss\": 0.21167129576206206, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[15] train metrics:{\"loss\": 0.19694797396659852, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[16] train metrics:{\"loss\": 0.18392377197742463, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[17] train metrics:{\"loss\": 0.17259916365146638, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.03 sec\n",
      "[18] train metrics:{\"loss\": 0.1627109557390213, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[19] train metrics:{\"loss\": 0.1539686530828476, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[20] train metrics:{\"loss\": 0.1460461437702179, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[21] train metrics:{\"loss\": 0.13884915113449098, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[22] train metrics:{\"loss\": 0.13232580125331878, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[23] train metrics:{\"loss\": 0.12654609382152557, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.03 sec\n",
      "[24] train metrics:{\"loss\": 0.12120856046676635, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[25] train metrics:{\"loss\": 0.1163624107837677, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[26] train metrics:{\"loss\": 0.11194890439510345, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[27] train metrics:{\"loss\": 0.10800713896751404, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.03 sec\n",
      "[28] train metrics:{\"loss\": 0.10436969995498657, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[29] train metrics:{\"loss\": 0.10109763145446778, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "[30] train metrics:{\"loss\": 0.09809654653072357, \"error\": 0.0}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 0.04 sec\n",
      "Pure training time: 1.1700000000000004 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset small_binary_mnist --model mlp --dataroot=data/small_binary_mnist/ --filters 1.0 --lr 0.001 \\\n",
    "--resume checkpoints/standard_model_for_small_binary_mnist.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 \\\n",
    "--forget-class 0,1,2,3,4,5 --num-to-forget 300 --seed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "log_dict={}\n",
    "training_epochs=30"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.8337, 0.3474], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.8324, 0.3487], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "model0 = copy.deepcopy(model)\n",
    "model_initial = copy.deepcopy(model)\n",
    "\n",
    "arch = args.model \n",
    "filters=args.filters\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "augment = False\n",
    "dataset = args.dataset\n",
    "class_to_forget = args.forget_class\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "num_classes=args.num_classes\n",
    "num_to_forget = args.num_to_forget\n",
    "num_total = len(train_loader.dataset)\n",
    "num_to_retain = num_total - 300#num_to_forget\n",
    "seed = args.seed\n",
    "unfreeze_start = None\n",
    "\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "\n",
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(m_name))\n",
    "model0.load_state_dict(torch.load(m0_name))\n",
    "model_initial.load_state_dict(torch.load(init_checkpoint))\n",
    "\n",
    "teacher = copy.deepcopy(model)\n",
    "student = copy.deepcopy(model)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "print(p)\n",
    "for p in model0.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "log_dict['epoch']=training_epochs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 32866\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameter_count(copy.deepcopy(model))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loads checkpoints"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['args']=args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between w(D) and w(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(model,model0):\n",
    "    distance=0\n",
    "    normalization=0\n",
    "    print(\"model.named_parameters\",model.named_parameters)\n",
    "    print(\"model0.named_parameters\",model0.named_parameters)\n",
    "    print(\"For Loop\")\n",
    "    for (k, p), (k0, p0) in zip(model.named_parameters(), model0.named_parameters()):\n",
    "        space='  ' if 'bias' in k else ''\n",
    "        print(\"p.data0\", p.data0)\n",
    "        print(\"p0.data0\", p0.data0)\n",
    "        current_dist=(p.data0-p0.data0).pow(2).sum().item()\n",
    "        current_norm=p.data0.pow(2).sum().item()\n",
    "        distance+=current_dist\n",
    "        normalization+=current_norm\n",
    "    print(f'Distance: {np.sqrt(distance)}')\n",
    "    print(f'Normalized Distance: {1.0*np.sqrt(distance/normalization)}')\n",
    "    return 1.0*np.sqrt(distance/normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.named_parameters <bound method Module.named_parameters of MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): StandardLinearLayer(in_features=1024, out_features=32, bias=True, beta=0.31622776601683794)\n",
      "    (1): ReLU()\n",
      "    (2): StandardLinearLayer(in_features=32, out_features=2, bias=True, beta=0.31622776601683794)\n",
      "  )\n",
      ")>\n",
      "model0.named_parameters <bound method Module.named_parameters of MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): StandardLinearLayer(in_features=1024, out_features=32, bias=True, beta=0.31622776601683794)\n",
      "    (1): ReLU()\n",
      "    (2): StandardLinearLayer(in_features=32, out_features=2, bias=True, beta=0.31622776601683794)\n",
      "  )\n",
      ")>\n",
      "For Loop\n",
      "p.data0 tensor([[-0.0832, -0.0317,  0.0080,  ...,  0.0650, -0.0346, -0.0338],\n",
      "        [-0.0027, -0.0888, -0.0519,  ..., -0.0379,  0.0615,  0.0149],\n",
      "        [-0.1416, -0.0336,  0.0156,  ...,  0.0427,  0.0694,  0.0308],\n",
      "        ...,\n",
      "        [-0.0182, -0.0285,  0.0117,  ...,  0.0094, -0.0225, -0.0526],\n",
      "        [-0.0159,  0.0288, -0.0066,  ...,  0.0381,  0.0633,  0.0451],\n",
      "        [ 0.0131, -0.0608,  0.0516,  ..., -0.0093,  0.0415, -0.0130]])\n",
      "p0.data0 tensor([[-0.0832, -0.0317,  0.0080,  ...,  0.0650, -0.0346, -0.0338],\n",
      "        [-0.0027, -0.0888, -0.0519,  ..., -0.0379,  0.0615,  0.0149],\n",
      "        [-0.1416, -0.0336,  0.0156,  ...,  0.0427,  0.0694,  0.0308],\n",
      "        ...,\n",
      "        [-0.0182, -0.0285,  0.0117,  ...,  0.0094, -0.0225, -0.0526],\n",
      "        [-0.0159,  0.0288, -0.0066,  ...,  0.0381,  0.0633,  0.0451],\n",
      "        [ 0.0131, -0.0608,  0.0516,  ..., -0.0093,  0.0415, -0.0130]])\n",
      "p.data0 tensor([ 0.2573,  0.5358, -0.9828, -0.2594, -0.2223,  0.3082,  0.4459, -0.1046,\n",
      "        -0.0916,  0.2712,  0.0109, -0.4475,  0.2409,  0.2732, -0.0356, -0.3446,\n",
      "        -0.4416,  0.1619, -0.5208,  0.0908,  0.1772, -0.1774, -0.6360,  0.2282,\n",
      "         0.4131,  0.2646, -0.3471, -0.2092, -0.0994, -0.2479,  0.0205,  0.4859])\n",
      "p0.data0 tensor([ 0.2575,  0.5358, -0.9828, -0.2659, -0.2171,  0.3086,  0.4474, -0.1030,\n",
      "        -0.0924,  0.2691,  0.0136, -0.4476,  0.2406,  0.2759, -0.0313, -0.3413,\n",
      "        -0.4418,  0.1609, -0.5212,  0.0890,  0.1799, -0.1780, -0.6359,  0.2403,\n",
      "         0.4134,  0.2656, -0.3478, -0.2070, -0.0991, -0.2470,  0.0382,  0.4859])\n",
      "p.data0 tensor([[ 0.4304,  0.1446,  0.1134,  0.3006, -0.2773, -0.2068, -0.2603,  0.1142,\n",
      "          0.4275,  0.3203, -0.2517,  0.0773,  0.2834, -0.0622,  0.1230, -0.1492,\n",
      "          0.1332,  0.0875,  0.2993, -0.0157, -0.1979,  0.0197,  0.0979, -0.4876,\n",
      "          0.1752, -0.2437,  0.0729,  0.0949,  0.0204, -0.2926, -0.0454,  0.0349],\n",
      "        [ 0.4449,  0.1725,  0.0478, -0.2369,  0.1116, -0.1989,  0.1030,  0.2430,\n",
      "          0.2074,  0.0619, -0.3998, -0.2730,  0.2129,  0.5720, -0.3072,  0.1779,\n",
      "         -0.0823, -0.0152, -0.1501, -0.1931,  0.3798,  0.2021,  0.0858,  0.0916,\n",
      "          0.0881,  0.3896, -0.1904, -0.1601,  0.1516,  0.1678,  0.3812,  0.0263]])\n",
      "p0.data0 tensor([[ 0.4306,  0.1573,  0.1134,  0.2736, -0.2818, -0.2101, -0.2624,  0.1148,\n",
      "          0.4232,  0.3182, -0.2482,  0.0770,  0.2799, -0.0625,  0.1383, -0.1508,\n",
      "          0.1284,  0.0868,  0.2992, -0.0145, -0.1892,  0.0199,  0.0986, -0.5099,\n",
      "          0.1740, -0.2340,  0.0727,  0.0965,  0.0255, -0.2929, -0.0556,  0.0296],\n",
      "        [ 0.4447,  0.1598,  0.0478, -0.2099,  0.1161, -0.1957,  0.1051,  0.2424,\n",
      "          0.2118,  0.0641, -0.4033, -0.2727,  0.2164,  0.5723, -0.3224,  0.1795,\n",
      "         -0.0776, -0.0146, -0.1500, -0.1943,  0.3712,  0.2019,  0.0851,  0.1139,\n",
      "          0.0892,  0.3799, -0.1903, -0.1617,  0.1464,  0.1680,  0.3914,  0.0316]])\n",
      "p.data0 tensor([0.8337, 0.3474])\n",
      "p0.data0 tensor([0.8324, 0.3487])\n",
      "Distance: 0.2552348965471117\n",
      "Normalized Distance: 0.02987307784490596\n"
     ]
    }
   ],
   "source": [
    "log_dict['dist_Original_Retrain']=distance(model,model0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance of w(D) from initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntk_init(resume,seed=1):\n",
    "    manual_seed(seed)\n",
    "    model_init = models.get_model(arch, num_classes=num_classes, filters_percentage=filters).to(args.device)\n",
    "    model_init.load_state_dict(torch.load(resume))\n",
    "    return model_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "for p in model_init.parameters():\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.named_parameters <bound method Module.named_parameters of MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): StandardLinearLayer(in_features=1024, out_features=32, bias=True, beta=0.31622776601683794)\n",
      "    (1): ReLU()\n",
      "    (2): StandardLinearLayer(in_features=32, out_features=2, bias=True, beta=0.31622776601683794)\n",
      "  )\n",
      ")>\n",
      "model0.named_parameters <bound method Module.named_parameters of MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): StandardLinearLayer(in_features=1024, out_features=32, bias=True, beta=0.31622776601683794)\n",
      "    (1): ReLU()\n",
      "    (2): StandardLinearLayer(in_features=32, out_features=2, bias=True, beta=0.31622776601683794)\n",
      "  )\n",
      ")>\n",
      "For Loop\n",
      "p.data0 tensor([[-0.0832, -0.0317,  0.0080,  ...,  0.0650, -0.0346, -0.0338],\n",
      "        [-0.0027, -0.0888, -0.0519,  ..., -0.0379,  0.0615,  0.0149],\n",
      "        [-0.1416, -0.0336,  0.0156,  ...,  0.0427,  0.0694,  0.0308],\n",
      "        ...,\n",
      "        [-0.0182, -0.0285,  0.0117,  ...,  0.0094, -0.0225, -0.0526],\n",
      "        [-0.0159,  0.0288, -0.0066,  ...,  0.0381,  0.0633,  0.0451],\n",
      "        [ 0.0131, -0.0608,  0.0516,  ..., -0.0093,  0.0415, -0.0130]])\n",
      "p0.data0 tensor([[-0.0832, -0.0317,  0.0080,  ...,  0.0650, -0.0346, -0.0338],\n",
      "        [-0.0027, -0.0888, -0.0519,  ..., -0.0379,  0.0615,  0.0149],\n",
      "        [-0.1416, -0.0336,  0.0156,  ...,  0.0427,  0.0694,  0.0308],\n",
      "        ...,\n",
      "        [-0.0182, -0.0285,  0.0117,  ...,  0.0094, -0.0225, -0.0526],\n",
      "        [-0.0159,  0.0288, -0.0066,  ...,  0.0381,  0.0633,  0.0451],\n",
      "        [ 0.0131, -0.0608,  0.0516,  ..., -0.0093,  0.0415, -0.0130]])\n",
      "p.data0 tensor([ 0.2570,  0.5360, -0.9828, -0.2736, -0.2282,  0.3082,  0.4359, -0.1057,\n",
      "        -0.0857,  0.2738,  0.0112, -0.4476,  0.2455,  0.2458, -0.0411, -0.3458,\n",
      "        -0.4463,  0.1637, -0.5212,  0.0843,  0.1513, -0.1780, -0.6362,  0.2063,\n",
      "         0.4134,  0.2404, -0.3497, -0.2096, -0.0949, -0.2470,  0.0197,  0.4874])\n",
      "p0.data0 tensor([ 0.2573,  0.5358, -0.9828, -0.2594, -0.2223,  0.3082,  0.4459, -0.1046,\n",
      "        -0.0916,  0.2712,  0.0109, -0.4475,  0.2409,  0.2732, -0.0356, -0.3446,\n",
      "        -0.4416,  0.1619, -0.5208,  0.0908,  0.1772, -0.1774, -0.6360,  0.2282,\n",
      "         0.4131,  0.2646, -0.3471, -0.2092, -0.0994, -0.2479,  0.0205,  0.4859])\n",
      "p.data0 tensor([[ 0.4165,  0.1779,  0.1134,  0.2528, -0.2743, -0.2084, -0.2076,  0.1134,\n",
      "          0.4338,  0.3145, -0.2518,  0.0770,  0.2922, -0.0379,  0.0544, -0.1496,\n",
      "          0.1223,  0.0839,  0.2992, -0.0547, -0.1302,  0.0199,  0.0938, -0.4739,\n",
      "          0.1300, -0.1818,  0.0711,  0.0949,  0.0130, -0.2929, -0.0437,  0.0628],\n",
      "        [ 0.4588,  0.1392,  0.0478, -0.1891,  0.1087, -0.1973,  0.0503,  0.2438,\n",
      "          0.2012,  0.0677, -0.3998, -0.2727,  0.2041,  0.5477, -0.2385,  0.1783,\n",
      "         -0.0714, -0.0116, -0.1500, -0.1541,  0.3122,  0.2019,  0.0899,  0.0779,\n",
      "          0.1333,  0.3276, -0.1887, -0.1601,  0.1590,  0.1680,  0.3795, -0.0016]])\n",
      "p0.data0 tensor([[ 0.4304,  0.1446,  0.1134,  0.3006, -0.2773, -0.2068, -0.2603,  0.1142,\n",
      "          0.4275,  0.3203, -0.2517,  0.0773,  0.2834, -0.0622,  0.1230, -0.1492,\n",
      "          0.1332,  0.0875,  0.2993, -0.0157, -0.1979,  0.0197,  0.0979, -0.4876,\n",
      "          0.1752, -0.2437,  0.0729,  0.0949,  0.0204, -0.2926, -0.0454,  0.0349],\n",
      "        [ 0.4449,  0.1725,  0.0478, -0.2369,  0.1116, -0.1989,  0.1030,  0.2430,\n",
      "          0.2074,  0.0619, -0.3998, -0.2730,  0.2129,  0.5720, -0.3072,  0.1779,\n",
      "         -0.0823, -0.0152, -0.1501, -0.1931,  0.3798,  0.2021,  0.0858,  0.0916,\n",
      "          0.0881,  0.3896, -0.1904, -0.1601,  0.1516,  0.1678,  0.3812,  0.0263]])\n",
      "p.data0 tensor([0.8454, 0.3357])\n",
      "p0.data0 tensor([0.8337, 0.3474])\n",
      "Distance: 0.6565052016377277\n",
      "Normalized Distance: 0.07722519553914614\n"
     ]
    }
   ],
   "source": [
    "log_dict['dist_Original_Original_init']=distance(model_init,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.retain_bs = 32\n",
    "args.forget_bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confuse mode: False\n",
      "split mode: train\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Replacing indexes [166 190   7  65  98 197 312 128 160  84 240  67  99 180 231  29  94 282\n",
      " 302 174 256 292 286  13   3 141 315 169  30 244  68 193 106 304 175 194\n",
      "  35 151 154  10  60 214 241 118 242 289 109 192 257  66 183  12 308  89\n",
      " 127  77 215  74  41  11 179 274 114  53 226 152  91 172   2 178 258 313\n",
      " 155 176  23 216 153 266 173 237 222 189 268 161  54   4 290 170 140 264\n",
      " 184 163 235 104  24 297   6 273 147  20  28 142 112 204 279  71 144 296\n",
      " 250 126  64  14 131 134  18  85 318 253  25 284 103 220 223 309 120 277\n",
      "  69 188 133  44 113  93 101   5 157 130  79 115 181 199 117 305 280 156\n",
      " 300 198 225  90 272 301   9  37 317  45 164 191 261 316 209  48  42 265\n",
      " 159  57 145 122 182 105 288 203 278 228 291   1 139 111 303 262 107   8\n",
      " 267 298 210 230  21 295 171 236 239 119  32 311 206  78 275 243  46 108\n",
      "  40 281 129  82 217 135 201 285 110 310 234  36  92 186 232  62   0  27\n",
      " 100 177 187 249 148  63  39 269 205 158 143  17  59 165 245  70 259 208\n",
      " 221 276 149 123  55  16 224 219 227 251 200 202 294 307 270 150  87 283\n",
      " 212 146  97 229 125  61  81 319  56  19  86 246 137  34 138 254  76  26\n",
      "  43  96  83 252 185  73 248 247 233 271 136  49 207 306 132  50 238  52\n",
      "  80  31 121 211 196  88 255  58  33  38  51 260]\n"
     ]
    }
   ],
   "source": [
    "train_loader_full, valid_loader_full, test_loader_full   = datasets.get_loaders(dataset, batch_size=args.batch_size, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "marked_loader, _, _ = datasets.get_loaders(dataset, class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "\n",
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "    manual_seed(seed)\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "    def _init_fn(worker_id):\n",
    "        np.random.seed(int(seed))\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)\n",
    "    \n",
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = forget_dataset.targets < 0\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, batch_size=args.forget_bs, seed=seed, shuffle=True)\n",
    "\n",
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = retain_dataset.targets >= 0\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, batch_size=args.retain_bs, seed=seed, shuffle=True)\n",
    "\n",
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "20\n",
      "12665\n",
      "320\n",
      "{0: 160, 1: 160}\n"
     ]
    }
   ],
   "source": [
    "print (len(forget_loader.dataset))\n",
    "print (len(retain_loader.dataset))\n",
    "print (len(test_loader_full.dataset))\n",
    "print (len(train_loader_full.dataset))\n",
    "from collections import Counter\n",
    "print(dict(Counter(train_loader_full.dataset.targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.optim = 'adam'\n",
    "args.gamma = 1\n",
    "args.alpha = 0.5\n",
    "args.beta = 0\n",
    "args.smoothing = 0.5\n",
    "args.msteps = 3\n",
    "args.clip = 0.2\n",
    "args.sstart = 10\n",
    "args.kd_T = 2\n",
    "args.distill = 'kd'\n",
    "\n",
    "args.sgda_epochs = 10\n",
    "args.sgda_learning_rate = 0.0005\n",
    "args.lr_decay_epochs = [5,8,9]\n",
    "args.lr_decay_rate = 0.1\n",
    "args.sgda_weight_decay = 0.1#5e-4\n",
    "args.sgda_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = copy.deepcopy(teacher)\n",
    "model_s = copy.deepcopy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is from https://github.com/ojus1/SmoothedGradientDescentAscent/blob/main/SGDA.py\n",
    "#For SGDA smoothing\n",
    "beta = 0.1\n",
    "def avg_fn(averaged_model_parameter, model_parameter, num_averaged): return (\n",
    "    1 - beta) * averaged_model_parameter + beta * model_parameter\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(\n",
    "    model_s, avg_fn=avg_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = nn.ModuleList([])\n",
    "module_list.append(model_s)\n",
    "trainable_list = nn.ModuleList([])\n",
    "trainable_list.append(model_s)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_div = DistillKL(args.kd_T)\n",
    "criterion_kd = DistillKL(args.kd_T)\n",
    "\n",
    "\n",
    "criterion_list = nn.ModuleList([])\n",
    "criterion_list.append(criterion_cls)    # classification loss\n",
    "criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
    "criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "\n",
    "# optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = optim.SGD(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"adam\": \n",
    "    optimizer = optim.Adam(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"rmsp\":\n",
    "    optimizer = optim.RMSprop(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list.append(model_t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    module_list.cuda()\n",
    "    criterion_list.cuda()\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True\n",
    "    swa_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> scrub unlearning ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "selected index k out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 10\u001B[0m\n\u001B[1;32m      6\u001B[0m lr \u001B[38;5;241m=\u001B[39m sgda_adjust_learning_rate(epoch, args, optimizer)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m==> scrub unlearning ...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 10\u001B[0m acc_r, acc5_r, loss_r \u001B[38;5;241m=\u001B[39m validate(retain_loader, model_s, criterion_cls, args, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     11\u001B[0m acc_f, acc5_f, loss_f \u001B[38;5;241m=\u001B[39m validate(forget_loader, model_s, criterion_cls, args, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     12\u001B[0m acc_rs\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;241m100\u001B[39m\u001B[38;5;241m-\u001B[39macc_r\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[0;32m~/PycharmProjects/machine_unlearning/thirdparty/repdistiller/helper/loops.py:1015\u001B[0m, in \u001B[0;36mvalidate\u001B[0;34m(val_loader, model, criterion, opt, quiet)\u001B[0m\n\u001B[1;32m   1012\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output, target)\n\u001B[1;32m   1014\u001B[0m \u001B[38;5;66;03m# measure accuracy and record loss\u001B[39;00m\n\u001B[0;32m-> 1015\u001B[0m acc1, acc5 \u001B[38;5;241m=\u001B[39m accuracy(output, target, topk\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m5\u001B[39m))\n\u001B[1;32m   1016\u001B[0m losses\u001B[38;5;241m.\u001B[39mupdate(loss\u001B[38;5;241m.\u001B[39mitem(), \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m))\n\u001B[1;32m   1017\u001B[0m top1\u001B[38;5;241m.\u001B[39mupdate(acc1[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m))\n",
      "File \u001B[0;32m~/PycharmProjects/machine_unlearning/thirdparty/repdistiller/helper/util.py:57\u001B[0m, in \u001B[0;36maccuracy\u001B[0;34m(output, target, topk)\u001B[0m\n\u001B[1;32m     54\u001B[0m maxk \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(topk)\n\u001B[1;32m     55\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m target\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 57\u001B[0m _, pred \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mtopk(maxk, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     58\u001B[0m pred \u001B[38;5;241m=\u001B[39m pred\u001B[38;5;241m.\u001B[39mt()\n\u001B[1;32m     59\u001B[0m correct \u001B[38;5;241m=\u001B[39m pred\u001B[38;5;241m.\u001B[39meq(target\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mexpand_as(pred))\n",
      "\u001B[0;31mRuntimeError\u001B[0m: selected index k out of range"
     ]
    }
   ],
   "source": [
    "acc_rs = []\n",
    "acc_fs = []\n",
    "acc_ts = []\n",
    "for epoch in range(1, args.sgda_epochs + 1):\n",
    "\n",
    "    lr = sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "\n",
    "    print(\"==> scrub unlearning ...\")\n",
    "\n",
    "    acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True)\n",
    "    acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True)\n",
    "    acc_rs.append(100-acc_r.item())\n",
    "    acc_fs.append(100-acc_f.item())\n",
    "\n",
    "    maximize_loss = 0\n",
    "    if epoch <= args.msteps:\n",
    "        maximize_loss = train_distill(epoch, forget_loader, module_list, swa_model, criterion_list, optimizer, args, \"maximize\")\n",
    "    train_acc, train_loss = train_distill(epoch, retain_loader, module_list, swa_model, criterion_list, optimizer, args, \"minimize\",)\n",
    "    if epoch >= args.sstart:\n",
    "        swa_model.update_parameters(model_s)\n",
    "\n",
    "    print (\"maximize loss: {:.2f}\\t minimize loss: {:.2f}\\t train_acc: {}\".format(maximize_loss, train_loss, train_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "indices = list(range(0,len(acc_rs)))\n",
    "plt.plot(indices, acc_rs, marker='*', alpha=1, label='retain-set')\n",
    "plt.plot(indices, acc_fs, marker='o', alpha=1, label='forget-set')\n",
    "plt.legend(prop={'size': 14})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.title('scrub retain- and forget- set error',size=18)\n",
    "plt.xlabel('epoch',size=14)\n",
    "plt.ylabel('error',size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTK based Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NTK Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_w_utils(model_init,dataloader,name='complete'):\n",
    "    model_init.eval()\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    G_list = []\n",
    "    f0_minus_y = []\n",
    "    for idx, batch in enumerate(dataloader):#(tqdm(dataloader,leave=False)):\n",
    "        print(\"One iteration:\", time.time())\n",
    "        batch = [tensor.to(next(model_init.parameters()).device) for tensor in batch]\n",
    "        input, target = batch\n",
    "        if 'mnist' in args.dataset:\n",
    "            input = input.view(input.shape[0],-1)\n",
    "        target = target.cpu().detach().numpy()\n",
    "        output = model_init(input)\n",
    "        G_sample=[]\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output[0,cls],model_init.parameters(),retain_graph=True)\n",
    "            grads = np.concatenate([g.view(-1).cpu().numpy() for g in grads])\n",
    "            G_sample.append(grads)\n",
    "            G_list.append(grads)\n",
    "        if args.lossfn=='mse':\n",
    "            p = output.cpu().detach().numpy().transpose()\n",
    "            #loss_hess = np.eye(len(p))\n",
    "            target = 2*target-1\n",
    "            f0_y_update = p-target\n",
    "        elif args.lossfn=='ce':\n",
    "            p = torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().transpose()\n",
    "            p[target]-=1\n",
    "            f0_y_update = copy.deepcopy(p)\n",
    "        f0_minus_y.append(f0_y_update)\n",
    "    return np.stack(G_list).transpose(),np.vstack(f0_minus_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobians and Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntk_time = 0\n",
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "t1 = time.time()\n",
    "G_r,f0_minus_y_r = delta_w_utils(copy.deepcopy(model),retain_loader,'complete')\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/G_r.npy',G_r)\n",
    "np.save('NTK_data/f0_minus_y_r.npy',f0_minus_y_r)\n",
    "del G_r, f0_minus_y_r\n",
    "\n",
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "t1 = time.time()\n",
    "G_f,f0_minus_y_f = delta_w_utils(copy.deepcopy(model),forget_loader,'retain') \n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/G_f.npy',G_f)\n",
    "np.save('NTK_data/f0_minus_y_f.npy',f0_minus_y_f)\n",
    "del G_f, f0_minus_y_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "G_f = np.load('NTK_data/G_f.npy')\n",
    "G = np.concatenate([G_r,G_f],axis=1)\n",
    "\n",
    "np.save('NTK_data/G.npy',G)\n",
    "del G, G_f, G_r\n",
    "\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "f0_minus_y_f = np.load('NTK_data/f0_minus_y_f.npy')\n",
    "f0_minus_y = np.concatenate([f0_minus_y_r,f0_minus_y_f])\n",
    "\n",
    "np.save('NTK_data/f0_minus_y.npy',f0_minus_y)\n",
    "del f0_minus_y, f0_minus_y_r, f0_minus_y_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only requires access to the gradients and the initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w_lin(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.load('NTK_data/G.npy')\n",
    "t1 = time.time()\n",
    "theta = G.transpose().dot(G) + num_total*args.weight_decay*np.eye(G.shape[1])\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "del G\n",
    "\n",
    "t1 = time.time()\n",
    "theta_inv = np.linalg.inv(theta)\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta.npy',theta)\n",
    "del theta\n",
    "\n",
    "G = np.load('NTK_data/G.npy')\n",
    "f0_minus_y = np.load('NTK_data/f0_minus_y.npy')\n",
    "t1 = time.time()\n",
    "w_complete = -G.dot(theta_inv.dot(f0_minus_y))\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_inv.npy',theta_inv)\n",
    "np.save('NTK_data/w_complete.npy',w_complete)\n",
    "\n",
    "del G, f0_minus_y, theta_inv, w_complete "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w_lin(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "t1 = time.time()\n",
    "theta_r = G_r.transpose().dot(G_r) + num_to_retain*args.weight_decay*np.eye(G_r.shape[1])\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "del G_r\n",
    "\n",
    "t1 = time.time()\n",
    "theta_r_inv = np.linalg.inv(theta_r)\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_r.npy',theta_r)\n",
    "del theta_r\n",
    "\n",
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "t1 = time.time()\n",
    "w_retain = -G_r.dot(theta_r_inv.dot(f0_minus_y_r))\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_r_inv.npy',theta_r_inv)\n",
    "np.save('NTK_data/w_retain.npy',w_retain)\n",
    "\n",
    "del G_r, f0_minus_y_r, theta_r_inv, w_retain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrubbing Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Scrubbing Direction\n",
    "w_complete = np.load('NTK_data/w_complete.npy')\n",
    "w_retain = np.load('NTK_data/w_retain.npy')\n",
    "delta_w = (w_retain-w_complete).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w_copy = copy.deepcopy(delta_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual Change in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w_actual = vectorize_params(model0)-vectorize_params(model)\n",
    "\n",
    "print(f'Actual Norm-: {np.linalg.norm(delta_w_actual)}')\n",
    "print(f'Predtn Norm-: {np.linalg.norm(delta_w)}')\n",
    "scale_ratio = np.linalg.norm(delta_w_actual)/np.linalg.norm(delta_w)\n",
    "print('Actual Scale: {}'.format(scale_ratio))\n",
    "log_dict['actual_scale_ratio']=scale_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trapezium Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pred_error = vectorize_params(model)-vectorize_params(model_init)-w_retain.squeeze()\n",
    "print(f\"Delta w -------: {np.linalg.norm(delta_w)}\")\n",
    "\n",
    "inner = np.inner(delta_w/np.linalg.norm(delta_w),m_pred_error/np.linalg.norm(m_pred_error))\n",
    "print(f\"Inner Product--: {inner}\")\n",
    "\n",
    "if inner<0:\n",
    "    angle = np.arccos(inner)-np.pi/2\n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.sin(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "else:\n",
    "    angle = np.arccos(inner) \n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.cos(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "\n",
    "predicted_scale=predicted_norm/np.linalg.norm(delta_w)\n",
    "predicted_scale\n",
    "print(f\"Predicted Scale:  {predicted_scale}\")\n",
    "log_dict['predicted_scale_ratio']=predicted_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized Inner Product between Prediction and Actual Scrubbing Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NIP(v1,v2):\n",
    "    nip = (np.inner(v1/np.linalg.norm(v1),v2/np.linalg.norm(v2)))\n",
    "    print(nip)\n",
    "    return nip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nip=NIP(delta_w_actual,delta_w)\n",
    "log_dict['nip']=nip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape delta_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_w_dict(delta_w,model):\n",
    "    # Give normalized delta_w\n",
    "    delta_w_dict = OrderedDict()\n",
    "    params_visited = 0\n",
    "    for k,p in model.named_parameters():\n",
    "        num_params = np.prod(list(p.shape))\n",
    "        update_params = delta_w[params_visited:params_visited+num_params]\n",
    "        delta_w_dict[k] = torch.Tensor(update_params).view_as(p)\n",
    "        params_visited+=num_params\n",
    "    return delta_w_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_predictions(model,dataloader,name):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metrics,activations,predictions=get_metrics(model,dataloader,criterion,True)\n",
    "    print(f\"{name} -> Loss:{np.round(metrics['loss'],3)}, Error:{metrics['error']}\")\n",
    "    log_dict[f\"{name}_loss\"]=metrics['loss']\n",
    "    log_dict[f\"{name}_error\"]=metrics['error']\n",
    "\n",
    "    return activations,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_distance(l1,l2,name):\n",
    "    dist = np.sum(np.abs(l1-l2))\n",
    "    print(f\"Predictions Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_predictions\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_distance(a1,a2,name):\n",
    "    dist = np.linalg.norm(a1-a2,ord=1,axis=1).mean()\n",
    "    print(f\"Activations Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_activations\"]=dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrub using NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=predicted_scale\n",
    "direction = get_delta_w_dict(delta_w,model)\n",
    "\n",
    "model_scrub = copy.deepcopy(model)\n",
    "for k,p in model_scrub.named_parameters():\n",
    "    p.data += (direction[k]*scale).to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune and Fisher Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        if scrub_act:\n",
    "            G = []\n",
    "            for cls in range(num_classes):\n",
    "                grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                grads = torch.cat([g.view(-1) for g in grads])\n",
    "                G.append(grads)\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            G = torch.stack(G).pow(2)\n",
    "            delta_f = torch.matmul(G,delta_w)\n",
    "            output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(model,model_init,weight_decay):\n",
    "    l2_loss = 0\n",
    "    for (k,p),(k_init,p_init) in zip(model.named_parameters(),model_init.named_parameters()):\n",
    "        if p.requires_grad:\n",
    "            l2_loss += (p-p_init).pow(2).sum()\n",
    "    l2_loss *= (weight_decay/2.)\n",
    "    return l2_loss\n",
    "\n",
    "def run_train_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    negative_gradient=False, negative_multiplier=-1, random_labels=False,\n",
    "                    quiet=False,delta_w=None,scrub_act=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            if split=='test' and scrub_act:\n",
    "                G = []\n",
    "                for cls in range(num_classes):\n",
    "                    grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                    grads = torch.cat([g.view(-1) for g in grads])\n",
    "                    G.append(grads)\n",
    "                grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "                G = torch.stack(G).pow(2)\n",
    "                delta_f = torch.matmul(G,delta_w)\n",
    "                output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "            \n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg\n",
    "\n",
    "def run_neggrad_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    forget_loader: torch.utils.data.DataLoader,\n",
    "                    alpha: float,\n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    quiet=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, (batch_retain,batch_forget) in enumerate(tqdm(zip(data_loader,cycle(forget_loader)), leave=False)):\n",
    "            batch_retain = [tensor.to(next(model.parameters()).device) for tensor in batch_retain]\n",
    "            batch_forget = [tensor.to(next(model.parameters()).device) for tensor in batch_forget]\n",
    "            input_r, target_r = batch_retain\n",
    "            input_f, target_f = batch_forget\n",
    "            output_r = model(input_r)\n",
    "            output_f = model(input_f)\n",
    "            loss = alpha*(loss_fn(output_r, target_r) + l2_penalty(model,model_init,args.weight_decay)) - (1-alpha)*loss_fn(output_f, target_f)\n",
    "            metrics.update(n=input_r.size(0), loss=loss_fn(output_r,target_r).item(), error=get_error(output_r, target_r))\n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model: nn.Module, data_loader: torch.utils.data.DataLoader, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_vanilla(epoch, data_loader, model, loss_fn, optimizer, args)\n",
    "\n",
    "def negative_grad(model: nn.Module, data_loader: torch.utils.data.DataLoader, forget_loader: torch.utils.data.DataLoader, alpha: float, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        run_neggrad_epoch(model, model_init, data_loader, forget_loader, alpha, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_negrad(epoch, data_loader, forget_loader, model, loss_fn, optimizer,  alpha)\n",
    "\n",
    "def fk_fientune(model: nn.Module, data_loader: torch.utils.data.DataLoader, args, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "        run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_negrad(epoch, data_loader, forget_loader, model, loss_fn, optimizer,  alpha)\n",
    "def test(model, data_loader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model_init=copy.deepcopy(model)\n",
    "    return run_train_epoch(model, model_init, data_loader, loss_fn, optimizer=None, split='test', epoch=epoch, ignore_index=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout_retrain(model, data_loader, test_loader, lr=0.1, epochs=500, threshold=0.01, quiet=True):\n",
    "    torch.manual_seed(seed)\n",
    "    model = copy.deepcopy(model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=True, num_samples=500)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=data_loader.batch_size, sampler=sampler, num_workers=data_loader.num_workers)\n",
    "    metrics = []\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        metrics.append(run_train_epoch(model, model_init, test_loader, loss_fn, optimizer, split='test', epoch=epoch, ignore_index=None, quiet=quiet))\n",
    "        if metrics[-1]['loss'] <= threshold:\n",
    "            break\n",
    "        run_train_epoch(model, model_init, data_loader_small, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "    return epoch, metrics\n",
    "\n",
    "def extract_retrain_time(metrics, threshold=0.1):\n",
    "    losses = np.array([m['loss'] for m in metrics])\n",
    "    return np.argmax(losses < threshold)\n",
    "\n",
    "def all_readouts(model,thresh=0.1,name='method'):\n",
    "    train_loader = torch.utils.data.DataLoader(train_loader_full.dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    retrain_time, _ = readout_retrain(model, train_loader, forget_loader, epochs=100, lr=0.1, threshold=thresh)\n",
    "    test_error = test(model, test_loader_full)['error']\n",
    "    forget_error = test(model, forget_loader)['error']\n",
    "    retain_error = test(model, retain_loader)['error']\n",
    "    print(f\"{name} ->\"\n",
    "          f\"\\tFull test error: {test_error:.2%}\"\n",
    "          f\"\\tForget error: {forget_error:.2%}\\tRetain error: {retain_error:.2%}\"\n",
    "          f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    log_dict[f\"{name}_retrain_time\"]=retrain_time+1\n",
    "    return(dict(test_error=test_error, forget_error=forget_error, retain_error=retain_error, retrain_time=retrain_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scrubf = copy.deepcopy(model_scrub)\n",
    "modelf = copy.deepcopy(model)\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters(), model_scrubf.parameters()):\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(dataset, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc = 0\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(args.device), orig_target.to(args.device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad_acc += (orig_target == target).float() * p.grad.data\n",
    "                    p.grad2_acc += prob[:, y] * p.grad.data.pow(2)\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc /= len(train_loader)\n",
    "        p.grad2_acc /= len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, model_scrubf)\n",
    "hessian(retain_loader.dataset, modelf)\n",
    "hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_var(p, is_base_dist=False, alpha=3e-6):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3)\n",
    "    if p.size(0) == num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var\n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    if not is_base_dist:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    else:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    if p.size(0) == num_classes and num_to_forget is None:\n",
    "        mu[class_to_forget] = 0\n",
    "        var[class_to_forget] = 0.0001\n",
    "    if p.size(0) == num_classes:\n",
    "        # Last layer\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        # BatchNorm\n",
    "        var *= 10\n",
    "#         var*=1\n",
    "    return mu, var\n",
    "\n",
    "def kl_divergence_fisher(mu0, var0, mu1, var1):\n",
    "    return ((mu1 - mu0).pow(2)/var0 + var1/var0 - torch.log(var1/var0) - 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Noise in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computes the amount of information not forgotten at all layers using the given alpha\n",
    "alpha = 1e-7\n",
    "total_kl = 0\n",
    "torch.manual_seed(seed)\n",
    "for (k, p), (k0, p0) in zip(modelf.named_parameters(), modelf0.named_parameters()):\n",
    "    mu0, var0 = get_mean_var(p, False, alpha=alpha)\n",
    "    mu1, var1 = get_mean_var(p0, True, alpha=alpha)\n",
    "    kl = kl_divergence_fisher(mu0, var0, mu1, var1).item()\n",
    "    total_kl += kl\n",
    "    print(k, f'{kl:.1f}')\n",
    "print(\"Total:\", total_kl)\n",
    "log_dict['fisher_info']=total_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_dir = []\n",
    "alpha = 1e-6\n",
    "torch.manual_seed(seed)\n",
    "for i, p in enumerate(modelf.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "    fisher_dir.append(var.sqrt().view(-1).cpu().detach().numpy())\n",
    "\n",
    "for i, p in enumerate(modelf0.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test(modelf, retain_loader))\n",
    "print(test(modelf, forget_loader))\n",
    "print(test(modelf, valid_loader_full))\n",
    "print(test(modelf, test_loader_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = copy.deepcopy(model)\n",
    "retain_loader = replace_loader_dataset(train_loader_full,retain_dataset, seed=seed, batch_size=args.batch_size, shuffle=True)    \n",
    "finetune(model_ft, retain_loader, epochs=10, quiet=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.ng_alpha = 0.95\n",
    "args.ng_epochs = 10\n",
    "args.ng_lr = 0.01\n",
    "model_ng = copy.deepcopy(model)    \n",
    "negative_grad(model_ng, retain_loader, forget_loader, alpha=args.ng_alpha, epochs=args.ng_epochs, quiet=True, lr=args.ng_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catastrophic Forgetting k layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr_decay_epochs = [10,15,20]\n",
    "args.cfk_lr = 0.01\n",
    "args.cfk_epochs = 10\n",
    "\n",
    "model_cfk = copy.deepcopy(model)\n",
    "\n",
    "for param in model_cfk.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "if args.model == 'allcnn':\n",
    "    layers = [9]\n",
    "    for k in layers:\n",
    "        for param in model_cfk.features[k].parameters():\n",
    "            param.requires_grad_(True)\n",
    "    \n",
    "elif args.model == \"resnet\":\n",
    "    for param in model_cfk.layer4.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "fk_fientune(model_cfk, retain_loader, args=args, epochs=args.cfk_epochs, quiet=True, lr=args.cfk_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Unlearning k layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The last block and classifier of resnet-18\n",
    "(layer4): Sequential(\n",
    "    (0): _ResBlock(\n",
    "      (bn1): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv1): Conv2d(102, 204, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "      (bn2): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (shortcut): Sequential(\n",
    "        (0): Conv2d(102, 204, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "      )\n",
    "    )\n",
    "    (1): _ResBlock(\n",
    "      (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv1): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (bn2): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    )\n",
    "  )\n",
    "(linear): Linear(in_features=204, out_features=5, bias=True)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" The last block and classifier of allcnn\n",
    "AllCNN(\n",
    "  (features): Sequential(\n",
    "    ...\n",
    "    (9): Conv(\n",
    "      (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (2): ReLU()\n",
    "    )\n",
    "    (10): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
    "    (11): Flatten()\n",
    "  )\n",
    "  (classifier): Sequential(\n",
    "    (0): Linear(in_features=192, out_features=5, bias=True)\n",
    "  )\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr_decay_epochs = [10,15,20]\n",
    "args.euk_lr = 0.01\n",
    "args.euk_epochs = training_epochs\n",
    "model_euk = copy.deepcopy(model)\n",
    "\n",
    "for param in model_euk.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "if args.model == 'allcnn':\n",
    "    with torch.no_grad():\n",
    "        for k in layers:\n",
    "            for i in range(0,3):\n",
    "                try:\n",
    "                    model_euk.features[k][i].weight.copy_(model_initial.features[k][i].weight)\n",
    "                except:\n",
    "                    print (\"block {}, layer {} does not have weights\".format(k,i))\n",
    "                try:\n",
    "                    model_euk.features[k][i].bias.copy_(model_initial.features[k][i].bias)\n",
    "                except:\n",
    "                    print (\"block {}, layer {} does not have bias\".format(k,i))\n",
    "        model_euk.classifier[0].weight.copy_(model_initial.classifier[0].weight)\n",
    "        model_euk.classifier[0].bias.copy_(model_initial.classifier[0].bias)\n",
    "    \n",
    "    for k in layers:\n",
    "        for param in model_euk.features[k].parameters():\n",
    "            param.requires_grad_(True)\n",
    "    \n",
    "elif args.model == \"resnet\":\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,2):\n",
    "            try:\n",
    "                model_euk.layer4[i].bn1.weight.copy_(model_initial.layer4[i].bn1.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].bn1.bias.copy_(model_initial.layer4[i].bn1.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv1.weight.copy_(model_initial.layer4[i].conv1.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv1.bias.copy_(model_initial.layer4[i].conv1.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "\n",
    "            try:\n",
    "                model_euk.layer4[i].bn2.weight.copy_(model_initial.layer4[i].bn2.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].bn2.bias.copy_(model_initial.layer4[i].bn2.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv2.weight.copy_(model_initial.layer4[i].conv2.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv2.bias.copy_(model_initial.layer4[i].conv2.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "\n",
    "        model_euk.layer4[0].shortcut[0].weight.copy_(model_initial.layer4[0].shortcut[0].weight)\n",
    "        \n",
    "    for param in model_euk.layer4.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "fk_fientune(model_euk, retain_loader, epochs=args.euk_epochs, quiet=True, lr=args.euk_lr, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try: readouts\n",
    "except: readouts = {}\n",
    "\n",
    "_,_=activations_predictions(copy.deepcopy(model),forget_loader,'Original_Model_D_f')\n",
    "thresh=log_dict['Original_Model_D_f_loss']+1e-5\n",
    "print(thresh)\n",
    "readouts[\"a\"] = all_readouts(copy.deepcopy(model),thresh,'Original')\n",
    "readouts[\"b\"] = all_readouts(copy.deepcopy(model0),thresh,'Retrain')\n",
    "readouts[\"c\"] = all_readouts(copy.deepcopy(model_ft),thresh,'Finetune')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model_ng),thresh,'NegGrad')\n",
    "readouts[\"e\"] = all_readouts(copy.deepcopy(model_cfk),thresh,'CF-k')\n",
    "readouts[\"f\"] = all_readouts(copy.deepcopy(model_euk),thresh,'EU-k')\n",
    "readouts[\"g\"] = all_readouts(copy.deepcopy(modelf),thresh,'Fisher')\n",
    "readouts[\"h\"] = all_readouts(copy.deepcopy(model_scrub),thresh,'NTK')\n",
    "readouts[\"i\"] = all_readouts(copy.deepcopy(model_s),thresh,'SCRUB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
