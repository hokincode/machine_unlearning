{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mw/2gycpkq57p97cpwvmwvm69m00000gn/T/ipykernel_2842/1595774001.py:24: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import variational\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from itertools import cycle\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from typing import List\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "from models import *\n",
    "import models\n",
    "from logger import *\n",
    "import wandb\n",
    "\n",
    "from thirdparty.repdistiller.helper.util import adjust_learning_rate as sgda_adjust_learning_rate\n",
    "from thirdparty.repdistiller.distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss\n",
    "from thirdparty.repdistiller.distiller_zoo import PKT, ABLoss, FactorTransfer, KDSVD, FSP, NSTLoss\n",
    "\n",
    "from thirdparty.repdistiller.helper.loops import train_distill, train_distill_hide, train_distill_linear, train_vanilla, train_negrad, train_bcu, train_bcu_distill, validate\n",
    "from thirdparty.repdistiller.helper.pretrain import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb():\n",
    "    import pdb\n",
    "    pdb.set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_count(model):\n",
    "    count=0\n",
    "    for p in model.parameters():\n",
    "        count+=np.prod(np.array(list(p.shape)))\n",
    "    print(f'Total Number of Parameters: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_params(model):\n",
    "    param = []\n",
    "    for p in model.parameters():\n",
    "        param.append(p.data.view(-1).cpu().numpy())\n",
    "    return np.concatenate(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_shape(model):\n",
    "    for k,p in model.named_parameters():\n",
    "        print(k,p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the original model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: mnist_mlp_1_0_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in mnist_mlp_1_0_forget_None_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Number of Classes: 10\n",
      "State OrderedDict([('layers.0.weight', tensor([[-0.0832, -0.0317,  0.0080,  ...,  0.0650, -0.0346, -0.0338],\n",
      "        [-0.0027, -0.0888, -0.0519,  ..., -0.0379,  0.0615,  0.0149],\n",
      "        [-0.1416, -0.0336,  0.0156,  ...,  0.0427,  0.0694,  0.0308],\n",
      "        ...,\n",
      "        [-0.0182, -0.0285,  0.0117,  ...,  0.0094, -0.0225, -0.0526],\n",
      "        [-0.0159,  0.0288, -0.0066,  ...,  0.0381,  0.0633,  0.0451],\n",
      "        [ 0.0131, -0.0608,  0.0516,  ..., -0.0093,  0.0415, -0.0130]])), ('layers.0.bias', tensor([ 0.2570,  0.5360, -0.9828, -0.2736, -0.2282,  0.3082,  0.4359, -0.1057,\n",
      "        -0.0857,  0.2738,  0.0112, -0.4476,  0.2455,  0.2458, -0.0411, -0.3458,\n",
      "        -0.4463,  0.1637, -0.5212,  0.0843,  0.1513, -0.1780, -0.6362,  0.2063,\n",
      "         0.4134,  0.2404, -0.3497, -0.2096, -0.0949, -0.2470,  0.0197,  0.4874])), ('layers.2.weight', tensor([[-0.3519, -0.2154, -0.3365, -0.1412,  0.4156,  0.1818,  0.0200, -0.0604,\n",
      "          0.2205,  0.2322, -0.0053, -0.1393, -0.2035, -0.0579,  0.0095, -0.1644,\n",
      "          0.2235,  0.3024,  0.2238, -0.7361,  0.2985,  0.1393,  0.1623, -0.0921,\n",
      "         -0.0720, -0.1394,  0.2537, -0.1786,  0.1402, -0.0607,  0.0045,  0.0832],\n",
      "        [ 0.3928, -0.1737, -0.1759, -0.4551,  0.2481, -0.4473, -0.0411, -0.2373,\n",
      "         -0.0809,  0.0645,  0.0849,  0.2062, -0.1421, -0.0616,  0.1801, -0.0840,\n",
      "         -0.2335, -0.0692,  0.1498, -0.0470, -0.1586, -0.1081,  0.0933, -0.2024,\n",
      "         -0.1436, -0.1707, -0.0688, -0.5912,  0.1219, -0.1256,  0.2843, -0.1910],\n",
      "        [ 0.4523, -0.3588,  0.0086,  0.0764,  0.1664,  0.4396,  0.0665,  0.4759,\n",
      "         -0.1288, -0.1356, -0.1838,  0.0269, -0.1764, -0.1239,  0.3496, -0.3435,\n",
      "          0.2170, -0.0702,  0.2358,  0.0587, -0.4519, -0.0437,  0.2893, -0.2968,\n",
      "          0.0510,  0.1216,  0.3035,  0.1648, -0.0432,  0.4584,  0.1138, -0.1609],\n",
      "        [-0.2260, -0.0343, -0.1534, -0.0083, -0.2569,  0.0590,  0.2407,  0.2322,\n",
      "         -0.0340,  0.2132,  0.1880,  0.2921, -0.0644, -0.0330,  0.4186,  0.4952,\n",
      "          0.0323,  0.1553, -0.1318, -0.1697,  0.2046, -0.2184,  0.0321, -0.3517,\n",
      "          0.3038, -0.0949, -0.1500,  0.1053, -0.0209, -0.1221, -0.1718, -0.0792],\n",
      "        [ 0.3664,  0.1804, -0.0120,  0.2229, -0.2088,  0.1287, -0.2451, -0.3353,\n",
      "          0.0897, -0.2442,  0.0131, -0.2590, -0.1019,  0.1473,  0.3632,  0.2329,\n",
      "          0.5305,  0.1337, -0.1440,  0.1620,  0.3382, -0.3171,  0.0806,  0.1881,\n",
      "          0.0109,  0.5281, -0.3445,  0.2525, -0.2200, -0.4710,  0.2022, -0.2743],\n",
      "        [ 0.1823, -0.0562, -0.2757, -0.0546, -0.1238, -0.1061,  0.0172, -0.0347,\n",
      "          0.4565,  0.1192, -0.0601,  0.2493, -0.2201,  0.0361,  0.0090,  0.2869,\n",
      "          0.0090, -0.2832, -0.0779, -0.2768, -0.3961, -0.1289,  0.1251, -0.0015,\n",
      "          0.2930,  0.1101,  0.0197,  0.2491,  0.0698,  0.3177,  0.0416,  0.0137],\n",
      "        [-0.3832,  0.2309, -0.1809,  0.1982,  0.0751,  0.0873, -0.1564,  0.0144,\n",
      "         -0.0449, -0.3521,  0.0023, -0.1973, -0.3833,  0.2016, -0.5147, -0.0191,\n",
      "          0.0122, -0.0809,  0.0038,  0.1643,  0.2994,  0.0281, -0.0899, -0.0444,\n",
      "          0.2220,  0.0072, -0.0604, -0.2090, -0.1963, -0.1330, -0.2076,  0.0046],\n",
      "        [-0.2165, -0.1100,  0.0688, -0.3747,  0.4035, -0.0398, -0.0595, -0.1467,\n",
      "         -0.0216,  0.1590, -0.0791, -0.5878,  0.2140,  0.0116,  0.0842,  0.0181,\n",
      "         -0.2411,  0.1744, -0.0815,  0.3069, -0.1610,  0.1093,  0.1149, -0.2111,\n",
      "         -0.4566, -0.0443,  0.1989, -0.1942, -0.2622, -0.1263,  0.3670, -0.2812],\n",
      "        [-0.1144,  0.1658, -0.1994,  0.2232, -0.0683,  0.2219,  0.2655,  0.4258,\n",
      "         -0.0326, -0.1462, -0.0584,  0.2980, -0.0663,  0.1896, -0.0486,  0.3528,\n",
      "         -0.2598,  0.0683,  0.2263, -0.3286,  0.0065, -0.0126, -0.1758,  0.2649,\n",
      "         -0.1740,  0.2994, -0.0692, -0.3554,  0.0484, -0.0219, -0.4493,  0.4110],\n",
      "        [ 0.4281, -0.0311, -0.3707,  0.5601, -0.2563, -0.1194, -0.2108, -0.0102,\n",
      "         -0.1516, -0.0340, -0.2634,  0.4547, -0.1523,  0.6067, -0.4058,  0.1322,\n",
      "         -0.3815,  0.2264, -0.0823, -0.2132, -0.0130,  0.1347,  0.1836,  0.1278,\n",
      "         -0.0120, -0.0764, -0.3119, -0.1457,  0.3770, -0.0323, -0.4128, -0.1316]])), ('layers.2.bias', tensor([ 0.2496,  0.6326,  0.5569,  0.1073,  0.2888, -0.0639,  0.1895,  0.2966,\n",
      "        -0.0117, -0.5746]))])\n",
      "Args checkpoints/standard_model_for_pilot.pt\n",
      "[0] train metrics:{\"loss\": 1.6957053787638154, \"error\": 0.4350262478126823}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 1.2201567613601685, \"error\": 0.2158}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 9.62 sec\n",
      "[1] train metrics:{\"loss\": 1.135481510219172, \"error\": 0.18546371135738687}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.68 sec\n",
      "[2] train metrics:{\"loss\": 1.0566367169994382, \"error\": 0.1521956503624698}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 7.26 sec\n",
      "[3] train metrics:{\"loss\": 1.0370397905877864, \"error\": 0.14057161903174736}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 8.1 sec\n",
      "[4] train metrics:{\"loss\": 1.0285105837671848, \"error\": 0.13388467627697692}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.59 sec\n",
      "[5] train metrics:{\"loss\": 1.0239834974342739, \"error\": 0.13053078910090826}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.68 sec\n",
      "[6] train metrics:{\"loss\": 1.0214508123054533, \"error\": 0.12780184984584617}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.48 sec\n",
      "[7] train metrics:{\"loss\": 1.0196682121115777, \"error\": 0.1277601866511124}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.6 sec\n",
      "[8] train metrics:{\"loss\": 1.0185629796230855, \"error\": 0.12644779601699857}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.63 sec\n",
      "[9] train metrics:{\"loss\": 1.0178989100958862, \"error\": 0.1261144904591284}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.67 sec\n",
      "[10] train metrics:{\"loss\": 1.0175718008781531, \"error\": 0.12505207899341722}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.89 sec\n",
      "[11] train metrics:{\"loss\": 1.0170587113515286, \"error\": 0.1247812682276477}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.32 sec\n",
      "[12] train metrics:{\"loss\": 1.016697402378289, \"error\": 0.1252187317723523}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.17 sec\n",
      "[13] train metrics:{\"loss\": 1.016376818798848, \"error\": 0.12488542621448212}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 7.63 sec\n",
      "[14] train metrics:{\"loss\": 1.0164363585892404, \"error\": 0.12386467794350471}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.81 sec\n",
      "[15] train metrics:{\"loss\": 1.0160483885343825, \"error\": 0.12453128905924506}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.86 sec\n",
      "[16] train metrics:{\"loss\": 1.0159531660292926, \"error\": 0.12482293142238148}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.65 sec\n",
      "[17] train metrics:{\"loss\": 1.015965955176559, \"error\": 0.12442713107241063}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.64 sec\n",
      "[18] train metrics:{\"loss\": 1.0159101984856695, \"error\": 0.1240104991250729}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.5 sec\n",
      "[19] train metrics:{\"loss\": 1.0157534404452986, \"error\": 0.12465627864344637}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.48 sec\n",
      "[20] train metrics:{\"loss\": 1.0156692651229822, \"error\": 0.12405216231980669}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.68 sec\n",
      "[21] train metrics:{\"loss\": 1.015744189621657, \"error\": 0.12451045746187818}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.41 sec\n",
      "[22] train metrics:{\"loss\": 1.015743130570143, \"error\": 0.12415632030664112}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.49 sec\n",
      "[23] train metrics:{\"loss\": 1.015751474828047, \"error\": 0.12469794183818016}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.47 sec\n",
      "[24] train metrics:{\"loss\": 1.0157363732977018, \"error\": 0.12426047829347554}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.93 sec\n",
      "[25] train metrics:{\"loss\": 1.0156331737730246, \"error\": 0.12367719356720273}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.81 sec\n",
      "[26] train metrics:{\"loss\": 1.0157239567924565, \"error\": 0.12380218315140405}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.67 sec\n",
      "[27] train metrics:{\"loss\": 1.0156740184009458, \"error\": 0.12392717273560537}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.42 sec\n",
      "[28] train metrics:{\"loss\": 1.0157745137292935, \"error\": 0.12438546787767686}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.5 sec\n",
      "[29] train metrics:{\"loss\": 1.0156028505246169, \"error\": 0.12455212065661195}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.46 sec\n",
      "[30] train metrics:{\"loss\": 1.015438807938419, \"error\": 0.12313557203566369}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.5 sec\n",
      "Pure training time: 185.5 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset mnist --model mlp --dataroot=data/MNIST/ --filters 1.0 --lr 0.001 \\\n",
    "--resume checkpoints/standard_model_for_pilot.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 --seed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: mnist_mlp_1_0_forget_[0, 1, 2, 3, 4, 5]_num_300_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3\n",
      "[Logging in mnist_mlp_1_0_forget_[0, 1, 2, 3, 4, 5]_num_300_lr_0_001_bs_128_ls_ce_wd_0_1_seed_3_training]\n",
      "confuse mode: False\n",
      "split mode: None\n",
      "Replacing indexes [ 8526 16694 15152 29352   141 36509 37843 16926 35321 26042 21345  5714\n",
      " 47547 14437 39793 44980 45137  3897 35780 29999 33655 37420 40980 28992\n",
      "   105 40544 41016 46885 46091 40786  5543  7014 20505 25936 16792  2457\n",
      "  1600 13007  7760 15774 46362 33702 32877  2543 33599 32886 35244 29239\n",
      "  8640 23665 45315  3038 22738  3554  4458 36789 11462  2191 23706 10872\n",
      "  9290 32216 25081 12968 28129 20523  5261 16762 26382 19263 12728 37875\n",
      " 36911  7326 21202 31998 26810 28700 36488 42838  3180 24261 25761 13600\n",
      " 18425 29148 39305  4254  2025  4307 45980 28150 36331  8747 35060 28517\n",
      "  8694 12015 38188 36605 26116 12256 18607 11847  1442 19196 18989 33194\n",
      " 26487  5803   828 40152 11342 43196 20715 41802 14043  8865 20144 41373\n",
      " 47285  8202 21858  1815 22335  2739 16404  6882  9838 14297  4773 29745\n",
      "  8784 20420  9033 18865 39460 23585 14421 25902 31362 17568 47282 10213\n",
      " 10851 27398  9097 13317  3946 12763 22347 26179 27559 12416 12204 19720\n",
      " 28625 43760   991 18748  3274 11284 33410 22005  3957 23606 30636 39992\n",
      "  1264 25266 17560 31423 46872  6399  8159 35119  1818 44722 35781 44342\n",
      " 25267   634 46251  4819 19794 29351 33480 27853 45953 19391  8342 23363\n",
      " 10898 22639 11929 33139 20729 23386 14207 42742  6533 15472 42530 19571\n",
      " 22377 37233 37954 28603  5153 40736 28158 13824 24596 47089  4148 32854\n",
      " 44602 27694 42719 24075  6046 21712  7058  1106 33206 27058 40874 12132\n",
      " 27016 21465 25892  8743  8810 33435 33167 23515 47454 24149 46878 44043\n",
      " 12757 41548 24280 38149  3902 38656 29473  9135 19714 31691 38719 42099\n",
      " 41461  6753 39036 38931 19219 20760  6409 44396  3192 30070 21387 10447\n",
      " 47641  5579 43237 20422 15418 29575 24229 42701  5380  5442 12690 44635\n",
      " 35380 24691 28596 36012 38802  8417 18947 17449 14339 25511  4570 22447\n",
      " 12060 46231 41953 29310 37274 10157  1795 18569 36490 23796 29340 43197]\n",
      "Number of Classes: 10\n",
      "State OrderedDict([('layers.0.weight', tensor([[-0.0832, -0.0317,  0.0080,  ...,  0.0650, -0.0346, -0.0338],\n",
      "        [-0.0027, -0.0888, -0.0519,  ..., -0.0379,  0.0615,  0.0149],\n",
      "        [-0.1416, -0.0336,  0.0156,  ...,  0.0427,  0.0694,  0.0308],\n",
      "        ...,\n",
      "        [-0.0182, -0.0285,  0.0117,  ...,  0.0094, -0.0225, -0.0526],\n",
      "        [-0.0159,  0.0288, -0.0066,  ...,  0.0381,  0.0633,  0.0451],\n",
      "        [ 0.0131, -0.0608,  0.0516,  ..., -0.0093,  0.0415, -0.0130]])), ('layers.0.bias', tensor([ 0.2570,  0.5360, -0.9828, -0.2736, -0.2282,  0.3082,  0.4359, -0.1057,\n",
      "        -0.0857,  0.2738,  0.0112, -0.4476,  0.2455,  0.2458, -0.0411, -0.3458,\n",
      "        -0.4463,  0.1637, -0.5212,  0.0843,  0.1513, -0.1780, -0.6362,  0.2063,\n",
      "         0.4134,  0.2404, -0.3497, -0.2096, -0.0949, -0.2470,  0.0197,  0.4874])), ('layers.2.weight', tensor([[-0.3519, -0.2154, -0.3365, -0.1412,  0.4156,  0.1818,  0.0200, -0.0604,\n",
      "          0.2205,  0.2322, -0.0053, -0.1393, -0.2035, -0.0579,  0.0095, -0.1644,\n",
      "          0.2235,  0.3024,  0.2238, -0.7361,  0.2985,  0.1393,  0.1623, -0.0921,\n",
      "         -0.0720, -0.1394,  0.2537, -0.1786,  0.1402, -0.0607,  0.0045,  0.0832],\n",
      "        [ 0.3928, -0.1737, -0.1759, -0.4551,  0.2481, -0.4473, -0.0411, -0.2373,\n",
      "         -0.0809,  0.0645,  0.0849,  0.2062, -0.1421, -0.0616,  0.1801, -0.0840,\n",
      "         -0.2335, -0.0692,  0.1498, -0.0470, -0.1586, -0.1081,  0.0933, -0.2024,\n",
      "         -0.1436, -0.1707, -0.0688, -0.5912,  0.1219, -0.1256,  0.2843, -0.1910],\n",
      "        [ 0.4523, -0.3588,  0.0086,  0.0764,  0.1664,  0.4396,  0.0665,  0.4759,\n",
      "         -0.1288, -0.1356, -0.1838,  0.0269, -0.1764, -0.1239,  0.3496, -0.3435,\n",
      "          0.2170, -0.0702,  0.2358,  0.0587, -0.4519, -0.0437,  0.2893, -0.2968,\n",
      "          0.0510,  0.1216,  0.3035,  0.1648, -0.0432,  0.4584,  0.1138, -0.1609],\n",
      "        [-0.2260, -0.0343, -0.1534, -0.0083, -0.2569,  0.0590,  0.2407,  0.2322,\n",
      "         -0.0340,  0.2132,  0.1880,  0.2921, -0.0644, -0.0330,  0.4186,  0.4952,\n",
      "          0.0323,  0.1553, -0.1318, -0.1697,  0.2046, -0.2184,  0.0321, -0.3517,\n",
      "          0.3038, -0.0949, -0.1500,  0.1053, -0.0209, -0.1221, -0.1718, -0.0792],\n",
      "        [ 0.3664,  0.1804, -0.0120,  0.2229, -0.2088,  0.1287, -0.2451, -0.3353,\n",
      "          0.0897, -0.2442,  0.0131, -0.2590, -0.1019,  0.1473,  0.3632,  0.2329,\n",
      "          0.5305,  0.1337, -0.1440,  0.1620,  0.3382, -0.3171,  0.0806,  0.1881,\n",
      "          0.0109,  0.5281, -0.3445,  0.2525, -0.2200, -0.4710,  0.2022, -0.2743],\n",
      "        [ 0.1823, -0.0562, -0.2757, -0.0546, -0.1238, -0.1061,  0.0172, -0.0347,\n",
      "          0.4565,  0.1192, -0.0601,  0.2493, -0.2201,  0.0361,  0.0090,  0.2869,\n",
      "          0.0090, -0.2832, -0.0779, -0.2768, -0.3961, -0.1289,  0.1251, -0.0015,\n",
      "          0.2930,  0.1101,  0.0197,  0.2491,  0.0698,  0.3177,  0.0416,  0.0137],\n",
      "        [-0.3832,  0.2309, -0.1809,  0.1982,  0.0751,  0.0873, -0.1564,  0.0144,\n",
      "         -0.0449, -0.3521,  0.0023, -0.1973, -0.3833,  0.2016, -0.5147, -0.0191,\n",
      "          0.0122, -0.0809,  0.0038,  0.1643,  0.2994,  0.0281, -0.0899, -0.0444,\n",
      "          0.2220,  0.0072, -0.0604, -0.2090, -0.1963, -0.1330, -0.2076,  0.0046],\n",
      "        [-0.2165, -0.1100,  0.0688, -0.3747,  0.4035, -0.0398, -0.0595, -0.1467,\n",
      "         -0.0216,  0.1590, -0.0791, -0.5878,  0.2140,  0.0116,  0.0842,  0.0181,\n",
      "         -0.2411,  0.1744, -0.0815,  0.3069, -0.1610,  0.1093,  0.1149, -0.2111,\n",
      "         -0.4566, -0.0443,  0.1989, -0.1942, -0.2622, -0.1263,  0.3670, -0.2812],\n",
      "        [-0.1144,  0.1658, -0.1994,  0.2232, -0.0683,  0.2219,  0.2655,  0.4258,\n",
      "         -0.0326, -0.1462, -0.0584,  0.2980, -0.0663,  0.1896, -0.0486,  0.3528,\n",
      "         -0.2598,  0.0683,  0.2263, -0.3286,  0.0065, -0.0126, -0.1758,  0.2649,\n",
      "         -0.1740,  0.2994, -0.0692, -0.3554,  0.0484, -0.0219, -0.4493,  0.4110],\n",
      "        [ 0.4281, -0.0311, -0.3707,  0.5601, -0.2563, -0.1194, -0.2108, -0.0102,\n",
      "         -0.1516, -0.0340, -0.2634,  0.4547, -0.1523,  0.6067, -0.4058,  0.1322,\n",
      "         -0.3815,  0.2264, -0.0823, -0.2132, -0.0130,  0.1347,  0.1836,  0.1278,\n",
      "         -0.0120, -0.0764, -0.3119, -0.1457,  0.3770, -0.0323, -0.4128, -0.1316]])), ('layers.2.bias', tensor([ 0.2496,  0.6326,  0.5569,  0.1073,  0.2888, -0.0639,  0.1895,  0.2966,\n",
      "        -0.0117, -0.5746]))])\n",
      "Args checkpoints/standard_model_for_pilot.pt\n",
      "[0] train metrics:{\"loss\": 1.6955508042470524, \"error\": 0.4350262478126823}\n",
      "Learning Rate : 0.001\n",
      "[0] test metrics:{\"loss\": 1.220088168334961, \"error\": 0.2163}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.6 sec\n",
      "[1] train metrics:{\"loss\": 1.1354534324730707, \"error\": 0.18535955337055246}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.51 sec\n",
      "[2] train metrics:{\"loss\": 1.056726030901704, \"error\": 0.15227897675193733}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.47 sec\n",
      "[3] train metrics:{\"loss\": 1.0371734988102366, \"error\": 0.14057161903174736}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.48 sec\n",
      "[4] train metrics:{\"loss\": 1.028648248931466, \"error\": 0.13371802349804182}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.63 sec\n",
      "[5] train metrics:{\"loss\": 1.024162956063285, \"error\": 0.13086409465877843}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.45 sec\n",
      "[6] train metrics:{\"loss\": 1.0216317594711923, \"error\": 0.12803099741688193}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.54 sec\n",
      "[7] train metrics:{\"loss\": 1.0198602803487915, \"error\": 0.1279476710274144}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.51 sec\n",
      "[8] train metrics:{\"loss\": 1.0187525477630677, \"error\": 0.12640613282226482}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.49 sec\n",
      "[9] train metrics:{\"loss\": 1.0180947887193221, \"error\": 0.12623948004332972}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.56 sec\n",
      "[10] train metrics:{\"loss\": 1.0177762444918677, \"error\": 0.12536455295392052}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.47 sec\n",
      "[11] train metrics:{\"loss\": 1.017264149882537, \"error\": 0.12498958420131656}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.6 sec\n",
      "[12] train metrics:{\"loss\": 1.016896297669234, \"error\": 0.1252187317723523}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.46 sec\n",
      "[13] train metrics:{\"loss\": 1.0165873311497888, \"error\": 0.1253437213565536}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.58 sec\n",
      "[14] train metrics:{\"loss\": 1.0166354835376115, \"error\": 0.12396883593033914}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.51 sec\n",
      "[15] train metrics:{\"loss\": 1.0162662003658363, \"error\": 0.12498958420131656}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.47 sec\n",
      "[16] train metrics:{\"loss\": 1.0161743731997765, \"error\": 0.12490625781184901}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.83 sec\n",
      "[17] train metrics:{\"loss\": 1.0162088719817681, \"error\": 0.12465627864344637}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.59 sec\n",
      "[18] train metrics:{\"loss\": 1.016125079642017, \"error\": 0.12413548870927422}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.03 sec\n",
      "[19] train metrics:{\"loss\": 1.0159665824681379, \"error\": 0.12457295225397884}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 6.23 sec\n",
      "[20] train metrics:{\"loss\": 1.0158672984883483, \"error\": 0.12428130989084243}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.85 sec\n",
      "[21] train metrics:{\"loss\": 1.0159629398599763, \"error\": 0.12455212065661195}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.61 sec\n",
      "[22] train metrics:{\"loss\": 1.0159606663110543, \"error\": 0.12409382551454046}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 7.59 sec\n",
      "[23] train metrics:{\"loss\": 1.0159797972316056, \"error\": 0.12498958420131656}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.68 sec\n",
      "[24] train metrics:{\"loss\": 1.0159649011910175, \"error\": 0.12407299391717357}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.54 sec\n",
      "[25] train metrics:{\"loss\": 1.015828314502779, \"error\": 0.12384384634613782}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.65 sec\n",
      "[26] train metrics:{\"loss\": 1.0159221884906515, \"error\": 0.12384384634613782}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.57 sec\n",
      "[27] train metrics:{\"loss\": 1.0158880807669817, \"error\": 0.12388550954087159}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.63 sec\n",
      "[28] train metrics:{\"loss\": 1.0159736355884146, \"error\": 0.12469794183818016}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.52 sec\n",
      "[29] train metrics:{\"loss\": 1.0158080630834456, \"error\": 0.12457295225397884}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.52 sec\n",
      "[30] train metrics:{\"loss\": 1.0156555478547615, \"error\": 0.12323973002249812}\n",
      "Learning Rate : 0.001\n",
      "Epoch Time: 5.58 sec\n",
      "Pure training time: 177.65000000000003 sec\n"
     ]
    }
   ],
   "source": [
    "%run main.py --dataset mnist --model mlp --dataroot=data/MNIST/ --filters 1.0 --lr 0.001 \\\n",
    "--resume checkpoints/standard_model_for_pilot.pt --disable-bn \\\n",
    "--weight-decay 0.1 --batch-size 128 --epochs 31 \\\n",
    "--forget-class 0,1,2,3,4,5 --num-to-forget 300 --seed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict={}\n",
    "training_epochs=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['epoch']=training_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 33130\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameter_count(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "model0 = copy.deepcopy(model)\n",
    "model_initial = copy.deepcopy(model)\n",
    "\n",
    "arch = args.model \n",
    "filters=args.filters\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "augment = False\n",
    "dataset = args.dataset\n",
    "class_to_forget = args.forget_class\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "num_classes=args.num_classes\n",
    "num_to_forget = args.num_to_forget\n",
    "num_total = len(train_loader.dataset)\n",
    "num_to_retain = num_total - 300#num_to_forget\n",
    "seed = args.seed\n",
    "unfreeze_start = None\n",
    "\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "\n",
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(m_name))\n",
    "model0.load_state_dict(torch.load(m0_name))\n",
    "model_initial.load_state_dict(torch.load(init_checkpoint))\n",
    "\n",
    "teacher = copy.deepcopy(model)\n",
    "student = copy.deepcopy(model)\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "for p in model0.parameters():\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['args']=args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between w(D) and w(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(model,model0):\n",
    "    distance=0\n",
    "    normalization=0\n",
    "    for (k, p), (k0, p0) in zip(model.named_parameters(), model0.named_parameters()):\n",
    "        space='  ' if 'bias' in k else ''\n",
    "        current_dist=(p.data0-p0.data0).pow(2).sum().item()\n",
    "        current_norm=p.data0.pow(2).sum().item()\n",
    "        distance+=current_dist\n",
    "        normalization+=current_norm\n",
    "    print(f'Distance: {np.sqrt(distance)}')\n",
    "    print(f'Normalized Distance: {1.0*np.sqrt(distance/normalization)}')\n",
    "    return 1.0*np.sqrt(distance/normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 0.01723946476033257\n",
      "Normalized Distance: 0.0017215296878199434\n"
     ]
    }
   ],
   "source": [
    "log_dict['dist_Original_Retrain']=distance(model,model0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance of w(D) from initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntk_init(resume,seed=1):\n",
    "    manual_seed(seed)\n",
    "    model_init = models.get_model(arch, num_classes=num_classes, filters_percentage=filters).to(args.device)\n",
    "    model_init.load_state_dict(torch.load(resume))\n",
    "    return model_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "for p in model_init.parameters():\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 2.838469428891656\n",
      "Normalized Distance: 0.30469125188309615\n"
     ]
    }
   ],
   "source": [
    "log_dict['dist_Original_Original_init']=distance(model_init,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.retain_bs = 32\n",
    "args.forget_bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confuse mode: False\n",
      "split mode: train\n",
      "confuse mode: False\n",
      "split mode: train\n",
      "Replacing indexes [ 8526 16694 15152 29352   141 36509 37843 16926 35321 26042 21345  5714\n",
      " 47547 14437 39793 44980 45137  3897 35780 29999 33655 37420 40980 28992\n",
      "   105 40544 41016 46885 46091 40786  5543  7014 20505 25936 16792  2457\n",
      "  1600 13007  7760 15774 46362 33702 32877  2543 33599 32886 35244 29239\n",
      "  8640 23665 45315  3038 22738  3554  4458 36789 11462  2191 23706 10872\n",
      "  9290 32216 25081 12968 28129 20523  5261 16762 26382 19263 12728 37875\n",
      " 36911  7326 21202 31998 26810 28700 36488 42838  3180 24261 25761 13600\n",
      " 18425 29148 39305  4254  2025  4307 45980 28150 36331  8747 35060 28517\n",
      "  8694 12015 38188 36605 26116 12256 18607 11847  1442 19196 18989 33194\n",
      " 26487  5803   828 40152 11342 43196 20715 41802 14043  8865 20144 41373\n",
      " 47285  8202 21858  1815 22335  2739 16404  6882  9838 14297  4773 29745\n",
      "  8784 20420  9033 18865 39460 23585 14421 25902 31362 17568 47282 10213\n",
      " 10851 27398  9097 13317  3946 12763 22347 26179 27559 12416 12204 19720\n",
      " 28625 43760   991 18748  3274 11284 33410 22005  3957 23606 30636 39992\n",
      "  1264 25266 17560 31423 46872  6399  8159 35119  1818 44722 35781 44342\n",
      " 25267   634 46251  4819 19794 29351 33480 27853 45953 19391  8342 23363\n",
      " 10898 22639 11929 33139 20729 23386 14207 42742  6533 15472 42530 19571\n",
      " 22377 37233 37954 28603  5153 40736 28158 13824 24596 47089  4148 32854\n",
      " 44602 27694 42719 24075  6046 21712  7058  1106 33206 27058 40874 12132\n",
      " 27016 21465 25892  8743  8810 33435 33167 23515 47454 24149 46878 44043\n",
      " 12757 41548 24280 38149  3902 38656 29473  9135 19714 31691 38719 42099\n",
      " 41461  6753 39036 38931 19219 20760  6409 44396  3192 30070 21387 10447\n",
      " 47641  5579 43237 20422 15418 29575 24229 42701  5380  5442 12690 44635\n",
      " 35380 24691 28596 36012 38802  8417 18947 17449 14339 25511  4570 22447\n",
      " 12060 46231 41953 29310 37274 10157  1795 18569 36490 23796 29340 43197]\n"
     ]
    }
   ],
   "source": [
    "train_loader_full, valid_loader_full, test_loader_full   = datasets.get_loaders(dataset, batch_size=args.batch_size, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "marked_loader, _, _ = datasets.get_loaders(dataset, class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, root=args.dataroot, augment=False, shuffle=True)\n",
    "\n",
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "    manual_seed(seed)\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "    def _init_fn(worker_id):\n",
    "        np.random.seed(int(seed))\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)\n",
    "    \n",
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = forget_dataset.targets < 0\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, batch_size=args.forget_bs, seed=seed, shuffle=True)\n",
    "\n",
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "marked = retain_dataset.targets >= 0\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, batch_size=args.retain_bs, seed=seed, shuffle=True)\n",
    "\n",
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "47704\n",
      "10000\n",
      "48004\n",
      "{5: 4337, 4: 4674, 9: 4760, 2: 4767, 1: 5394, 3: 4905, 6: 4735, 7: 5012, 8: 4681, 0: 4739}\n"
     ]
    }
   ],
   "source": [
    "print (len(forget_loader.dataset))\n",
    "print (len(retain_loader.dataset))\n",
    "print (len(test_loader_full.dataset))\n",
    "print (len(train_loader_full.dataset))\n",
    "from collections import Counter\n",
    "print(dict(Counter(train_loader_full.dataset.targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.optim = 'adam'\n",
    "args.gamma = 1\n",
    "args.alpha = 0.5\n",
    "args.beta = 0\n",
    "args.smoothing = 0.5\n",
    "args.msteps = 3\n",
    "args.clip = 0.2\n",
    "args.sstart = 10\n",
    "args.kd_T = 2\n",
    "args.distill = 'kd'\n",
    "\n",
    "args.sgda_epochs = 10\n",
    "args.sgda_learning_rate = 0.0005\n",
    "args.lr_decay_epochs = [5,8,9]\n",
    "args.lr_decay_rate = 0.1\n",
    "args.sgda_weight_decay = 0.1#5e-4\n",
    "args.sgda_momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = copy.deepcopy(teacher)\n",
    "model_s = copy.deepcopy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is from https://github.com/ojus1/SmoothedGradientDescentAscent/blob/main/SGDA.py\n",
    "#For SGDA smoothing\n",
    "beta = 0.1\n",
    "def avg_fn(averaged_model_parameter, model_parameter, num_averaged): return (\n",
    "    1 - beta) * averaged_model_parameter + beta * model_parameter\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(\n",
    "    model_s, avg_fn=avg_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = nn.ModuleList([])\n",
    "module_list.append(model_s)\n",
    "trainable_list = nn.ModuleList([])\n",
    "trainable_list.append(model_s)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_div = DistillKL(args.kd_T)\n",
    "criterion_kd = DistillKL(args.kd_T)\n",
    "\n",
    "\n",
    "criterion_list = nn.ModuleList([])\n",
    "criterion_list.append(criterion_cls)    # classification loss\n",
    "criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
    "criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "\n",
    "# optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = optim.SGD(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"adam\": \n",
    "    optimizer = optim.Adam(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"rmsp\":\n",
    "    optimizer = optim.RMSprop(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list.append(model_t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    module_list.cuda()\n",
    "    criterion_list.cuda()\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True\n",
    "    swa_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> scrub unlearning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billdeng/anaconda3/envs/unlearning_Version1/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Acc@1 87.582 \n",
      "maximize loss: 0.07\t minimize loss: 2.04\t train_acc: 87.58175659179688\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 87.599 \n",
      "maximize loss: 1.49\t minimize loss: 2.12\t train_acc: 87.59852600097656\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 87.649 \n",
      "maximize loss: 1.50\t minimize loss: 2.12\t train_acc: 87.64883422851562\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 87.590 \n",
      "maximize loss: 0.00\t minimize loss: 2.12\t train_acc: 87.59014129638672\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 87.630 \n",
      "maximize loss: 0.00\t minimize loss: 2.12\t train_acc: 87.62996673583984\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 87.886 \n",
      "maximize loss: 0.00\t minimize loss: 2.10\t train_acc: 87.88571166992188\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 87.909 \n",
      "maximize loss: 0.00\t minimize loss: 2.10\t train_acc: 87.90876770019531\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 87.854 \n",
      "maximize loss: 0.00\t minimize loss: 2.10\t train_acc: 87.8542709350586\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 87.953 \n",
      "maximize loss: 0.00\t minimize loss: 2.09\t train_acc: 87.95278930664062\n",
      "==> scrub unlearning ...\n",
      " * Acc@1 87.955 \n",
      "maximize loss: 0.00\t minimize loss: 2.09\t train_acc: 87.95488739013672\n"
     ]
    }
   ],
   "source": [
    "acc_rs = []\n",
    "acc_fs = []\n",
    "acc_ts = []\n",
    "for epoch in range(1, args.sgda_epochs + 1):\n",
    "\n",
    "    lr = sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "\n",
    "    print(\"==> scrub unlearning ...\")\n",
    "\n",
    "    acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True)\n",
    "    acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True)\n",
    "    acc_rs.append(100-acc_r.item())\n",
    "    acc_fs.append(100-acc_f.item())\n",
    "\n",
    "    maximize_loss = 0\n",
    "    if epoch <= args.msteps:\n",
    "        maximize_loss = train_distill(epoch, forget_loader, module_list, swa_model, criterion_list, optimizer, args, \"maximize\")\n",
    "    train_acc, train_loss = train_distill(epoch, retain_loader, module_list, swa_model, criterion_list, optimizer, args, \"minimize\",)\n",
    "    if epoch >= args.sstart:\n",
    "        swa_model.update_parameters(model_s)\n",
    "\n",
    "    print (\"maximize loss: {:.2f}\\t minimize loss: {:.2f}\\t train_acc: {}\".format(maximize_loss, train_loss, train_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "indices = list(range(0,len(acc_rs)))\n",
    "plt.plot(indices, acc_rs, marker='*', alpha=1, label='retain-set')\n",
    "plt.plot(indices, acc_fs, marker='o', alpha=1, label='forget-set')\n",
    "plt.legend(prop={'size': 14})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.title('scrub retain- and forget- set error',size=18)\n",
    "plt.xlabel('epoch',size=14)\n",
    "plt.ylabel('error',size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTK based Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NTK Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_w_utils(model_init,dataloader,name='complete'):\n",
    "    model_init.eval()\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    G_list = []\n",
    "    f0_minus_y = []\n",
    "    for idx, batch in enumerate(dataloader):#(tqdm(dataloader,leave=False)):\n",
    "        print(\"One iteration:\", time.time())\n",
    "        batch = [tensor.to(next(model_init.parameters()).device) for tensor in batch]\n",
    "        input, target = batch\n",
    "        if 'mnist' in args.dataset:\n",
    "            input = input.view(input.shape[0],-1)\n",
    "        target = target.cpu().detach().numpy()\n",
    "        output = model_init(input)\n",
    "        G_sample=[]\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output[0,cls],model_init.parameters(),retain_graph=True)\n",
    "            grads = np.concatenate([g.view(-1).cpu().numpy() for g in grads])\n",
    "            G_sample.append(grads)\n",
    "            G_list.append(grads)\n",
    "        if args.lossfn=='mse':\n",
    "            p = output.cpu().detach().numpy().transpose()\n",
    "            #loss_hess = np.eye(len(p))\n",
    "            target = 2*target-1\n",
    "            f0_y_update = p-target\n",
    "        elif args.lossfn=='ce':\n",
    "            p = torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().transpose()\n",
    "            p[target]-=1\n",
    "            f0_y_update = copy.deepcopy(p)\n",
    "        f0_minus_y.append(f0_y_update)\n",
    "    return np.stack(G_list).transpose(),np.vstack(f0_minus_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobians and Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One iteration: 1691951288.00553\n",
      "One iteration: 1691951288.0094552\n",
      "One iteration: 1691951288.012887\n",
      "One iteration: 1691951288.0168178\n",
      "One iteration: 1691951288.019975\n",
      "One iteration: 1691951288.023285\n",
      "One iteration: 1691951288.0281138\n",
      "One iteration: 1691951288.031371\n",
      "One iteration: 1691951288.0355\n",
      "One iteration: 1691951288.038831\n",
      "One iteration: 1691951288.0418088\n",
      "One iteration: 1691951288.045284\n",
      "One iteration: 1691951288.048745\n",
      "One iteration: 1691951288.05192\n",
      "One iteration: 1691951288.055404\n",
      "One iteration: 1691951288.0583198\n",
      "One iteration: 1691951288.06127\n",
      "One iteration: 1691951288.064678\n",
      "One iteration: 1691951288.068932\n",
      "One iteration: 1691951288.072638\n",
      "One iteration: 1691951288.0756931\n",
      "One iteration: 1691951288.078752\n",
      "One iteration: 1691951288.081811\n",
      "One iteration: 1691951288.084922\n",
      "One iteration: 1691951288.0876942\n",
      "One iteration: 1691951288.090925\n",
      "One iteration: 1691951288.093795\n",
      "One iteration: 1691951288.09731\n",
      "One iteration: 1691951288.1010828\n",
      "One iteration: 1691951288.104105\n",
      "One iteration: 1691951288.1074848\n",
      "One iteration: 1691951288.110743\n",
      "One iteration: 1691951288.113379\n",
      "One iteration: 1691951288.11605\n",
      "One iteration: 1691951288.118696\n",
      "One iteration: 1691951288.1214662\n",
      "One iteration: 1691951288.124662\n",
      "One iteration: 1691951288.1272619\n",
      "One iteration: 1691951288.129708\n",
      "One iteration: 1691951288.132257\n",
      "One iteration: 1691951288.135305\n",
      "One iteration: 1691951288.1384141\n",
      "One iteration: 1691951288.141757\n",
      "One iteration: 1691951288.144397\n",
      "One iteration: 1691951288.146883\n",
      "One iteration: 1691951288.149737\n",
      "One iteration: 1691951288.1525\n",
      "One iteration: 1691951288.155657\n",
      "One iteration: 1691951288.1589272\n",
      "One iteration: 1691951288.161546\n",
      "One iteration: 1691951288.164045\n",
      "One iteration: 1691951288.166838\n",
      "One iteration: 1691951288.169488\n",
      "One iteration: 1691951288.172553\n",
      "One iteration: 1691951288.1758358\n",
      "One iteration: 1691951288.178615\n",
      "One iteration: 1691951288.181334\n",
      "One iteration: 1691951288.1841438\n",
      "One iteration: 1691951288.1868758\n",
      "One iteration: 1691951288.190231\n",
      "One iteration: 1691951288.1934998\n",
      "One iteration: 1691951288.196071\n",
      "One iteration: 1691951288.1993148\n",
      "One iteration: 1691951288.202069\n",
      "One iteration: 1691951288.205014\n",
      "One iteration: 1691951288.2096472\n",
      "One iteration: 1691951288.2130802\n",
      "One iteration: 1691951288.216154\n",
      "One iteration: 1691951288.218842\n",
      "One iteration: 1691951288.2218611\n",
      "One iteration: 1691951288.225523\n",
      "One iteration: 1691951288.2286248\n",
      "One iteration: 1691951288.231444\n",
      "One iteration: 1691951288.2343879\n",
      "One iteration: 1691951288.237908\n",
      "One iteration: 1691951288.2413561\n",
      "One iteration: 1691951288.244787\n",
      "One iteration: 1691951288.247589\n",
      "One iteration: 1691951288.251209\n",
      "One iteration: 1691951288.254904\n",
      "One iteration: 1691951288.2577732\n",
      "One iteration: 1691951288.26087\n",
      "One iteration: 1691951288.265103\n",
      "One iteration: 1691951288.268832\n",
      "One iteration: 1691951288.272537\n",
      "One iteration: 1691951288.275798\n",
      "One iteration: 1691951288.279053\n",
      "One iteration: 1691951288.2819672\n",
      "One iteration: 1691951288.284925\n",
      "One iteration: 1691951288.288069\n",
      "One iteration: 1691951288.291002\n",
      "One iteration: 1691951288.294425\n",
      "One iteration: 1691951288.297303\n",
      "One iteration: 1691951288.300222\n",
      "One iteration: 1691951288.3033369\n",
      "One iteration: 1691951288.306258\n",
      "One iteration: 1691951288.310229\n",
      "One iteration: 1691951288.313335\n",
      "One iteration: 1691951288.316444\n",
      "One iteration: 1691951288.319384\n",
      "One iteration: 1691951288.3225222\n",
      "One iteration: 1691951288.325954\n",
      "One iteration: 1691951288.3295202\n",
      "One iteration: 1691951288.3324401\n",
      "One iteration: 1691951288.336006\n",
      "One iteration: 1691951288.339381\n",
      "One iteration: 1691951288.3429801\n",
      "One iteration: 1691951288.346134\n",
      "One iteration: 1691951288.349407\n",
      "One iteration: 1691951288.352656\n",
      "One iteration: 1691951288.355801\n",
      "One iteration: 1691951288.359283\n",
      "One iteration: 1691951288.362421\n",
      "One iteration: 1691951288.365786\n",
      "One iteration: 1691951288.368791\n",
      "One iteration: 1691951288.371688\n",
      "One iteration: 1691951288.375434\n",
      "One iteration: 1691951288.378659\n",
      "One iteration: 1691951288.381344\n",
      "One iteration: 1691951288.3844962\n",
      "One iteration: 1691951288.38725\n",
      "One iteration: 1691951288.390388\n",
      "One iteration: 1691951288.3936808\n",
      "One iteration: 1691951288.396672\n",
      "One iteration: 1691951288.399414\n",
      "One iteration: 1691951288.402102\n",
      "One iteration: 1691951288.405115\n",
      "One iteration: 1691951288.4082592\n",
      "One iteration: 1691951288.41218\n",
      "One iteration: 1691951288.415458\n",
      "One iteration: 1691951288.418265\n",
      "One iteration: 1691951288.421036\n",
      "One iteration: 1691951288.423904\n",
      "One iteration: 1691951288.426991\n",
      "One iteration: 1691951288.4295838\n",
      "One iteration: 1691951288.432197\n",
      "One iteration: 1691951288.435147\n",
      "One iteration: 1691951288.438284\n",
      "One iteration: 1691951288.441289\n",
      "One iteration: 1691951288.4449868\n",
      "One iteration: 1691951288.448035\n",
      "One iteration: 1691951288.451199\n",
      "One iteration: 1691951288.453921\n",
      "One iteration: 1691951288.457005\n",
      "One iteration: 1691951288.4603\n",
      "One iteration: 1691951288.462996\n",
      "One iteration: 1691951288.4659252\n",
      "One iteration: 1691951288.468874\n",
      "One iteration: 1691951288.471859\n",
      "One iteration: 1691951288.474792\n",
      "One iteration: 1691951288.477909\n",
      "One iteration: 1691951288.4805791\n",
      "One iteration: 1691951288.483698\n",
      "One iteration: 1691951288.486921\n",
      "One iteration: 1691951288.489772\n",
      "One iteration: 1691951288.493093\n",
      "One iteration: 1691951288.4957042\n",
      "One iteration: 1691951288.498399\n",
      "One iteration: 1691951288.501677\n",
      "One iteration: 1691951288.504886\n",
      "One iteration: 1691951288.5078988\n",
      "One iteration: 1691951288.51121\n",
      "One iteration: 1691951288.513803\n",
      "One iteration: 1691951288.516534\n",
      "One iteration: 1691951288.5193658\n",
      "One iteration: 1691951288.52233\n",
      "One iteration: 1691951288.525472\n",
      "One iteration: 1691951288.528534\n",
      "One iteration: 1691951288.531242\n",
      "One iteration: 1691951288.534059\n",
      "One iteration: 1691951288.536863\n",
      "One iteration: 1691951288.539946\n",
      "One iteration: 1691951288.543359\n",
      "One iteration: 1691951288.546008\n",
      "One iteration: 1691951288.548766\n",
      "One iteration: 1691951288.551883\n",
      "One iteration: 1691951288.5549362\n",
      "One iteration: 1691951288.5579462\n",
      "One iteration: 1691951288.561225\n",
      "One iteration: 1691951288.564118\n",
      "One iteration: 1691951288.566895\n",
      "One iteration: 1691951288.569721\n",
      "One iteration: 1691951288.57265\n",
      "One iteration: 1691951288.575455\n",
      "One iteration: 1691951288.578601\n",
      "One iteration: 1691951288.5812302\n",
      "One iteration: 1691951288.5840309\n",
      "One iteration: 1691951288.586729\n",
      "One iteration: 1691951288.589921\n",
      "One iteration: 1691951288.593706\n",
      "One iteration: 1691951288.597803\n",
      "One iteration: 1691951288.601609\n",
      "One iteration: 1691951288.6052532\n",
      "One iteration: 1691951288.608706\n",
      "One iteration: 1691951288.61231\n",
      "One iteration: 1691951288.61643\n",
      "One iteration: 1691951288.6195269\n",
      "One iteration: 1691951288.622535\n",
      "One iteration: 1691951288.625386\n",
      "One iteration: 1691951288.628721\n",
      "One iteration: 1691951288.6313689\n",
      "One iteration: 1691951288.634256\n",
      "One iteration: 1691951288.637141\n",
      "One iteration: 1691951288.640327\n",
      "One iteration: 1691951288.644361\n",
      "One iteration: 1691951288.6480918\n",
      "One iteration: 1691951288.651516\n",
      "One iteration: 1691951288.654558\n",
      "One iteration: 1691951288.6576488\n",
      "One iteration: 1691951288.6609972\n",
      "One iteration: 1691951288.664683\n",
      "One iteration: 1691951288.667747\n",
      "One iteration: 1691951288.670604\n",
      "One iteration: 1691951288.673599\n",
      "One iteration: 1691951288.676704\n",
      "One iteration: 1691951288.679626\n",
      "One iteration: 1691951288.682442\n",
      "One iteration: 1691951288.6853812\n",
      "One iteration: 1691951288.688294\n",
      "One iteration: 1691951288.691322\n",
      "One iteration: 1691951288.694504\n",
      "One iteration: 1691951288.69715\n",
      "One iteration: 1691951288.6999528\n",
      "One iteration: 1691951288.7026792\n",
      "One iteration: 1691951288.705699\n",
      "One iteration: 1691951288.709017\n",
      "One iteration: 1691951288.7119591\n",
      "One iteration: 1691951288.714714\n",
      "One iteration: 1691951288.71759\n",
      "One iteration: 1691951288.720303\n",
      "One iteration: 1691951288.7233129\n",
      "One iteration: 1691951288.7263339\n",
      "One iteration: 1691951288.729341\n",
      "One iteration: 1691951288.732474\n",
      "One iteration: 1691951288.735408\n",
      "One iteration: 1691951288.738463\n",
      "One iteration: 1691951288.741241\n",
      "One iteration: 1691951288.74429\n",
      "One iteration: 1691951288.74704\n",
      "One iteration: 1691951288.749795\n",
      "One iteration: 1691951288.752466\n",
      "One iteration: 1691951288.755421\n",
      "One iteration: 1691951288.7583811\n",
      "One iteration: 1691951288.7614532\n",
      "One iteration: 1691951288.7642372\n",
      "One iteration: 1691951288.767255\n",
      "One iteration: 1691951288.77015\n",
      "One iteration: 1691951288.7734232\n",
      "One iteration: 1691951288.777088\n",
      "One iteration: 1691951288.780504\n",
      "One iteration: 1691951288.784128\n",
      "One iteration: 1691951288.7872891\n",
      "One iteration: 1691951288.790047\n",
      "One iteration: 1691951288.793633\n",
      "One iteration: 1691951288.797188\n",
      "One iteration: 1691951288.8008761\n",
      "One iteration: 1691951288.8041012\n",
      "One iteration: 1691951288.807229\n",
      "One iteration: 1691951288.8103411\n",
      "One iteration: 1691951288.813874\n",
      "One iteration: 1691951288.817935\n",
      "One iteration: 1691951288.822009\n",
      "One iteration: 1691951288.825325\n",
      "One iteration: 1691951288.8283641\n",
      "One iteration: 1691951288.831218\n",
      "One iteration: 1691951288.834677\n",
      "One iteration: 1691951288.8379579\n",
      "One iteration: 1691951288.8417332\n",
      "One iteration: 1691951288.844838\n",
      "One iteration: 1691951288.847767\n",
      "One iteration: 1691951288.851412\n",
      "One iteration: 1691951288.854382\n",
      "One iteration: 1691951288.857728\n",
      "One iteration: 1691951288.8611882\n",
      "One iteration: 1691951288.863738\n",
      "One iteration: 1691951288.866873\n",
      "One iteration: 1691951288.869715\n",
      "One iteration: 1691951288.872812\n",
      "One iteration: 1691951288.876858\n",
      "One iteration: 1691951288.879848\n",
      "One iteration: 1691951288.882678\n",
      "One iteration: 1691951288.885548\n",
      "One iteration: 1691951288.888565\n",
      "One iteration: 1691951288.892122\n",
      "One iteration: 1691951288.894846\n",
      "One iteration: 1691951288.897401\n",
      "One iteration: 1691951288.900197\n",
      "One iteration: 1691951288.9029918\n",
      "One iteration: 1691951288.9062889\n",
      "One iteration: 1691951288.9097521\n",
      "One iteration: 1691951288.9123971\n",
      "One iteration: 1691951288.915097\n",
      "One iteration: 1691951288.9178822\n",
      "One iteration: 1691951288.92063\n",
      "One iteration: 1691951288.92375\n",
      "One iteration: 1691951288.926961\n",
      "One iteration: 1691951288.929706\n",
      "One iteration: 1691951288.9326909\n",
      "One iteration: 1691951288.9355378\n",
      "One iteration: 1691951288.938505\n",
      "One iteration: 1691951288.941802\n",
      "One iteration: 1691951288.944457\n",
      "One iteration: 1691951288.947069\n",
      "One iteration: 1691951288.9499578\n",
      "One iteration: 1691951288.952694\n",
      "One iteration: 1691951288.95595\n",
      "One iteration: 1691951288.959081\n",
      "One iteration: 1691951288.9616268\n",
      "One iteration: 1691951288.9641302\n",
      "One iteration: 1691951288.9672902\n",
      "One iteration: 1691951288.9702039\n",
      "One iteration: 1691951288.973388\n",
      "One iteration: 1691951288.976543\n",
      "One iteration: 1691951288.9794502\n",
      "One iteration: 1691951288.982166\n",
      "One iteration: 1691951288.9849112\n",
      "One iteration: 1691951288.987912\n",
      "One iteration: 1691951288.991348\n",
      "One iteration: 1691951288.9947982\n",
      "One iteration: 1691951288.9974332\n",
      "One iteration: 1691951289.0005271\n",
      "One iteration: 1691951289.00328\n",
      "One iteration: 1691951289.006175\n",
      "One iteration: 1691951289.009158\n",
      "One iteration: 1691951289.012393\n",
      "One iteration: 1691951289.015\n",
      "One iteration: 1691951289.0178041\n",
      "One iteration: 1691951289.0216992\n",
      "One iteration: 1691951289.024927\n",
      "One iteration: 1691951289.028329\n",
      "One iteration: 1691951289.030997\n",
      "One iteration: 1691951289.03398\n",
      "One iteration: 1691951289.0368278\n",
      "One iteration: 1691951289.040138\n",
      "One iteration: 1691951289.043686\n",
      "One iteration: 1691951289.046773\n",
      "One iteration: 1691951289.050072\n",
      "One iteration: 1691951289.053169\n",
      "One iteration: 1691951289.0563378\n",
      "One iteration: 1691951289.059408\n",
      "One iteration: 1691951289.062571\n",
      "One iteration: 1691951289.0658529\n",
      "One iteration: 1691951289.068584\n",
      "One iteration: 1691951289.071334\n",
      "One iteration: 1691951289.073884\n",
      "One iteration: 1691951289.07648\n",
      "One iteration: 1691951289.079123\n",
      "One iteration: 1691951289.081662\n",
      "One iteration: 1691951289.084511\n",
      "One iteration: 1691951289.087224\n",
      "One iteration: 1691951289.090071\n",
      "One iteration: 1691951289.092871\n",
      "One iteration: 1691951289.096069\n",
      "One iteration: 1691951289.0993762\n",
      "One iteration: 1691951289.1030972\n",
      "One iteration: 1691951289.1067252\n",
      "One iteration: 1691951289.109847\n",
      "One iteration: 1691951289.113015\n",
      "One iteration: 1691951289.1159968\n",
      "One iteration: 1691951289.118765\n",
      "One iteration: 1691951289.121521\n",
      "One iteration: 1691951289.1244411\n",
      "One iteration: 1691951289.1273322\n",
      "One iteration: 1691951289.130359\n",
      "One iteration: 1691951289.133232\n",
      "One iteration: 1691951289.135968\n",
      "One iteration: 1691951289.138728\n",
      "One iteration: 1691951289.141502\n",
      "One iteration: 1691951289.14469\n",
      "One iteration: 1691951289.147348\n",
      "One iteration: 1691951289.15028\n",
      "One iteration: 1691951289.1529129\n",
      "One iteration: 1691951289.156009\n",
      "One iteration: 1691951289.1590579\n",
      "One iteration: 1691951289.162306\n",
      "One iteration: 1691951289.165072\n",
      "One iteration: 1691951289.167815\n",
      "One iteration: 1691951289.170476\n",
      "One iteration: 1691951289.1732218\n",
      "One iteration: 1691951289.176395\n",
      "One iteration: 1691951289.1793642\n",
      "One iteration: 1691951289.1819992\n",
      "One iteration: 1691951289.184766\n",
      "One iteration: 1691951289.187902\n",
      "One iteration: 1691951289.1908321\n",
      "One iteration: 1691951289.193986\n",
      "One iteration: 1691951289.196764\n",
      "One iteration: 1691951289.199575\n",
      "One iteration: 1691951289.2022662\n",
      "One iteration: 1691951289.2051258\n",
      "One iteration: 1691951289.2081048\n",
      "One iteration: 1691951289.211216\n",
      "One iteration: 1691951289.213838\n",
      "One iteration: 1691951289.216625\n",
      "One iteration: 1691951289.219255\n",
      "One iteration: 1691951289.222249\n",
      "One iteration: 1691951289.226285\n",
      "One iteration: 1691951289.22995\n",
      "One iteration: 1691951289.232735\n",
      "One iteration: 1691951289.2354078\n",
      "One iteration: 1691951289.238314\n",
      "One iteration: 1691951289.241209\n",
      "One iteration: 1691951289.244784\n",
      "One iteration: 1691951289.247781\n",
      "One iteration: 1691951289.251342\n",
      "One iteration: 1691951289.254696\n",
      "One iteration: 1691951289.257828\n",
      "One iteration: 1691951289.2608151\n",
      "One iteration: 1691951289.263873\n",
      "One iteration: 1691951289.267209\n",
      "One iteration: 1691951289.270018\n",
      "One iteration: 1691951289.273063\n",
      "One iteration: 1691951289.2762299\n",
      "One iteration: 1691951289.279295\n",
      "One iteration: 1691951289.282038\n",
      "One iteration: 1691951289.284807\n",
      "One iteration: 1691951289.2877078\n",
      "One iteration: 1691951289.290436\n",
      "One iteration: 1691951289.2937338\n",
      "One iteration: 1691951289.296578\n",
      "One iteration: 1691951289.299447\n",
      "One iteration: 1691951289.3021598\n",
      "One iteration: 1691951289.305075\n",
      "One iteration: 1691951289.3081331\n",
      "One iteration: 1691951289.3114529\n",
      "One iteration: 1691951289.314187\n",
      "One iteration: 1691951289.317041\n",
      "One iteration: 1691951289.3197892\n",
      "One iteration: 1691951289.322936\n",
      "One iteration: 1691951289.326019\n",
      "One iteration: 1691951289.3287911\n",
      "One iteration: 1691951289.331242\n",
      "One iteration: 1691951289.334432\n",
      "One iteration: 1691951289.3372731\n",
      "One iteration: 1691951289.340234\n",
      "One iteration: 1691951289.3433359\n",
      "One iteration: 1691951289.34655\n",
      "One iteration: 1691951289.349325\n",
      "One iteration: 1691951289.352032\n",
      "One iteration: 1691951289.35485\n",
      "One iteration: 1691951289.35775\n",
      "One iteration: 1691951289.3608341\n",
      "One iteration: 1691951289.3635762\n",
      "One iteration: 1691951289.366335\n",
      "One iteration: 1691951289.368984\n",
      "One iteration: 1691951289.371823\n",
      "One iteration: 1691951289.37468\n",
      "One iteration: 1691951289.3777988\n",
      "One iteration: 1691951289.380565\n",
      "One iteration: 1691951289.3833652\n",
      "One iteration: 1691951289.386034\n",
      "One iteration: 1691951289.389194\n",
      "One iteration: 1691951289.392262\n",
      "One iteration: 1691951289.39535\n",
      "One iteration: 1691951289.3979352\n",
      "One iteration: 1691951289.400749\n",
      "One iteration: 1691951289.403413\n",
      "One iteration: 1691951289.406354\n",
      "One iteration: 1691951289.409187\n",
      "One iteration: 1691951289.4123101\n",
      "One iteration: 1691951289.415086\n",
      "One iteration: 1691951289.417839\n",
      "One iteration: 1691951289.420521\n",
      "One iteration: 1691951289.423412\n",
      "One iteration: 1691951289.426282\n",
      "One iteration: 1691951289.429842\n",
      "One iteration: 1691951289.433247\n",
      "One iteration: 1691951289.4360409\n",
      "One iteration: 1691951289.4390411\n",
      "One iteration: 1691951289.441803\n",
      "One iteration: 1691951289.4448571\n",
      "One iteration: 1691951289.447561\n",
      "One iteration: 1691951289.450311\n",
      "One iteration: 1691951289.453231\n",
      "One iteration: 1691951289.456363\n",
      "One iteration: 1691951289.459881\n",
      "One iteration: 1691951289.463328\n",
      "One iteration: 1691951289.466581\n",
      "One iteration: 1691951289.469398\n",
      "One iteration: 1691951289.472394\n",
      "One iteration: 1691951289.475216\n",
      "One iteration: 1691951289.4785438\n",
      "One iteration: 1691951289.481108\n",
      "One iteration: 1691951289.4840572\n",
      "One iteration: 1691951289.4870348\n",
      "One iteration: 1691951289.4899478\n",
      "One iteration: 1691951289.492863\n",
      "One iteration: 1691951289.496043\n",
      "One iteration: 1691951289.498882\n",
      "One iteration: 1691951289.501804\n",
      "One iteration: 1691951289.504426\n",
      "One iteration: 1691951289.507412\n",
      "One iteration: 1691951289.510564\n",
      "One iteration: 1691951289.5135539\n",
      "One iteration: 1691951289.516456\n",
      "One iteration: 1691951289.51915\n",
      "One iteration: 1691951289.5222442\n",
      "One iteration: 1691951289.5293791\n",
      "One iteration: 1691951289.538956\n",
      "One iteration: 1691951289.5504398\n",
      "One iteration: 1691951289.5566442\n",
      "One iteration: 1691951289.5630999\n",
      "One iteration: 1691951289.56631\n",
      "One iteration: 1691951289.56988\n",
      "One iteration: 1691951289.5741482\n",
      "One iteration: 1691951289.578966\n",
      "One iteration: 1691951289.5824902\n",
      "One iteration: 1691951289.586043\n",
      "One iteration: 1691951289.597166\n",
      "One iteration: 1691951289.617913\n",
      "One iteration: 1691951289.626672\n",
      "One iteration: 1691951289.631636\n",
      "One iteration: 1691951289.635623\n",
      "One iteration: 1691951289.6397371\n",
      "One iteration: 1691951289.644147\n",
      "One iteration: 1691951289.647431\n",
      "One iteration: 1691951289.650889\n",
      "One iteration: 1691951289.65413\n",
      "One iteration: 1691951289.6575491\n",
      "One iteration: 1691951289.661105\n",
      "One iteration: 1691951289.664581\n",
      "One iteration: 1691951289.668001\n",
      "One iteration: 1691951289.6709251\n",
      "One iteration: 1691951289.674177\n",
      "One iteration: 1691951289.677313\n",
      "One iteration: 1691951289.6802392\n",
      "One iteration: 1691951289.682993\n",
      "One iteration: 1691951289.685724\n",
      "One iteration: 1691951289.6886568\n",
      "One iteration: 1691951289.6917398\n",
      "One iteration: 1691951289.695078\n",
      "One iteration: 1691951289.698364\n",
      "One iteration: 1691951289.7013178\n",
      "One iteration: 1691951289.704004\n",
      "One iteration: 1691951289.70735\n",
      "One iteration: 1691951289.710647\n",
      "One iteration: 1691951289.7138891\n",
      "One iteration: 1691951289.7169\n",
      "One iteration: 1691951289.7196312\n",
      "One iteration: 1691951289.7226958\n",
      "One iteration: 1691951289.725546\n",
      "One iteration: 1691951289.728645\n",
      "One iteration: 1691951289.731337\n",
      "One iteration: 1691951289.7344809\n",
      "One iteration: 1691951289.737231\n",
      "One iteration: 1691951289.7403488\n",
      "One iteration: 1691951289.743666\n",
      "One iteration: 1691951289.746872\n",
      "One iteration: 1691951289.7497191\n",
      "One iteration: 1691951289.752625\n",
      "One iteration: 1691951289.755648\n",
      "One iteration: 1691951289.758726\n",
      "One iteration: 1691951289.761804\n",
      "One iteration: 1691951289.765034\n",
      "One iteration: 1691951289.767822\n",
      "One iteration: 1691951289.770578\n",
      "One iteration: 1691951289.7733629\n",
      "One iteration: 1691951289.776532\n",
      "One iteration: 1691951289.779556\n",
      "One iteration: 1691951289.782275\n",
      "One iteration: 1691951289.785079\n",
      "One iteration: 1691951289.788249\n",
      "One iteration: 1691951289.791083\n",
      "One iteration: 1691951289.794181\n",
      "One iteration: 1691951289.797049\n",
      "One iteration: 1691951289.800107\n",
      "One iteration: 1691951289.802749\n",
      "One iteration: 1691951289.8058822\n",
      "One iteration: 1691951289.808667\n",
      "One iteration: 1691951289.8121119\n",
      "One iteration: 1691951289.8147268\n",
      "One iteration: 1691951289.8175209\n",
      "One iteration: 1691951289.820128\n",
      "One iteration: 1691951289.823623\n",
      "One iteration: 1691951289.826732\n",
      "One iteration: 1691951289.829983\n",
      "One iteration: 1691951289.833544\n",
      "One iteration: 1691951289.836749\n",
      "One iteration: 1691951289.839563\n",
      "One iteration: 1691951289.8424242\n",
      "One iteration: 1691951289.845688\n",
      "One iteration: 1691951289.848499\n",
      "One iteration: 1691951289.851237\n",
      "One iteration: 1691951289.853884\n",
      "One iteration: 1691951289.8569658\n",
      "One iteration: 1691951289.860161\n",
      "One iteration: 1691951289.863137\n",
      "One iteration: 1691951289.866335\n",
      "One iteration: 1691951289.869478\n",
      "One iteration: 1691951289.872511\n",
      "One iteration: 1691951289.87539\n",
      "One iteration: 1691951289.878391\n",
      "One iteration: 1691951289.8814678\n",
      "One iteration: 1691951289.8845131\n",
      "One iteration: 1691951289.8872309\n",
      "One iteration: 1691951289.890023\n",
      "One iteration: 1691951289.893253\n",
      "One iteration: 1691951289.89631\n",
      "One iteration: 1691951289.899267\n",
      "One iteration: 1691951289.902164\n",
      "One iteration: 1691951289.905298\n",
      "One iteration: 1691951289.9084811\n",
      "One iteration: 1691951289.911836\n",
      "One iteration: 1691951289.914476\n",
      "One iteration: 1691951289.917162\n",
      "One iteration: 1691951289.9199429\n",
      "One iteration: 1691951289.922839\n",
      "One iteration: 1691951289.92562\n",
      "One iteration: 1691951289.928758\n",
      "One iteration: 1691951289.931434\n",
      "One iteration: 1691951289.934654\n",
      "One iteration: 1691951289.937406\n",
      "One iteration: 1691951289.940237\n",
      "One iteration: 1691951289.943265\n",
      "One iteration: 1691951289.946371\n",
      "One iteration: 1691951289.949138\n",
      "One iteration: 1691951289.952019\n",
      "One iteration: 1691951289.954848\n",
      "One iteration: 1691951289.95779\n",
      "One iteration: 1691951289.96095\n",
      "One iteration: 1691951289.963624\n",
      "One iteration: 1691951289.9664881\n",
      "One iteration: 1691951289.9692538\n",
      "One iteration: 1691951289.972354\n",
      "One iteration: 1691951289.9750881\n",
      "One iteration: 1691951289.978127\n",
      "One iteration: 1691951289.9807382\n",
      "One iteration: 1691951289.983613\n",
      "One iteration: 1691951289.9863198\n",
      "One iteration: 1691951289.989378\n",
      "One iteration: 1691951289.9922292\n",
      "One iteration: 1691951289.9955819\n",
      "One iteration: 1691951289.9983401\n",
      "One iteration: 1691951290.0011652\n",
      "One iteration: 1691951290.0037818\n",
      "One iteration: 1691951290.006635\n",
      "One iteration: 1691951290.009778\n",
      "One iteration: 1691951290.012685\n",
      "One iteration: 1691951290.015306\n",
      "One iteration: 1691951290.0182002\n",
      "One iteration: 1691951290.0208561\n",
      "One iteration: 1691951290.023852\n",
      "One iteration: 1691951290.026956\n",
      "One iteration: 1691951290.02979\n",
      "One iteration: 1691951290.03248\n",
      "One iteration: 1691951290.035816\n",
      "One iteration: 1691951290.039598\n",
      "One iteration: 1691951290.0425558\n",
      "One iteration: 1691951290.045654\n",
      "One iteration: 1691951290.0484369\n",
      "One iteration: 1691951290.0512152\n",
      "One iteration: 1691951290.053837\n",
      "One iteration: 1691951290.056711\n",
      "One iteration: 1691951290.059811\n",
      "One iteration: 1691951290.0634181\n",
      "One iteration: 1691951290.066646\n",
      "One iteration: 1691951290.0699499\n",
      "One iteration: 1691951290.073173\n",
      "One iteration: 1691951290.0761092\n",
      "One iteration: 1691951290.079006\n",
      "One iteration: 1691951290.0822542\n",
      "One iteration: 1691951290.085088\n",
      "One iteration: 1691951290.087698\n",
      "One iteration: 1691951290.090275\n",
      "One iteration: 1691951290.09298\n",
      "One iteration: 1691951290.095627\n",
      "One iteration: 1691951290.09822\n",
      "One iteration: 1691951290.101284\n",
      "One iteration: 1691951290.104644\n",
      "One iteration: 1691951290.107588\n",
      "One iteration: 1691951290.110898\n",
      "One iteration: 1691951290.1136231\n",
      "One iteration: 1691951290.116989\n",
      "One iteration: 1691951290.119732\n",
      "One iteration: 1691951290.1225789\n",
      "One iteration: 1691951290.125221\n",
      "One iteration: 1691951290.128646\n",
      "One iteration: 1691951290.131813\n",
      "One iteration: 1691951290.1349702\n",
      "One iteration: 1691951290.137722\n",
      "One iteration: 1691951290.140606\n",
      "One iteration: 1691951290.143309\n",
      "One iteration: 1691951290.146434\n",
      "One iteration: 1691951290.149282\n",
      "One iteration: 1691951290.1520169\n",
      "One iteration: 1691951290.154892\n",
      "One iteration: 1691951290.157693\n",
      "One iteration: 1691951290.160523\n",
      "One iteration: 1691951290.163714\n",
      "One iteration: 1691951290.166601\n",
      "One iteration: 1691951290.169298\n",
      "One iteration: 1691951290.172311\n",
      "One iteration: 1691951290.17502\n",
      "One iteration: 1691951290.177993\n",
      "One iteration: 1691951290.181098\n",
      "One iteration: 1691951290.184109\n",
      "One iteration: 1691951290.186822\n",
      "One iteration: 1691951290.189719\n",
      "One iteration: 1691951290.1924338\n",
      "One iteration: 1691951290.195617\n",
      "One iteration: 1691951290.19859\n",
      "One iteration: 1691951290.201605\n",
      "One iteration: 1691951290.2042298\n",
      "One iteration: 1691951290.207006\n",
      "One iteration: 1691951290.209732\n",
      "One iteration: 1691951290.212726\n",
      "One iteration: 1691951290.215523\n",
      "One iteration: 1691951290.218296\n",
      "One iteration: 1691951290.2209191\n",
      "One iteration: 1691951290.223763\n",
      "One iteration: 1691951290.2265139\n",
      "One iteration: 1691951290.229555\n",
      "One iteration: 1691951290.232363\n",
      "One iteration: 1691951290.235104\n",
      "One iteration: 1691951290.238858\n",
      "One iteration: 1691951290.2423801\n",
      "One iteration: 1691951290.24535\n",
      "One iteration: 1691951290.248542\n",
      "One iteration: 1691951290.2514172\n",
      "One iteration: 1691951290.254068\n",
      "One iteration: 1691951290.256845\n",
      "One iteration: 1691951290.259893\n",
      "One iteration: 1691951290.263096\n",
      "One iteration: 1691951290.265995\n",
      "One iteration: 1691951290.268873\n",
      "One iteration: 1691951290.272025\n",
      "One iteration: 1691951290.274998\n",
      "One iteration: 1691951290.277921\n",
      "One iteration: 1691951290.281068\n",
      "One iteration: 1691951290.284491\n",
      "One iteration: 1691951290.287286\n",
      "One iteration: 1691951290.290045\n",
      "One iteration: 1691951290.2927518\n",
      "One iteration: 1691951290.2953348\n",
      "One iteration: 1691951290.297887\n",
      "One iteration: 1691951290.3011332\n",
      "One iteration: 1691951290.3038201\n",
      "One iteration: 1691951290.3069508\n",
      "One iteration: 1691951290.309991\n",
      "One iteration: 1691951290.313201\n",
      "One iteration: 1691951290.316129\n",
      "One iteration: 1691951290.3188758\n",
      "One iteration: 1691951290.32185\n",
      "One iteration: 1691951290.324544\n",
      "One iteration: 1691951290.327712\n",
      "One iteration: 1691951290.330723\n",
      "One iteration: 1691951290.3337731\n",
      "One iteration: 1691951290.336488\n",
      "One iteration: 1691951290.33937\n",
      "One iteration: 1691951290.342016\n",
      "One iteration: 1691951290.3452811\n",
      "One iteration: 1691951290.348453\n",
      "One iteration: 1691951290.351276\n",
      "One iteration: 1691951290.3539\n",
      "One iteration: 1691951290.356769\n",
      "One iteration: 1691951290.35945\n",
      "One iteration: 1691951290.3628879\n",
      "One iteration: 1691951290.3656929\n",
      "One iteration: 1691951290.368493\n",
      "One iteration: 1691951290.371134\n",
      "One iteration: 1691951290.3740091\n",
      "One iteration: 1691951290.376713\n",
      "One iteration: 1691951290.3797069\n",
      "One iteration: 1691951290.382682\n",
      "One iteration: 1691951290.385421\n",
      "One iteration: 1691951290.3881888\n",
      "One iteration: 1691951290.390917\n",
      "One iteration: 1691951290.393569\n",
      "One iteration: 1691951290.396627\n",
      "One iteration: 1691951290.399521\n",
      "One iteration: 1691951290.4022138\n",
      "One iteration: 1691951290.40502\n",
      "One iteration: 1691951290.407906\n",
      "One iteration: 1691951290.410891\n",
      "One iteration: 1691951290.414017\n",
      "One iteration: 1691951290.4170299\n",
      "One iteration: 1691951290.419768\n",
      "One iteration: 1691951290.422752\n",
      "One iteration: 1691951290.425399\n",
      "One iteration: 1691951290.4283268\n",
      "One iteration: 1691951290.431297\n",
      "One iteration: 1691951290.434156\n",
      "One iteration: 1691951290.436776\n",
      "One iteration: 1691951290.439618\n",
      "One iteration: 1691951290.443382\n",
      "One iteration: 1691951290.446817\n",
      "One iteration: 1691951290.449714\n",
      "One iteration: 1691951290.45235\n",
      "One iteration: 1691951290.455151\n",
      "One iteration: 1691951290.458092\n",
      "One iteration: 1691951290.461164\n",
      "One iteration: 1691951290.46458\n",
      "One iteration: 1691951290.467676\n",
      "One iteration: 1691951290.470571\n",
      "One iteration: 1691951290.4737868\n",
      "One iteration: 1691951290.476824\n",
      "One iteration: 1691951290.480076\n",
      "One iteration: 1691951290.483062\n",
      "One iteration: 1691951290.485777\n",
      "One iteration: 1691951290.488799\n",
      "One iteration: 1691951290.4915829\n",
      "One iteration: 1691951290.4948559\n",
      "One iteration: 1691951290.4979131\n",
      "One iteration: 1691951290.5007539\n",
      "One iteration: 1691951290.503393\n",
      "One iteration: 1691951290.5065842\n",
      "One iteration: 1691951290.50959\n",
      "One iteration: 1691951290.512924\n",
      "One iteration: 1691951290.51566\n",
      "One iteration: 1691951290.518508\n",
      "One iteration: 1691951290.5212202\n",
      "One iteration: 1691951290.523964\n",
      "One iteration: 1691951290.526922\n",
      "One iteration: 1691951290.5298119\n",
      "One iteration: 1691951290.532539\n",
      "One iteration: 1691951290.535286\n",
      "One iteration: 1691951290.5380638\n",
      "One iteration: 1691951290.540926\n",
      "One iteration: 1691951290.54414\n",
      "One iteration: 1691951290.546794\n",
      "One iteration: 1691951290.549918\n",
      "One iteration: 1691951290.552635\n",
      "One iteration: 1691951290.5555992\n",
      "One iteration: 1691951290.558557\n",
      "One iteration: 1691951290.561651\n",
      "One iteration: 1691951290.564351\n",
      "One iteration: 1691951290.567235\n",
      "One iteration: 1691951290.570015\n",
      "One iteration: 1691951290.572961\n",
      "One iteration: 1691951290.575828\n",
      "One iteration: 1691951290.578996\n",
      "One iteration: 1691951290.58179\n",
      "One iteration: 1691951290.584543\n",
      "One iteration: 1691951290.587244\n",
      "One iteration: 1691951290.590215\n",
      "One iteration: 1691951290.593329\n",
      "One iteration: 1691951290.596566\n",
      "One iteration: 1691951290.599988\n",
      "One iteration: 1691951290.604008\n",
      "One iteration: 1691951290.607649\n",
      "One iteration: 1691951290.610678\n",
      "One iteration: 1691951290.613889\n",
      "One iteration: 1691951290.617127\n",
      "One iteration: 1691951290.619832\n",
      "One iteration: 1691951290.622795\n",
      "One iteration: 1691951290.6255271\n",
      "One iteration: 1691951290.628524\n",
      "One iteration: 1691951290.631437\n",
      "One iteration: 1691951290.634309\n",
      "One iteration: 1691951290.636971\n",
      "One iteration: 1691951290.639914\n",
      "One iteration: 1691951290.642842\n",
      "One iteration: 1691951290.647062\n",
      "One iteration: 1691951290.65081\n",
      "One iteration: 1691951290.6535392\n",
      "One iteration: 1691951290.6563818\n",
      "One iteration: 1691951290.659382\n",
      "One iteration: 1691951290.662525\n",
      "One iteration: 1691951290.66525\n",
      "One iteration: 1691951290.6681988\n",
      "One iteration: 1691951290.6709461\n",
      "One iteration: 1691951290.674291\n",
      "One iteration: 1691951290.67779\n",
      "One iteration: 1691951290.680403\n",
      "One iteration: 1691951290.683476\n",
      "One iteration: 1691951290.686529\n",
      "One iteration: 1691951290.689756\n",
      "One iteration: 1691951290.692575\n",
      "One iteration: 1691951290.6955452\n",
      "One iteration: 1691951290.699021\n",
      "One iteration: 1691951290.7019231\n",
      "One iteration: 1691951290.704812\n",
      "One iteration: 1691951290.707773\n",
      "One iteration: 1691951290.710944\n",
      "One iteration: 1691951290.713642\n",
      "One iteration: 1691951290.7164428\n",
      "One iteration: 1691951290.7192068\n",
      "One iteration: 1691951290.722235\n",
      "One iteration: 1691951290.725034\n",
      "One iteration: 1691951290.728232\n",
      "One iteration: 1691951290.730901\n",
      "One iteration: 1691951290.733985\n",
      "One iteration: 1691951290.736703\n",
      "One iteration: 1691951290.7396312\n",
      "One iteration: 1691951290.7427998\n",
      "One iteration: 1691951290.7458959\n",
      "One iteration: 1691951290.748689\n",
      "One iteration: 1691951290.751453\n",
      "One iteration: 1691951290.7540731\n",
      "One iteration: 1691951290.757317\n",
      "One iteration: 1691951290.76043\n",
      "One iteration: 1691951290.763356\n",
      "One iteration: 1691951290.76609\n",
      "One iteration: 1691951290.768724\n",
      "One iteration: 1691951290.771604\n",
      "One iteration: 1691951290.774282\n",
      "One iteration: 1691951290.77748\n",
      "One iteration: 1691951290.780096\n",
      "One iteration: 1691951290.7827961\n",
      "One iteration: 1691951290.7854102\n",
      "One iteration: 1691951290.788207\n",
      "One iteration: 1691951290.791067\n",
      "One iteration: 1691951290.7941608\n",
      "One iteration: 1691951290.796702\n",
      "One iteration: 1691951290.7996202\n",
      "One iteration: 1691951290.8022652\n",
      "One iteration: 1691951290.805054\n",
      "One iteration: 1691951290.8078148\n",
      "One iteration: 1691951290.810901\n",
      "One iteration: 1691951290.813575\n",
      "One iteration: 1691951290.816354\n",
      "One iteration: 1691951290.818961\n",
      "One iteration: 1691951290.821822\n",
      "One iteration: 1691951290.8244622\n",
      "One iteration: 1691951290.827623\n",
      "One iteration: 1691951290.830236\n",
      "One iteration: 1691951290.833086\n",
      "One iteration: 1691951290.835825\n",
      "One iteration: 1691951290.838629\n",
      "One iteration: 1691951290.841163\n",
      "One iteration: 1691951290.8441741\n",
      "One iteration: 1691951290.846731\n",
      "One iteration: 1691951290.850323\n",
      "One iteration: 1691951290.853877\n",
      "One iteration: 1691951290.857024\n",
      "One iteration: 1691951290.8601532\n",
      "One iteration: 1691951290.863439\n",
      "One iteration: 1691951290.866373\n",
      "One iteration: 1691951290.869057\n",
      "One iteration: 1691951290.87198\n",
      "One iteration: 1691951290.874718\n",
      "One iteration: 1691951290.8779619\n",
      "One iteration: 1691951290.881258\n",
      "One iteration: 1691951290.8845062\n",
      "One iteration: 1691951290.887509\n",
      "One iteration: 1691951290.8903968\n",
      "One iteration: 1691951290.8933198\n",
      "One iteration: 1691951290.896993\n",
      "One iteration: 1691951290.90004\n",
      "One iteration: 1691951290.902814\n",
      "One iteration: 1691951290.905678\n",
      "One iteration: 1691951290.908591\n",
      "One iteration: 1691951290.912044\n",
      "One iteration: 1691951290.914637\n",
      "One iteration: 1691951290.917442\n",
      "One iteration: 1691951290.920228\n",
      "One iteration: 1691951290.923265\n",
      "One iteration: 1691951290.926355\n",
      "One iteration: 1691951290.929413\n",
      "One iteration: 1691951290.932103\n",
      "One iteration: 1691951290.934817\n",
      "One iteration: 1691951290.9374762\n",
      "One iteration: 1691951290.940327\n",
      "One iteration: 1691951290.943349\n",
      "One iteration: 1691951290.946531\n",
      "One iteration: 1691951290.950005\n",
      "One iteration: 1691951290.9527738\n",
      "One iteration: 1691951290.955793\n",
      "One iteration: 1691951290.9587462\n",
      "One iteration: 1691951290.961946\n",
      "One iteration: 1691951290.9646962\n",
      "One iteration: 1691951290.9675741\n",
      "One iteration: 1691951290.970355\n",
      "One iteration: 1691951290.97356\n",
      "One iteration: 1691951290.976761\n",
      "One iteration: 1691951290.979887\n",
      "One iteration: 1691951290.983042\n",
      "One iteration: 1691951290.985827\n",
      "One iteration: 1691951290.988708\n",
      "One iteration: 1691951290.9918962\n",
      "One iteration: 1691951290.995093\n",
      "One iteration: 1691951290.997768\n",
      "One iteration: 1691951291.000597\n",
      "One iteration: 1691951291.003316\n",
      "One iteration: 1691951291.006405\n",
      "One iteration: 1691951291.0097558\n",
      "One iteration: 1691951291.01301\n",
      "One iteration: 1691951291.0159528\n",
      "One iteration: 1691951291.018778\n",
      "One iteration: 1691951291.021827\n",
      "One iteration: 1691951291.024527\n",
      "One iteration: 1691951291.02762\n",
      "One iteration: 1691951291.030575\n",
      "One iteration: 1691951291.033314\n",
      "One iteration: 1691951291.0359411\n",
      "One iteration: 1691951291.038814\n",
      "One iteration: 1691951291.041627\n",
      "One iteration: 1691951291.044749\n",
      "One iteration: 1691951291.047334\n",
      "One iteration: 1691951291.050128\n",
      "One iteration: 1691951291.053602\n",
      "One iteration: 1691951291.057329\n",
      "One iteration: 1691951291.060731\n",
      "One iteration: 1691951291.0637472\n",
      "One iteration: 1691951291.066868\n",
      "One iteration: 1691951291.0695832\n",
      "One iteration: 1691951291.072609\n",
      "One iteration: 1691951291.0753422\n",
      "One iteration: 1691951291.0785909\n",
      "One iteration: 1691951291.0813692\n",
      "One iteration: 1691951291.0842931\n",
      "One iteration: 1691951291.087334\n",
      "One iteration: 1691951291.090276\n",
      "One iteration: 1691951291.093214\n",
      "One iteration: 1691951291.096046\n",
      "One iteration: 1691951291.099317\n",
      "One iteration: 1691951291.102056\n",
      "One iteration: 1691951291.10484\n",
      "One iteration: 1691951291.107628\n",
      "One iteration: 1691951291.110483\n",
      "One iteration: 1691951291.113146\n",
      "One iteration: 1691951291.115777\n",
      "One iteration: 1691951291.118465\n",
      "One iteration: 1691951291.121195\n",
      "One iteration: 1691951291.124099\n",
      "One iteration: 1691951291.1273239\n",
      "One iteration: 1691951291.129945\n",
      "One iteration: 1691951291.132639\n",
      "One iteration: 1691951291.1354308\n",
      "One iteration: 1691951291.138432\n",
      "One iteration: 1691951291.141175\n",
      "One iteration: 1691951291.144125\n",
      "One iteration: 1691951291.1469598\n",
      "One iteration: 1691951291.150006\n",
      "One iteration: 1691951291.1527958\n",
      "One iteration: 1691951291.155723\n",
      "One iteration: 1691951291.158481\n",
      "One iteration: 1691951291.16155\n",
      "One iteration: 1691951291.164555\n",
      "One iteration: 1691951291.1676748\n",
      "One iteration: 1691951291.17078\n",
      "One iteration: 1691951291.173413\n",
      "One iteration: 1691951291.1760268\n",
      "One iteration: 1691951291.1789708\n",
      "One iteration: 1691951291.182016\n",
      "One iteration: 1691951291.184831\n",
      "One iteration: 1691951291.187531\n",
      "One iteration: 1691951291.19029\n",
      "One iteration: 1691951291.1929631\n",
      "One iteration: 1691951291.1959848\n",
      "One iteration: 1691951291.1989162\n",
      "One iteration: 1691951291.201824\n",
      "One iteration: 1691951291.2044659\n",
      "One iteration: 1691951291.207319\n",
      "One iteration: 1691951291.210111\n",
      "One iteration: 1691951291.2133079\n",
      "One iteration: 1691951291.216028\n",
      "One iteration: 1691951291.218729\n",
      "One iteration: 1691951291.221764\n",
      "One iteration: 1691951291.224579\n",
      "One iteration: 1691951291.227626\n",
      "One iteration: 1691951291.230804\n",
      "One iteration: 1691951291.233757\n",
      "One iteration: 1691951291.236496\n",
      "One iteration: 1691951291.239439\n",
      "One iteration: 1691951291.2423449\n",
      "One iteration: 1691951291.245426\n",
      "One iteration: 1691951291.248571\n",
      "One iteration: 1691951291.25155\n",
      "One iteration: 1691951291.2542\n",
      "One iteration: 1691951291.257927\n",
      "One iteration: 1691951291.261163\n",
      "One iteration: 1691951291.264274\n",
      "One iteration: 1691951291.2671108\n",
      "One iteration: 1691951291.2698681\n",
      "One iteration: 1691951291.2726479\n",
      "One iteration: 1691951291.2754078\n",
      "One iteration: 1691951291.278296\n",
      "One iteration: 1691951291.281347\n",
      "One iteration: 1691951291.284391\n",
      "One iteration: 1691951291.287308\n",
      "One iteration: 1691951291.291297\n",
      "One iteration: 1691951291.2944808\n",
      "One iteration: 1691951291.2974982\n",
      "One iteration: 1691951291.300649\n",
      "One iteration: 1691951291.3050182\n",
      "One iteration: 1691951291.308\n",
      "One iteration: 1691951291.311448\n",
      "One iteration: 1691951291.314539\n",
      "One iteration: 1691951291.3173509\n",
      "One iteration: 1691951291.3201432\n",
      "One iteration: 1691951291.322963\n",
      "One iteration: 1691951291.3258932\n",
      "One iteration: 1691951291.328992\n",
      "One iteration: 1691951291.331771\n",
      "One iteration: 1691951291.334555\n",
      "One iteration: 1691951291.337255\n",
      "One iteration: 1691951291.340255\n",
      "One iteration: 1691951291.343477\n",
      "One iteration: 1691951291.346289\n",
      "One iteration: 1691951291.3491352\n",
      "One iteration: 1691951291.351899\n",
      "One iteration: 1691951291.354717\n",
      "One iteration: 1691951291.357493\n",
      "One iteration: 1691951291.360777\n",
      "One iteration: 1691951291.363542\n",
      "One iteration: 1691951291.3664289\n",
      "One iteration: 1691951291.369198\n",
      "One iteration: 1691951291.372088\n",
      "One iteration: 1691951291.374988\n",
      "One iteration: 1691951291.378182\n",
      "One iteration: 1691951291.3808792\n",
      "One iteration: 1691951291.383851\n",
      "One iteration: 1691951291.386652\n",
      "One iteration: 1691951291.389719\n",
      "One iteration: 1691951291.392599\n",
      "One iteration: 1691951291.395892\n",
      "One iteration: 1691951291.3986151\n",
      "One iteration: 1691951291.401509\n",
      "One iteration: 1691951291.4041002\n",
      "One iteration: 1691951291.407039\n",
      "One iteration: 1691951291.410222\n",
      "One iteration: 1691951291.413159\n",
      "One iteration: 1691951291.416115\n",
      "One iteration: 1691951291.418835\n",
      "One iteration: 1691951291.4216478\n",
      "One iteration: 1691951291.424261\n",
      "One iteration: 1691951291.4274838\n",
      "One iteration: 1691951291.430154\n",
      "One iteration: 1691951291.4330492\n",
      "One iteration: 1691951291.435702\n",
      "One iteration: 1691951291.438436\n",
      "One iteration: 1691951291.4410381\n",
      "One iteration: 1691951291.444572\n",
      "One iteration: 1691951291.4471612\n",
      "One iteration: 1691951291.449898\n",
      "One iteration: 1691951291.4525928\n",
      "One iteration: 1691951291.455374\n",
      "One iteration: 1691951291.4581099\n",
      "One iteration: 1691951291.4625769\n",
      "One iteration: 1691951291.466184\n",
      "One iteration: 1691951291.4689732\n",
      "One iteration: 1691951291.47196\n",
      "One iteration: 1691951291.474668\n",
      "One iteration: 1691951291.477704\n",
      "One iteration: 1691951291.480246\n",
      "One iteration: 1691951291.483256\n",
      "One iteration: 1691951291.486093\n",
      "One iteration: 1691951291.48949\n",
      "One iteration: 1691951291.493175\n",
      "One iteration: 1691951291.496675\n",
      "One iteration: 1691951291.499597\n",
      "One iteration: 1691951291.502367\n",
      "One iteration: 1691951291.5054379\n",
      "One iteration: 1691951291.50834\n",
      "One iteration: 1691951291.511467\n",
      "One iteration: 1691951291.5141802\n",
      "One iteration: 1691951291.516906\n",
      "One iteration: 1691951291.519715\n",
      "One iteration: 1691951291.5226429\n",
      "One iteration: 1691951291.52542\n",
      "One iteration: 1691951291.5287302\n",
      "One iteration: 1691951291.53141\n",
      "One iteration: 1691951291.5343308\n",
      "One iteration: 1691951291.537055\n",
      "One iteration: 1691951291.539961\n",
      "One iteration: 1691951291.5429199\n",
      "One iteration: 1691951291.546064\n",
      "One iteration: 1691951291.548955\n",
      "One iteration: 1691951291.551733\n",
      "One iteration: 1691951291.5544388\n",
      "One iteration: 1691951291.5573251\n",
      "One iteration: 1691951291.560469\n",
      "One iteration: 1691951291.56334\n",
      "One iteration: 1691951291.566127\n",
      "One iteration: 1691951291.568814\n",
      "One iteration: 1691951291.5716789\n",
      "One iteration: 1691951291.574419\n",
      "One iteration: 1691951291.5774322\n",
      "One iteration: 1691951291.580134\n",
      "One iteration: 1691951291.5828748\n",
      "One iteration: 1691951291.585539\n",
      "One iteration: 1691951291.588475\n",
      "One iteration: 1691951291.591255\n",
      "One iteration: 1691951291.594184\n",
      "One iteration: 1691951291.597367\n",
      "One iteration: 1691951291.600227\n",
      "One iteration: 1691951291.603242\n",
      "One iteration: 1691951291.606505\n",
      "One iteration: 1691951291.609429\n",
      "One iteration: 1691951291.612781\n",
      "One iteration: 1691951291.6156642\n",
      "One iteration: 1691951291.6184268\n",
      "One iteration: 1691951291.621087\n",
      "One iteration: 1691951291.62397\n",
      "One iteration: 1691951291.6269498\n",
      "One iteration: 1691951291.630035\n",
      "One iteration: 1691951291.632936\n",
      "One iteration: 1691951291.635676\n",
      "One iteration: 1691951291.638578\n",
      "One iteration: 1691951291.641293\n",
      "One iteration: 1691951291.6444101\n",
      "One iteration: 1691951291.646986\n",
      "One iteration: 1691951291.649812\n",
      "One iteration: 1691951291.6524858\n",
      "One iteration: 1691951291.655339\n",
      "One iteration: 1691951291.658145\n",
      "One iteration: 1691951291.6612859\n",
      "One iteration: 1691951291.6644511\n",
      "One iteration: 1691951291.667928\n",
      "One iteration: 1691951291.670633\n",
      "One iteration: 1691951291.673482\n",
      "One iteration: 1691951291.676469\n",
      "One iteration: 1691951291.679563\n",
      "One iteration: 1691951291.6824179\n",
      "One iteration: 1691951291.685101\n",
      "One iteration: 1691951291.6882088\n",
      "One iteration: 1691951291.691339\n",
      "One iteration: 1691951291.697061\n",
      "One iteration: 1691951291.7012749\n",
      "One iteration: 1691951291.705854\n",
      "One iteration: 1691951291.7091799\n",
      "One iteration: 1691951291.7120771\n",
      "One iteration: 1691951291.715294\n",
      "One iteration: 1691951291.7183108\n",
      "One iteration: 1691951291.721502\n",
      "One iteration: 1691951291.724353\n",
      "One iteration: 1691951291.7275\n",
      "One iteration: 1691951291.730709\n",
      "One iteration: 1691951291.73382\n",
      "One iteration: 1691951291.736754\n",
      "One iteration: 1691951291.739698\n",
      "One iteration: 1691951291.742899\n",
      "One iteration: 1691951291.7461371\n",
      "One iteration: 1691951291.748854\n",
      "One iteration: 1691951291.7515929\n",
      "One iteration: 1691951291.7543218\n",
      "One iteration: 1691951291.757254\n",
      "One iteration: 1691951291.7605262\n",
      "One iteration: 1691951291.763212\n",
      "One iteration: 1691951291.7658482\n",
      "One iteration: 1691951291.768571\n",
      "One iteration: 1691951291.771609\n",
      "One iteration: 1691951291.774545\n",
      "One iteration: 1691951291.7777479\n",
      "One iteration: 1691951291.780385\n",
      "One iteration: 1691951291.7836359\n",
      "One iteration: 1691951291.7864761\n",
      "One iteration: 1691951291.789576\n",
      "One iteration: 1691951291.792927\n",
      "One iteration: 1691951291.7956648\n",
      "One iteration: 1691951291.798264\n",
      "One iteration: 1691951291.801217\n",
      "One iteration: 1691951291.803935\n",
      "One iteration: 1691951291.8070788\n",
      "One iteration: 1691951291.810508\n",
      "One iteration: 1691951291.813354\n",
      "One iteration: 1691951291.8164842\n",
      "One iteration: 1691951291.819089\n",
      "One iteration: 1691951291.82198\n",
      "One iteration: 1691951291.8251011\n",
      "One iteration: 1691951291.828321\n",
      "One iteration: 1691951291.8309171\n",
      "One iteration: 1691951291.833771\n",
      "One iteration: 1691951291.836583\n",
      "One iteration: 1691951291.8394601\n",
      "One iteration: 1691951291.842253\n",
      "One iteration: 1691951291.8453472\n",
      "One iteration: 1691951291.8479319\n",
      "One iteration: 1691951291.850849\n",
      "One iteration: 1691951291.8534951\n",
      "One iteration: 1691951291.856582\n",
      "One iteration: 1691951291.859782\n",
      "One iteration: 1691951291.863059\n",
      "One iteration: 1691951291.867685\n",
      "One iteration: 1691951291.871933\n",
      "One iteration: 1691951291.8750641\n",
      "One iteration: 1691951291.879501\n",
      "One iteration: 1691951291.883181\n",
      "One iteration: 1691951291.886254\n",
      "One iteration: 1691951291.8899899\n",
      "One iteration: 1691951291.894146\n",
      "One iteration: 1691951291.897321\n",
      "One iteration: 1691951291.901037\n",
      "One iteration: 1691951291.904962\n",
      "One iteration: 1691951291.908454\n",
      "One iteration: 1691951291.9123468\n",
      "One iteration: 1691951291.916158\n",
      "One iteration: 1691951291.919103\n",
      "One iteration: 1691951291.922001\n",
      "One iteration: 1691951291.924824\n",
      "One iteration: 1691951291.927648\n",
      "One iteration: 1691951291.930462\n",
      "One iteration: 1691951291.9335701\n",
      "One iteration: 1691951291.936686\n",
      "One iteration: 1691951291.940129\n",
      "One iteration: 1691951291.944308\n",
      "One iteration: 1691951291.9475\n",
      "One iteration: 1691951291.950906\n",
      "One iteration: 1691951291.954282\n",
      "One iteration: 1691951291.9580128\n",
      "One iteration: 1691951291.962142\n",
      "One iteration: 1691951291.965557\n",
      "One iteration: 1691951291.969204\n",
      "One iteration: 1691951291.972874\n",
      "One iteration: 1691951291.976118\n",
      "One iteration: 1691951291.9800699\n",
      "One iteration: 1691951291.9839518\n",
      "One iteration: 1691951291.987954\n",
      "One iteration: 1691951291.991877\n",
      "One iteration: 1691951291.996074\n",
      "One iteration: 1691951291.9999738\n",
      "One iteration: 1691951292.003235\n",
      "One iteration: 1691951292.007017\n",
      "One iteration: 1691951292.0109508\n",
      "One iteration: 1691951292.015082\n",
      "One iteration: 1691951292.018661\n",
      "One iteration: 1691951292.022364\n",
      "One iteration: 1691951292.025708\n",
      "One iteration: 1691951292.029427\n",
      "One iteration: 1691951292.033346\n",
      "One iteration: 1691951292.036674\n",
      "One iteration: 1691951292.040704\n",
      "One iteration: 1691951292.044702\n",
      "One iteration: 1691951292.048665\n",
      "One iteration: 1691951292.0525062\n",
      "One iteration: 1691951292.056532\n",
      "One iteration: 1691951292.061013\n",
      "One iteration: 1691951292.0643358\n",
      "One iteration: 1691951292.06787\n",
      "One iteration: 1691951292.0729642\n",
      "One iteration: 1691951292.076725\n",
      "One iteration: 1691951292.0805812\n",
      "One iteration: 1691951292.0847812\n",
      "One iteration: 1691951292.08874\n",
      "One iteration: 1691951292.092124\n",
      "One iteration: 1691951292.0958679\n",
      "One iteration: 1691951292.0998552\n",
      "One iteration: 1691951292.103358\n",
      "One iteration: 1691951292.107093\n",
      "One iteration: 1691951292.110626\n",
      "One iteration: 1691951292.1148632\n",
      "One iteration: 1691951292.119295\n",
      "One iteration: 1691951292.123698\n",
      "One iteration: 1691951292.1272588\n",
      "One iteration: 1691951292.131056\n",
      "One iteration: 1691951292.1347651\n",
      "One iteration: 1691951292.138586\n",
      "One iteration: 1691951292.14231\n",
      "One iteration: 1691951292.14606\n",
      "One iteration: 1691951292.149578\n",
      "One iteration: 1691951292.153922\n",
      "One iteration: 1691951292.157784\n",
      "One iteration: 1691951292.161149\n",
      "One iteration: 1691951292.164705\n",
      "One iteration: 1691951292.168233\n",
      "One iteration: 1691951292.171594\n",
      "One iteration: 1691951292.174954\n",
      "One iteration: 1691951292.178659\n",
      "One iteration: 1691951292.182264\n",
      "One iteration: 1691951292.1855948\n",
      "One iteration: 1691951292.189095\n",
      "One iteration: 1691951292.192417\n",
      "One iteration: 1691951292.195851\n",
      "One iteration: 1691951292.1994371\n",
      "One iteration: 1691951292.20266\n",
      "One iteration: 1691951292.206144\n",
      "One iteration: 1691951292.209368\n",
      "One iteration: 1691951292.213102\n",
      "One iteration: 1691951292.216931\n",
      "One iteration: 1691951292.2208252\n",
      "One iteration: 1691951292.224501\n",
      "One iteration: 1691951292.2280169\n",
      "One iteration: 1691951292.231764\n",
      "One iteration: 1691951292.235209\n",
      "One iteration: 1691951292.2387168\n",
      "One iteration: 1691951292.241955\n",
      "One iteration: 1691951292.24557\n",
      "One iteration: 1691951292.249355\n",
      "One iteration: 1691951292.252873\n",
      "One iteration: 1691951292.256601\n",
      "One iteration: 1691951292.259837\n",
      "One iteration: 1691951292.263413\n",
      "One iteration: 1691951292.2668328\n",
      "One iteration: 1691951292.270025\n",
      "One iteration: 1691951292.273778\n",
      "One iteration: 1691951292.2783842\n",
      "One iteration: 1691951292.2819772\n",
      "One iteration: 1691951292.285384\n",
      "One iteration: 1691951292.289428\n",
      "One iteration: 1691951292.293081\n",
      "One iteration: 1691951292.296554\n",
      "One iteration: 1691951292.2999659\n",
      "One iteration: 1691951292.3033721\n",
      "One iteration: 1691951292.306695\n",
      "One iteration: 1691951292.309804\n",
      "One iteration: 1691951292.3134148\n",
      "One iteration: 1691951292.316861\n",
      "One iteration: 1691951292.319694\n",
      "One iteration: 1691951292.322869\n",
      "One iteration: 1691951292.325737\n",
      "One iteration: 1691951292.328792\n",
      "One iteration: 1691951292.331435\n",
      "One iteration: 1691951292.334344\n",
      "One iteration: 1691951292.337183\n",
      "One iteration: 1691951292.340062\n",
      "One iteration: 1691951292.34324\n",
      "One iteration: 1691951292.346368\n",
      "One iteration: 1691951292.349115\n",
      "One iteration: 1691951292.352003\n",
      "One iteration: 1691951292.354996\n",
      "One iteration: 1691951292.35812\n",
      "One iteration: 1691951292.361553\n",
      "One iteration: 1691951292.364248\n",
      "One iteration: 1691951292.3673768\n",
      "One iteration: 1691951292.370204\n",
      "One iteration: 1691951292.373193\n",
      "One iteration: 1691951292.376558\n",
      "One iteration: 1691951292.379668\n",
      "One iteration: 1691951292.382549\n",
      "One iteration: 1691951292.3854358\n",
      "One iteration: 1691951292.388466\n",
      "One iteration: 1691951292.39124\n",
      "One iteration: 1691951292.394458\n",
      "One iteration: 1691951292.397177\n",
      "One iteration: 1691951292.400127\n",
      "One iteration: 1691951292.4028459\n",
      "One iteration: 1691951292.4058828\n",
      "One iteration: 1691951292.4088802\n",
      "One iteration: 1691951292.4120839\n",
      "One iteration: 1691951292.414724\n",
      "One iteration: 1691951292.417633\n",
      "One iteration: 1691951292.4203641\n",
      "One iteration: 1691951292.4233751\n",
      "One iteration: 1691951292.426532\n",
      "One iteration: 1691951292.429578\n",
      "One iteration: 1691951292.43245\n",
      "One iteration: 1691951292.435262\n",
      "One iteration: 1691951292.43824\n",
      "One iteration: 1691951292.441042\n",
      "One iteration: 1691951292.444167\n",
      "One iteration: 1691951292.446849\n",
      "One iteration: 1691951292.4497018\n",
      "One iteration: 1691951292.45245\n",
      "One iteration: 1691951292.455342\n",
      "One iteration: 1691951292.458286\n",
      "One iteration: 1691951292.4615319\n",
      "One iteration: 1691951292.46465\n",
      "One iteration: 1691951292.467558\n",
      "One iteration: 1691951292.4702442\n",
      "One iteration: 1691951292.472985\n",
      "One iteration: 1691951292.4771042\n",
      "One iteration: 1691951292.4811819\n",
      "One iteration: 1691951292.4842\n",
      "One iteration: 1691951292.48694\n",
      "One iteration: 1691951292.489911\n",
      "One iteration: 1691951292.493022\n",
      "One iteration: 1691951292.49615\n",
      "One iteration: 1691951292.4990358\n",
      "One iteration: 1691951292.502014\n",
      "One iteration: 1691951292.504995\n",
      "One iteration: 1691951292.508285\n",
      "One iteration: 1691951292.51168\n",
      "One iteration: 1691951292.515241\n",
      "One iteration: 1691951292.519263\n",
      "One iteration: 1691951292.5228028\n",
      "One iteration: 1691951292.5262332\n",
      "One iteration: 1691951292.529512\n",
      "One iteration: 1691951292.533086\n",
      "One iteration: 1691951292.5359511\n",
      "One iteration: 1691951292.539128\n",
      "One iteration: 1691951292.542696\n",
      "One iteration: 1691951292.546295\n",
      "One iteration: 1691951292.549493\n",
      "One iteration: 1691951292.552431\n",
      "One iteration: 1691951292.555373\n",
      "One iteration: 1691951292.558434\n",
      "One iteration: 1691951292.561682\n",
      "One iteration: 1691951292.56433\n",
      "One iteration: 1691951292.567266\n",
      "One iteration: 1691951292.570018\n",
      "One iteration: 1691951292.572917\n",
      "One iteration: 1691951292.575883\n",
      "One iteration: 1691951292.579085\n",
      "One iteration: 1691951292.581867\n",
      "One iteration: 1691951292.584672\n",
      "One iteration: 1691951292.5876942\n",
      "One iteration: 1691951292.590435\n",
      "One iteration: 1691951292.5935419\n",
      "One iteration: 1691951292.596638\n",
      "One iteration: 1691951292.599454\n",
      "One iteration: 1691951292.602307\n",
      "One iteration: 1691951292.605498\n",
      "One iteration: 1691951292.608636\n",
      "One iteration: 1691951292.6119711\n",
      "One iteration: 1691951292.614689\n",
      "One iteration: 1691951292.617898\n",
      "One iteration: 1691951292.620658\n",
      "One iteration: 1691951292.623661\n",
      "One iteration: 1691951292.626792\n",
      "One iteration: 1691951292.629735\n",
      "One iteration: 1691951292.632525\n",
      "One iteration: 1691951292.635347\n",
      "One iteration: 1691951292.638389\n",
      "One iteration: 1691951292.6411269\n",
      "One iteration: 1691951292.644237\n",
      "One iteration: 1691951292.6468768\n",
      "One iteration: 1691951292.649707\n",
      "One iteration: 1691951292.652329\n",
      "One iteration: 1691951292.655283\n",
      "One iteration: 1691951292.658371\n",
      "One iteration: 1691951292.661636\n",
      "One iteration: 1691951292.664432\n",
      "One iteration: 1691951292.667488\n",
      "One iteration: 1691951292.670215\n",
      "One iteration: 1691951292.673185\n",
      "One iteration: 1691951292.676209\n",
      "One iteration: 1691951292.6800928\n",
      "One iteration: 1691951292.6837618\n",
      "One iteration: 1691951292.686552\n",
      "One iteration: 1691951292.6896372\n",
      "One iteration: 1691951292.692804\n",
      "One iteration: 1691951292.695989\n",
      "One iteration: 1691951292.6987278\n",
      "One iteration: 1691951292.7017689\n",
      "One iteration: 1691951292.704583\n",
      "One iteration: 1691951292.707594\n",
      "One iteration: 1691951292.7116318\n",
      "One iteration: 1691951292.714923\n",
      "One iteration: 1691951292.718175\n",
      "One iteration: 1691951292.721187\n",
      "One iteration: 1691951292.7240791\n",
      "One iteration: 1691951292.7270532\n",
      "One iteration: 1691951292.730444\n",
      "One iteration: 1691951292.733477\n",
      "One iteration: 1691951292.736435\n",
      "One iteration: 1691951292.73964\n",
      "One iteration: 1691951292.7427242\n",
      "One iteration: 1691951292.746044\n",
      "One iteration: 1691951292.7489269\n",
      "One iteration: 1691951292.7519119\n",
      "One iteration: 1691951292.754854\n",
      "One iteration: 1691951292.757878\n",
      "One iteration: 1691951292.7611651\n",
      "One iteration: 1691951292.763965\n",
      "One iteration: 1691951292.766787\n",
      "One iteration: 1691951292.769578\n",
      "One iteration: 1691951292.7725341\n",
      "One iteration: 1691951292.7755651\n",
      "One iteration: 1691951292.7788649\n",
      "One iteration: 1691951292.781673\n",
      "One iteration: 1691951292.784532\n",
      "One iteration: 1691951292.787394\n",
      "One iteration: 1691951292.79036\n",
      "One iteration: 1691951292.793584\n",
      "One iteration: 1691951292.796597\n",
      "One iteration: 1691951292.799477\n",
      "One iteration: 1691951292.802298\n",
      "One iteration: 1691951292.80537\n",
      "One iteration: 1691951292.808646\n",
      "One iteration: 1691951292.811847\n",
      "One iteration: 1691951292.814544\n",
      "One iteration: 1691951292.8178189\n",
      "One iteration: 1691951292.8205261\n",
      "One iteration: 1691951292.8234\n",
      "One iteration: 1691951292.826626\n",
      "One iteration: 1691951292.829586\n",
      "One iteration: 1691951292.8323488\n",
      "One iteration: 1691951292.835277\n",
      "One iteration: 1691951292.838094\n",
      "One iteration: 1691951292.841048\n",
      "One iteration: 1691951292.844089\n",
      "One iteration: 1691951292.8468502\n",
      "One iteration: 1691951292.849607\n",
      "One iteration: 1691951292.8523362\n",
      "One iteration: 1691951292.8554602\n",
      "One iteration: 1691951292.858109\n",
      "One iteration: 1691951292.8614862\n",
      "One iteration: 1691951292.864164\n",
      "One iteration: 1691951292.8671\n",
      "One iteration: 1691951292.869983\n",
      "One iteration: 1691951292.873106\n",
      "One iteration: 1691951292.876267\n",
      "One iteration: 1691951292.879771\n",
      "One iteration: 1691951292.883435\n",
      "One iteration: 1691951292.886948\n",
      "One iteration: 1691951292.8901818\n",
      "One iteration: 1691951292.893376\n",
      "One iteration: 1691951292.896733\n",
      "One iteration: 1691951292.899729\n",
      "One iteration: 1691951292.9032562\n",
      "One iteration: 1691951292.907027\n",
      "One iteration: 1691951292.910698\n",
      "One iteration: 1691951292.914643\n",
      "One iteration: 1691951292.9185169\n",
      "One iteration: 1691951292.9219258\n",
      "One iteration: 1691951292.925838\n",
      "One iteration: 1691951292.9289742\n",
      "One iteration: 1691951292.933152\n",
      "One iteration: 1691951292.9365199\n",
      "One iteration: 1691951292.939896\n",
      "One iteration: 1691951292.943455\n",
      "One iteration: 1691951292.9465191\n",
      "One iteration: 1691951292.949603\n",
      "One iteration: 1691951292.952856\n",
      "One iteration: 1691951292.956104\n",
      "One iteration: 1691951292.96063\n",
      "One iteration: 1691951292.964177\n",
      "One iteration: 1691951292.967721\n",
      "One iteration: 1691951292.97104\n",
      "One iteration: 1691951292.9746182\n",
      "One iteration: 1691951292.978107\n",
      "One iteration: 1691951292.981375\n",
      "One iteration: 1691951292.985538\n",
      "One iteration: 1691951292.9897668\n",
      "One iteration: 1691951292.993492\n",
      "One iteration: 1691951292.997633\n",
      "One iteration: 1691951293.000958\n",
      "One iteration: 1691951293.004328\n",
      "One iteration: 1691951293.008061\n",
      "One iteration: 1691951293.011501\n",
      "One iteration: 1691951293.015031\n",
      "One iteration: 1691951293.0183759\n",
      "One iteration: 1691951293.021934\n",
      "One iteration: 1691951293.025286\n",
      "One iteration: 1691951293.0289311\n",
      "One iteration: 1691951293.031868\n",
      "One iteration: 1691951293.035051\n",
      "One iteration: 1691951293.038495\n",
      "One iteration: 1691951293.041889\n",
      "One iteration: 1691951293.045055\n",
      "One iteration: 1691951293.0482922\n",
      "One iteration: 1691951293.051266\n",
      "One iteration: 1691951293.0544288\n",
      "One iteration: 1691951293.0573928\n",
      "One iteration: 1691951293.0608718\n",
      "One iteration: 1691951293.064142\n",
      "One iteration: 1691951293.067233\n",
      "One iteration: 1691951293.070864\n",
      "One iteration: 1691951293.073878\n",
      "One iteration: 1691951293.077683\n",
      "One iteration: 1691951293.080272\n",
      "One iteration: 1691951293.083104\n",
      "One iteration: 1691951293.0869892\n",
      "One iteration: 1691951293.090581\n",
      "One iteration: 1691951293.094094\n",
      "One iteration: 1691951293.096647\n",
      "One iteration: 1691951293.099334\n",
      "One iteration: 1691951293.1026409\n",
      "One iteration: 1691951293.105809\n",
      "One iteration: 1691951293.109853\n",
      "One iteration: 1691951293.112638\n",
      "One iteration: 1691951293.115595\n",
      "One iteration: 1691951293.118793\n",
      "One iteration: 1691951293.12208\n",
      "One iteration: 1691951293.1261399\n",
      "One iteration: 1691951293.1303859\n",
      "One iteration: 1691951293.133434\n",
      "One iteration: 1691951293.136396\n",
      "One iteration: 1691951293.1395059\n",
      "One iteration: 1691951293.142847\n",
      "One iteration: 1691951293.145591\n",
      "One iteration: 1691951293.148234\n",
      "One iteration: 1691951293.151225\n",
      "One iteration: 1691951293.154211\n",
      "One iteration: 1691951293.157325\n",
      "One iteration: 1691951293.160755\n",
      "One iteration: 1691951293.163497\n",
      "One iteration: 1691951293.166354\n",
      "One iteration: 1691951293.169187\n",
      "One iteration: 1691951293.172215\n",
      "One iteration: 1691951293.175591\n",
      "One iteration: 1691951293.178344\n",
      "One iteration: 1691951293.181014\n",
      "One iteration: 1691951293.1840808\n",
      "One iteration: 1691951293.186862\n",
      "One iteration: 1691951293.189906\n",
      "One iteration: 1691951293.193164\n",
      "One iteration: 1691951293.195868\n",
      "One iteration: 1691951293.198543\n",
      "One iteration: 1691951293.201387\n",
      "One iteration: 1691951293.2042692\n",
      "One iteration: 1691951293.207057\n",
      "One iteration: 1691951293.210439\n",
      "One iteration: 1691951293.213123\n",
      "One iteration: 1691951293.215798\n",
      "One iteration: 1691951293.2186282\n",
      "One iteration: 1691951293.2214952\n",
      "One iteration: 1691951293.2246022\n",
      "One iteration: 1691951293.2277582\n",
      "One iteration: 1691951293.23044\n",
      "One iteration: 1691951293.233218\n",
      "One iteration: 1691951293.23614\n",
      "One iteration: 1691951293.23913\n",
      "One iteration: 1691951293.242341\n",
      "One iteration: 1691951293.245568\n",
      "One iteration: 1691951293.248183\n",
      "One iteration: 1691951293.25113\n",
      "One iteration: 1691951293.2539158\n",
      "One iteration: 1691951293.256978\n",
      "One iteration: 1691951293.260349\n",
      "One iteration: 1691951293.2629602\n",
      "One iteration: 1691951293.265686\n",
      "One iteration: 1691951293.2684531\n",
      "One iteration: 1691951293.271242\n",
      "One iteration: 1691951293.2742028\n",
      "One iteration: 1691951293.277372\n",
      "One iteration: 1691951293.280042\n",
      "One iteration: 1691951293.28293\n",
      "One iteration: 1691951293.285588\n",
      "One iteration: 1691951293.289259\n",
      "One iteration: 1691951293.292809\n",
      "One iteration: 1691951293.2962072\n",
      "One iteration: 1691951293.2993112\n",
      "One iteration: 1691951293.302251\n",
      "One iteration: 1691951293.305378\n",
      "One iteration: 1691951293.3081799\n",
      "One iteration: 1691951293.31158\n",
      "One iteration: 1691951293.3144\n",
      "One iteration: 1691951293.317812\n",
      "One iteration: 1691951293.320928\n",
      "One iteration: 1691951293.324319\n",
      "One iteration: 1691951293.328019\n",
      "One iteration: 1691951293.331014\n",
      "One iteration: 1691951293.334066\n",
      "One iteration: 1691951293.3370082\n",
      "One iteration: 1691951293.3400068\n",
      "One iteration: 1691951293.343112\n",
      "One iteration: 1691951293.346268\n",
      "One iteration: 1691951293.349211\n",
      "One iteration: 1691951293.352066\n",
      "One iteration: 1691951293.3548942\n",
      "One iteration: 1691951293.357831\n",
      "One iteration: 1691951293.361025\n",
      "One iteration: 1691951293.363719\n",
      "One iteration: 1691951293.366524\n",
      "One iteration: 1691951293.369279\n",
      "One iteration: 1691951293.372106\n",
      "One iteration: 1691951293.3749251\n",
      "One iteration: 1691951293.378295\n",
      "One iteration: 1691951293.380991\n",
      "One iteration: 1691951293.383893\n",
      "One iteration: 1691951293.386841\n",
      "One iteration: 1691951293.38989\n",
      "One iteration: 1691951293.393028\n",
      "One iteration: 1691951293.39644\n",
      "One iteration: 1691951293.3995469\n",
      "One iteration: 1691951293.402325\n",
      "One iteration: 1691951293.405165\n",
      "One iteration: 1691951293.407925\n",
      "One iteration: 1691951293.411419\n",
      "One iteration: 1691951293.414167\n",
      "One iteration: 1691951293.417311\n",
      "One iteration: 1691951293.4200552\n",
      "One iteration: 1691951293.423027\n",
      "One iteration: 1691951293.425941\n",
      "One iteration: 1691951293.42913\n",
      "One iteration: 1691951293.43201\n",
      "One iteration: 1691951293.434834\n",
      "One iteration: 1691951293.43782\n",
      "One iteration: 1691951293.4405482\n",
      "One iteration: 1691951293.443935\n",
      "One iteration: 1691951293.446655\n",
      "One iteration: 1691951293.449535\n",
      "One iteration: 1691951293.452318\n",
      "One iteration: 1691951293.455374\n",
      "One iteration: 1691951293.457933\n",
      "One iteration: 1691951293.461181\n",
      "One iteration: 1691951293.464504\n",
      "One iteration: 1691951293.467735\n",
      "One iteration: 1691951293.470647\n",
      "One iteration: 1691951293.473601\n",
      "One iteration: 1691951293.476778\n",
      "One iteration: 1691951293.479708\n",
      "One iteration: 1691951293.4826581\n",
      "One iteration: 1691951293.485456\n",
      "One iteration: 1691951293.4885278\n",
      "One iteration: 1691951293.4922152\n",
      "One iteration: 1691951293.496165\n",
      "One iteration: 1691951293.499073\n",
      "One iteration: 1691951293.5022058\n",
      "One iteration: 1691951293.5051641\n",
      "One iteration: 1691951293.508012\n",
      "One iteration: 1691951293.511483\n",
      "One iteration: 1691951293.5142221\n",
      "One iteration: 1691951293.5172648\n",
      "One iteration: 1691951293.520133\n",
      "One iteration: 1691951293.523612\n",
      "One iteration: 1691951293.527648\n",
      "One iteration: 1691951293.531945\n",
      "One iteration: 1691951293.535367\n",
      "One iteration: 1691951293.538846\n",
      "One iteration: 1691951293.5422971\n",
      "One iteration: 1691951293.546273\n",
      "One iteration: 1691951293.549331\n",
      "One iteration: 1691951293.552203\n",
      "One iteration: 1691951293.555438\n",
      "One iteration: 1691951293.558753\n",
      "One iteration: 1691951293.5619998\n",
      "One iteration: 1691951293.5646322\n",
      "One iteration: 1691951293.567966\n",
      "One iteration: 1691951293.570941\n",
      "One iteration: 1691951293.57388\n",
      "One iteration: 1691951293.577203\n",
      "One iteration: 1691951293.579873\n",
      "One iteration: 1691951293.5826309\n",
      "One iteration: 1691951293.585511\n",
      "One iteration: 1691951293.588447\n",
      "One iteration: 1691951293.591594\n",
      "One iteration: 1691951293.5950878\n",
      "One iteration: 1691951293.597717\n",
      "One iteration: 1691951293.6007512\n",
      "One iteration: 1691951293.603699\n",
      "One iteration: 1691951293.6069179\n",
      "One iteration: 1691951293.611609\n",
      "One iteration: 1691951293.615009\n",
      "One iteration: 1691951293.618819\n",
      "One iteration: 1691951293.621825\n",
      "One iteration: 1691951293.624832\n",
      "One iteration: 1691951293.628287\n",
      "One iteration: 1691951293.631017\n",
      "One iteration: 1691951293.634065\n",
      "One iteration: 1691951293.636907\n",
      "One iteration: 1691951293.640047\n",
      "One iteration: 1691951293.64381\n",
      "One iteration: 1691951293.646477\n",
      "One iteration: 1691951293.649454\n",
      "One iteration: 1691951293.6523058\n",
      "One iteration: 1691951293.655553\n",
      "One iteration: 1691951293.658628\n",
      "One iteration: 1691951293.661988\n",
      "One iteration: 1691951293.66466\n",
      "One iteration: 1691951293.667569\n",
      "One iteration: 1691951293.670398\n",
      "One iteration: 1691951293.673418\n",
      "One iteration: 1691951293.6767201\n",
      "One iteration: 1691951293.679359\n",
      "One iteration: 1691951293.6821141\n",
      "One iteration: 1691951293.684905\n",
      "One iteration: 1691951293.687666\n",
      "One iteration: 1691951293.690418\n",
      "One iteration: 1691951293.6935968\n",
      "One iteration: 1691951293.697698\n",
      "One iteration: 1691951293.70095\n",
      "One iteration: 1691951293.7038\n",
      "One iteration: 1691951293.706976\n",
      "One iteration: 1691951293.710463\n",
      "One iteration: 1691951293.7134888\n",
      "One iteration: 1691951293.7165\n",
      "One iteration: 1691951293.719436\n",
      "One iteration: 1691951293.7225668\n",
      "One iteration: 1691951293.725617\n",
      "One iteration: 1691951293.729007\n",
      "One iteration: 1691951293.732196\n",
      "One iteration: 1691951293.735647\n",
      "One iteration: 1691951293.7387211\n",
      "One iteration: 1691951293.7415922\n",
      "One iteration: 1691951293.744997\n",
      "One iteration: 1691951293.74835\n",
      "One iteration: 1691951293.7513201\n",
      "One iteration: 1691951293.754053\n",
      "One iteration: 1691951293.756973\n",
      "One iteration: 1691951293.760256\n",
      "One iteration: 1691951293.763319\n",
      "One iteration: 1691951293.766146\n",
      "One iteration: 1691951293.7688231\n",
      "One iteration: 1691951293.771831\n",
      "One iteration: 1691951293.7745368\n",
      "One iteration: 1691951293.7776792\n",
      "One iteration: 1691951293.780514\n",
      "One iteration: 1691951293.783722\n",
      "One iteration: 1691951293.786596\n",
      "One iteration: 1691951293.7897458\n",
      "One iteration: 1691951293.792979\n",
      "One iteration: 1691951293.796213\n",
      "One iteration: 1691951293.799604\n",
      "One iteration: 1691951293.802439\n",
      "One iteration: 1691951293.805394\n",
      "One iteration: 1691951293.808129\n",
      "One iteration: 1691951293.81136\n",
      "One iteration: 1691951293.814042\n",
      "One iteration: 1691951293.8171651\n",
      "One iteration: 1691951293.8200839\n",
      "One iteration: 1691951293.823173\n",
      "One iteration: 1691951293.826168\n",
      "One iteration: 1691951293.82933\n",
      "One iteration: 1691951293.832268\n",
      "One iteration: 1691951293.835225\n",
      "One iteration: 1691951293.838148\n",
      "One iteration: 1691951293.840933\n",
      "One iteration: 1691951293.844106\n",
      "One iteration: 1691951293.8468149\n",
      "One iteration: 1691951293.8497689\n",
      "One iteration: 1691951293.852552\n",
      "One iteration: 1691951293.8556218\n",
      "One iteration: 1691951293.85846\n",
      "One iteration: 1691951293.861829\n",
      "One iteration: 1691951293.8645241\n",
      "One iteration: 1691951293.8677042\n",
      "One iteration: 1691951293.870453\n",
      "One iteration: 1691951293.873327\n",
      "One iteration: 1691951293.8764281\n",
      "One iteration: 1691951293.879471\n",
      "One iteration: 1691951293.882333\n",
      "One iteration: 1691951293.885102\n",
      "One iteration: 1691951293.888164\n",
      "One iteration: 1691951293.890901\n",
      "One iteration: 1691951293.893972\n",
      "One iteration: 1691951293.8966389\n",
      "One iteration: 1691951293.9000828\n",
      "One iteration: 1691951293.903448\n",
      "One iteration: 1691951293.906338\n",
      "One iteration: 1691951293.909552\n",
      "One iteration: 1691951293.913017\n",
      "One iteration: 1691951293.915936\n",
      "One iteration: 1691951293.918721\n",
      "One iteration: 1691951293.92174\n",
      "One iteration: 1691951293.9247239\n",
      "One iteration: 1691951293.928181\n",
      "One iteration: 1691951293.93139\n",
      "One iteration: 1691951293.934814\n",
      "One iteration: 1691951293.937975\n",
      "One iteration: 1691951293.9417632\n",
      "One iteration: 1691951293.945266\n",
      "One iteration: 1691951293.94926\n",
      "One iteration: 1691951293.952311\n",
      "One iteration: 1691951293.95555\n",
      "One iteration: 1691951293.958729\n",
      "One iteration: 1691951293.9623182\n",
      "One iteration: 1691951293.965357\n",
      "One iteration: 1691951293.968564\n",
      "One iteration: 1691951293.971646\n",
      "One iteration: 1691951293.9745688\n",
      "One iteration: 1691951293.977988\n",
      "One iteration: 1691951293.980744\n",
      "One iteration: 1691951293.983814\n",
      "One iteration: 1691951293.986785\n",
      "One iteration: 1691951293.990201\n",
      "One iteration: 1691951293.99404\n",
      "One iteration: 1691951293.996873\n",
      "One iteration: 1691951294.0002098\n",
      "One iteration: 1691951294.003048\n",
      "One iteration: 1691951294.00622\n",
      "One iteration: 1691951294.009664\n",
      "One iteration: 1691951294.012416\n",
      "One iteration: 1691951294.015216\n",
      "One iteration: 1691951294.0182269\n",
      "One iteration: 1691951294.0213811\n",
      "One iteration: 1691951294.0247629\n",
      "One iteration: 1691951294.028051\n",
      "One iteration: 1691951294.031038\n",
      "One iteration: 1691951294.034008\n",
      "One iteration: 1691951294.036903\n",
      "One iteration: 1691951294.039805\n",
      "One iteration: 1691951294.042915\n",
      "One iteration: 1691951294.04629\n",
      "One iteration: 1691951294.0490298\n",
      "One iteration: 1691951294.051834\n",
      "One iteration: 1691951294.0552828\n",
      "One iteration: 1691951294.058177\n",
      "One iteration: 1691951294.061379\n",
      "One iteration: 1691951294.0641\n",
      "One iteration: 1691951294.066902\n",
      "One iteration: 1691951294.069534\n",
      "One iteration: 1691951294.072941\n",
      "One iteration: 1691951294.0757608\n",
      "One iteration: 1691951294.079013\n",
      "One iteration: 1691951294.081829\n",
      "One iteration: 1691951294.084595\n",
      "One iteration: 1691951294.087327\n",
      "One iteration: 1691951294.090274\n",
      "One iteration: 1691951294.093549\n",
      "One iteration: 1691951294.096447\n",
      "One iteration: 1691951294.0992692\n",
      "One iteration: 1691951294.103064\n",
      "One iteration: 1691951294.106855\n",
      "One iteration: 1691951294.111008\n",
      "One iteration: 1691951294.114499\n",
      "One iteration: 1691951294.11877\n",
      "One iteration: 1691951294.121829\n",
      "One iteration: 1691951294.1253939\n",
      "One iteration: 1691951294.128555\n",
      "One iteration: 1691951294.131478\n",
      "One iteration: 1691951294.13461\n",
      "One iteration: 1691951294.137571\n",
      "One iteration: 1691951294.140981\n",
      "One iteration: 1691951294.144487\n",
      "One iteration: 1691951294.147739\n",
      "One iteration: 1691951294.151693\n",
      "One iteration: 1691951294.154742\n",
      "One iteration: 1691951294.157831\n",
      "One iteration: 1691951294.1610508\n",
      "One iteration: 1691951294.164854\n",
      "One iteration: 1691951294.1677902\n",
      "One iteration: 1691951294.1705081\n",
      "One iteration: 1691951294.1734889\n",
      "One iteration: 1691951294.176703\n",
      "One iteration: 1691951294.179717\n",
      "One iteration: 1691951294.1825469\n",
      "One iteration: 1691951294.185387\n",
      "One iteration: 1691951294.188704\n",
      "One iteration: 1691951294.191467\n",
      "One iteration: 1691951294.194636\n",
      "One iteration: 1691951294.1974568\n",
      "One iteration: 1691951294.20035\n",
      "One iteration: 1691951294.20315\n",
      "One iteration: 1691951294.206562\n",
      "One iteration: 1691951294.2096949\n",
      "One iteration: 1691951294.213299\n",
      "One iteration: 1691951294.2164\n",
      "One iteration: 1691951294.2191958\n",
      "One iteration: 1691951294.222101\n",
      "One iteration: 1691951294.225164\n",
      "One iteration: 1691951294.228419\n",
      "One iteration: 1691951294.231194\n",
      "One iteration: 1691951294.234159\n",
      "One iteration: 1691951294.237004\n",
      "One iteration: 1691951294.240292\n",
      "One iteration: 1691951294.243621\n",
      "One iteration: 1691951294.247266\n",
      "One iteration: 1691951294.250643\n",
      "One iteration: 1691951294.253381\n",
      "One iteration: 1691951294.256447\n",
      "One iteration: 1691951294.2595131\n",
      "One iteration: 1691951294.262957\n",
      "One iteration: 1691951294.2657752\n",
      "One iteration: 1691951294.268539\n",
      "One iteration: 1691951294.271277\n",
      "One iteration: 1691951294.274127\n",
      "One iteration: 1691951294.2773252\n",
      "One iteration: 1691951294.2805068\n",
      "One iteration: 1691951294.283916\n",
      "One iteration: 1691951294.286726\n",
      "One iteration: 1691951294.2898\n",
      "One iteration: 1691951294.2928472\n",
      "One iteration: 1691951294.296067\n",
      "One iteration: 1691951294.2989068\n",
      "One iteration: 1691951294.3018749\n",
      "One iteration: 1691951294.3055491\n",
      "One iteration: 1691951294.3089778\n",
      "One iteration: 1691951294.312303\n",
      "One iteration: 1691951294.3149\n",
      "One iteration: 1691951294.317787\n",
      "One iteration: 1691951294.320489\n",
      "One iteration: 1691951294.323769\n",
      "One iteration: 1691951294.327321\n",
      "One iteration: 1691951294.3301382\n",
      "One iteration: 1691951294.333077\n",
      "One iteration: 1691951294.336354\n",
      "One iteration: 1691951294.339743\n",
      "One iteration: 1691951294.343209\n",
      "One iteration: 1691951294.3465128\n",
      "One iteration: 1691951294.3495302\n",
      "One iteration: 1691951294.3523772\n",
      "One iteration: 1691951294.355448\n",
      "One iteration: 1691951294.358332\n",
      "One iteration: 1691951294.36165\n",
      "One iteration: 1691951294.36445\n",
      "One iteration: 1691951294.367402\n",
      "One iteration: 1691951294.370166\n",
      "One iteration: 1691951294.372978\n",
      "One iteration: 1691951294.376032\n",
      "One iteration: 1691951294.379223\n",
      "One iteration: 1691951294.3819451\n",
      "One iteration: 1691951294.384934\n",
      "One iteration: 1691951294.3877041\n",
      "One iteration: 1691951294.390647\n",
      "One iteration: 1691951294.393764\n",
      "One iteration: 1691951294.396702\n",
      "One iteration: 1691951294.3997052\n",
      "One iteration: 1691951294.402566\n",
      "One iteration: 1691951294.405779\n",
      "One iteration: 1691951294.408952\n",
      "One iteration: 1691951294.412313\n",
      "One iteration: 1691951294.4151611\n",
      "One iteration: 1691951294.4180522\n",
      "One iteration: 1691951294.420892\n",
      "One iteration: 1691951294.423986\n",
      "One iteration: 1691951294.427091\n",
      "One iteration: 1691951294.4300292\n",
      "One iteration: 1691951294.433026\n",
      "One iteration: 1691951294.435806\n",
      "One iteration: 1691951294.438783\n",
      "One iteration: 1691951294.441813\n",
      "One iteration: 1691951294.444962\n",
      "One iteration: 1691951294.4477062\n",
      "One iteration: 1691951294.450649\n",
      "One iteration: 1691951294.453516\n",
      "One iteration: 1691951294.456589\n",
      "One iteration: 1691951294.4596128\n",
      "One iteration: 1691951294.4630232\n",
      "One iteration: 1691951294.465913\n",
      "One iteration: 1691951294.468778\n",
      "One iteration: 1691951294.471713\n",
      "One iteration: 1691951294.474577\n",
      "One iteration: 1691951294.477582\n",
      "One iteration: 1691951294.480774\n",
      "One iteration: 1691951294.483692\n",
      "One iteration: 1691951294.486516\n",
      "One iteration: 1691951294.489774\n",
      "One iteration: 1691951294.4925282\n",
      "One iteration: 1691951294.495508\n",
      "One iteration: 1691951294.4986649\n",
      "One iteration: 1691951294.5015192\n",
      "One iteration: 1691951294.504261\n",
      "One iteration: 1691951294.508107\n",
      "One iteration: 1691951294.5121238\n",
      "One iteration: 1691951294.515425\n",
      "One iteration: 1691951294.5183182\n",
      "One iteration: 1691951294.521153\n",
      "One iteration: 1691951294.523903\n",
      "One iteration: 1691951294.526853\n",
      "One iteration: 1691951294.530161\n",
      "One iteration: 1691951294.5335622\n",
      "One iteration: 1691951294.536486\n",
      "One iteration: 1691951294.539554\n",
      "One iteration: 1691951294.5425391\n",
      "One iteration: 1691951294.5458062\n",
      "One iteration: 1691951294.548779\n",
      "One iteration: 1691951294.552181\n",
      "One iteration: 1691951294.555557\n",
      "One iteration: 1691951294.558387\n",
      "One iteration: 1691951294.561407\n",
      "One iteration: 1691951294.564655\n",
      "One iteration: 1691951294.567889\n",
      "One iteration: 1691951294.570494\n",
      "One iteration: 1691951294.573208\n",
      "One iteration: 1691951294.5758169\n",
      "One iteration: 1691951294.578351\n",
      "One iteration: 1691951294.581068\n",
      "One iteration: 1691951294.5838988\n",
      "One iteration: 1691951294.586695\n",
      "One iteration: 1691951294.589618\n",
      "One iteration: 1691951294.592675\n",
      "One iteration: 1691951294.595926\n",
      "One iteration: 1691951294.5988648\n",
      "One iteration: 1691951294.601682\n",
      "One iteration: 1691951294.6046479\n",
      "One iteration: 1691951294.60769\n",
      "One iteration: 1691951294.610574\n",
      "One iteration: 1691951294.614403\n",
      "One iteration: 1691951294.617772\n",
      "One iteration: 1691951294.620728\n",
      "One iteration: 1691951294.6243122\n",
      "One iteration: 1691951294.627359\n",
      "One iteration: 1691951294.63081\n",
      "One iteration: 1691951294.6338\n",
      "One iteration: 1691951294.636575\n",
      "One iteration: 1691951294.639495\n",
      "One iteration: 1691951294.642255\n",
      "One iteration: 1691951294.6452649\n",
      "One iteration: 1691951294.64836\n",
      "One iteration: 1691951294.651231\n",
      "One iteration: 1691951294.653942\n",
      "One iteration: 1691951294.6569788\n",
      "One iteration: 1691951294.6598191\n",
      "One iteration: 1691951294.662904\n",
      "One iteration: 1691951294.665772\n",
      "One iteration: 1691951294.6686049\n",
      "One iteration: 1691951294.671474\n",
      "One iteration: 1691951294.674259\n",
      "One iteration: 1691951294.677034\n",
      "One iteration: 1691951294.680133\n",
      "One iteration: 1691951294.682946\n",
      "One iteration: 1691951294.685557\n",
      "One iteration: 1691951294.6884742\n",
      "One iteration: 1691951294.6912389\n",
      "One iteration: 1691951294.694269\n",
      "One iteration: 1691951294.6974452\n",
      "One iteration: 1691951294.70047\n",
      "One iteration: 1691951294.703199\n",
      "One iteration: 1691951294.706163\n",
      "One iteration: 1691951294.709007\n",
      "One iteration: 1691951294.7154078\n",
      "One iteration: 1691951294.718786\n",
      "One iteration: 1691951294.721597\n",
      "One iteration: 1691951294.724446\n",
      "One iteration: 1691951294.7273989\n",
      "One iteration: 1691951294.730603\n",
      "One iteration: 1691951294.7336488\n",
      "One iteration: 1691951294.7364469\n",
      "One iteration: 1691951294.7394428\n",
      "One iteration: 1691951294.742254\n",
      "One iteration: 1691951294.746079\n",
      "One iteration: 1691951294.749533\n",
      "One iteration: 1691951294.75289\n",
      "One iteration: 1691951294.756772\n",
      "One iteration: 1691951294.759645\n",
      "One iteration: 1691951294.762499\n",
      "One iteration: 1691951294.765444\n",
      "One iteration: 1691951294.768726\n",
      "One iteration: 1691951294.771701\n",
      "One iteration: 1691951294.7743301\n",
      "One iteration: 1691951294.7770398\n",
      "One iteration: 1691951294.779683\n",
      "One iteration: 1691951294.7822652\n",
      "One iteration: 1691951294.785425\n",
      "One iteration: 1691951294.788768\n",
      "One iteration: 1691951294.791602\n",
      "One iteration: 1691951294.794865\n",
      "One iteration: 1691951294.798104\n",
      "One iteration: 1691951294.80125\n",
      "One iteration: 1691951294.804069\n",
      "One iteration: 1691951294.8069298\n",
      "One iteration: 1691951294.810045\n",
      "One iteration: 1691951294.8131268\n",
      "One iteration: 1691951294.815957\n",
      "One iteration: 1691951294.818707\n",
      "One iteration: 1691951294.821578\n",
      "One iteration: 1691951294.824707\n",
      "One iteration: 1691951294.827667\n",
      "One iteration: 1691951294.830837\n",
      "One iteration: 1691951294.83391\n",
      "One iteration: 1691951294.836721\n",
      "One iteration: 1691951294.8397791\n",
      "One iteration: 1691951294.842636\n",
      "One iteration: 1691951294.845741\n",
      "One iteration: 1691951294.8489192\n",
      "One iteration: 1691951294.851746\n",
      "One iteration: 1691951294.8544059\n",
      "One iteration: 1691951294.857266\n",
      "One iteration: 1691951294.860297\n",
      "One iteration: 1691951294.8635871\n",
      "One iteration: 1691951294.866708\n",
      "One iteration: 1691951294.869421\n",
      "One iteration: 1691951294.872307\n",
      "One iteration: 1691951294.8751192\n",
      "One iteration: 1691951294.878356\n",
      "One iteration: 1691951294.881344\n",
      "One iteration: 1691951294.884255\n",
      "One iteration: 1691951294.88694\n",
      "One iteration: 1691951294.889814\n",
      "One iteration: 1691951294.8923578\n",
      "One iteration: 1691951294.8958\n",
      "One iteration: 1691951294.898527\n",
      "One iteration: 1691951294.901362\n",
      "One iteration: 1691951294.9040241\n",
      "One iteration: 1691951294.906989\n",
      "One iteration: 1691951294.9096022\n",
      "One iteration: 1691951294.912761\n",
      "One iteration: 1691951294.91554\n",
      "One iteration: 1691951294.9189272\n",
      "One iteration: 1691951294.92237\n",
      "One iteration: 1691951294.925067\n",
      "One iteration: 1691951294.928245\n",
      "One iteration: 1691951294.931468\n",
      "One iteration: 1691951294.934649\n",
      "One iteration: 1691951294.937369\n",
      "One iteration: 1691951294.940119\n",
      "One iteration: 1691951294.942865\n",
      "One iteration: 1691951294.945915\n",
      "One iteration: 1691951294.948651\n",
      "One iteration: 1691951294.951552\n",
      "One iteration: 1691951294.954465\n",
      "One iteration: 1691951294.957717\n",
      "One iteration: 1691951294.96092\n",
      "One iteration: 1691951294.964264\n",
      "One iteration: 1691951294.967232\n",
      "One iteration: 1691951294.970009\n",
      "One iteration: 1691951294.973114\n",
      "One iteration: 1691951294.976321\n",
      "One iteration: 1691951294.979541\n",
      "One iteration: 1691951294.98235\n",
      "One iteration: 1691951294.985271\n",
      "One iteration: 1691951294.9880562\n",
      "One iteration: 1691951294.991037\n",
      "One iteration: 1691951294.994167\n",
      "One iteration: 1691951294.996958\n",
      "One iteration: 1691951294.999848\n",
      "One iteration: 1691951295.002822\n",
      "One iteration: 1691951295.006125\n",
      "One iteration: 1691951295.009534\n",
      "One iteration: 1691951295.012539\n",
      "One iteration: 1691951295.015351\n",
      "One iteration: 1691951295.018442\n",
      "One iteration: 1691951295.0212722\n",
      "One iteration: 1691951295.0241358\n",
      "One iteration: 1691951295.027571\n",
      "One iteration: 1691951295.030283\n",
      "One iteration: 1691951295.033003\n",
      "One iteration: 1691951295.0358038\n",
      "One iteration: 1691951295.039021\n",
      "One iteration: 1691951295.042219\n",
      "One iteration: 1691951295.0452821\n",
      "One iteration: 1691951295.048011\n",
      "One iteration: 1691951295.051044\n",
      "One iteration: 1691951295.053715\n",
      "One iteration: 1691951295.056926\n",
      "One iteration: 1691951295.0602539\n",
      "One iteration: 1691951295.062907\n",
      "One iteration: 1691951295.0656261\n",
      "One iteration: 1691951295.068373\n",
      "One iteration: 1691951295.071412\n",
      "One iteration: 1691951295.07422\n",
      "One iteration: 1691951295.0774772\n",
      "One iteration: 1691951295.080058\n",
      "One iteration: 1691951295.0829709\n",
      "One iteration: 1691951295.0856042\n",
      "One iteration: 1691951295.088737\n",
      "One iteration: 1691951295.0915189\n",
      "One iteration: 1691951295.0947468\n",
      "One iteration: 1691951295.097408\n",
      "One iteration: 1691951295.10045\n",
      "One iteration: 1691951295.1031868\n",
      "One iteration: 1691951295.106261\n",
      "One iteration: 1691951295.109452\n",
      "One iteration: 1691951295.112837\n",
      "One iteration: 1691951295.1159449\n",
      "One iteration: 1691951295.118918\n",
      "One iteration: 1691951295.122968\n",
      "One iteration: 1691951295.1261692\n",
      "One iteration: 1691951295.129293\n",
      "One iteration: 1691951295.1321118\n",
      "One iteration: 1691951295.1354618\n",
      "One iteration: 1691951295.13863\n",
      "One iteration: 1691951295.1414962\n",
      "One iteration: 1691951295.145129\n",
      "One iteration: 1691951295.1477718\n",
      "One iteration: 1691951295.150746\n",
      "One iteration: 1691951295.153402\n",
      "One iteration: 1691951295.156854\n",
      "One iteration: 1691951295.160479\n",
      "One iteration: 1691951295.1635368\n",
      "One iteration: 1691951295.166498\n",
      "One iteration: 1691951295.169534\n",
      "One iteration: 1691951295.1727161\n",
      "One iteration: 1691951295.175862\n",
      "One iteration: 1691951295.1792982\n",
      "One iteration: 1691951295.182844\n",
      "One iteration: 1691951295.185598\n",
      "One iteration: 1691951295.188611\n",
      "One iteration: 1691951295.1913202\n",
      "One iteration: 1691951295.1946151\n",
      "One iteration: 1691951295.197323\n",
      "One iteration: 1691951295.2002718\n",
      "One iteration: 1691951295.203027\n",
      "One iteration: 1691951295.206393\n",
      "One iteration: 1691951295.209559\n",
      "One iteration: 1691951295.212748\n",
      "One iteration: 1691951295.215722\n",
      "One iteration: 1691951295.218514\n",
      "One iteration: 1691951295.2212732\n",
      "One iteration: 1691951295.224421\n",
      "One iteration: 1691951295.2277708\n",
      "One iteration: 1691951295.230356\n",
      "One iteration: 1691951295.233336\n",
      "One iteration: 1691951295.236182\n",
      "One iteration: 1691951295.23909\n",
      "One iteration: 1691951295.242597\n",
      "One iteration: 1691951295.2456279\n",
      "One iteration: 1691951295.2483861\n",
      "One iteration: 1691951295.251714\n",
      "One iteration: 1691951295.2550678\n",
      "One iteration: 1691951295.257725\n",
      "One iteration: 1691951295.2616441\n",
      "One iteration: 1691951295.264884\n",
      "One iteration: 1691951295.2681332\n",
      "One iteration: 1691951295.271154\n",
      "One iteration: 1691951295.274323\n",
      "One iteration: 1691951295.278058\n",
      "One iteration: 1691951295.280938\n",
      "One iteration: 1691951295.284356\n",
      "One iteration: 1691951295.2876449\n",
      "One iteration: 1691951295.290829\n",
      "One iteration: 1691951295.297537\n",
      "One iteration: 1691951295.301862\n",
      "One iteration: 1691951295.305911\n",
      "One iteration: 1691951295.308888\n",
      "One iteration: 1691951295.312075\n",
      "One iteration: 1691951295.3147788\n",
      "One iteration: 1691951295.317656\n",
      "One iteration: 1691951295.3210502\n",
      "One iteration: 1691951295.324186\n",
      "One iteration: 1691951295.328701\n",
      "One iteration: 1691951295.331871\n",
      "One iteration: 1691951295.334762\n",
      "One iteration: 1691951295.338231\n",
      "One iteration: 1691951295.34095\n",
      "One iteration: 1691951295.344243\n",
      "One iteration: 1691951295.347073\n",
      "One iteration: 1691951295.350084\n",
      "One iteration: 1691951295.353054\n",
      "One iteration: 1691951295.356184\n",
      "One iteration: 1691951295.359194\n",
      "One iteration: 1691951295.362533\n",
      "One iteration: 1691951295.365941\n",
      "One iteration: 1691951295.370045\n",
      "One iteration: 1691951295.372971\n",
      "One iteration: 1691951295.375826\n",
      "One iteration: 1691951295.379702\n",
      "One iteration: 1691951295.383081\n",
      "One iteration: 1691951295.385875\n",
      "One iteration: 1691951295.388872\n",
      "One iteration: 1691951295.391637\n",
      "One iteration: 1691951295.395\n",
      "One iteration: 1691951295.397743\n",
      "One iteration: 1691951295.40133\n",
      "One iteration: 1691951295.4040298\n",
      "One iteration: 1691951295.407155\n",
      "One iteration: 1691951295.4104161\n",
      "One iteration: 1691951295.4133382\n",
      "One iteration: 1691951295.416777\n",
      "One iteration: 1691951295.4195251\n",
      "One iteration: 1691951295.4224691\n",
      "One iteration: 1691951295.42516\n",
      "One iteration: 1691951295.4284842\n",
      "One iteration: 1691951295.4311318\n",
      "One iteration: 1691951295.434289\n",
      "One iteration: 1691951295.437069\n",
      "One iteration: 1691951295.440025\n",
      "One iteration: 1691951295.443029\n",
      "One iteration: 1691951295.446252\n",
      "One iteration: 1691951295.449108\n",
      "One iteration: 1691951295.452597\n",
      "One iteration: 1691951295.4556239\n",
      "One iteration: 1691951295.458657\n",
      "One iteration: 1691951295.462077\n",
      "One iteration: 1691951295.4648561\n",
      "One iteration: 1691951295.467891\n",
      "One iteration: 1691951295.470676\n",
      "One iteration: 1691951295.473475\n",
      "One iteration: 1691951295.476622\n",
      "One iteration: 1691951295.4802089\n",
      "One iteration: 1691951295.483254\n",
      "One iteration: 1691951295.486343\n",
      "One iteration: 1691951295.489417\n",
      "One iteration: 1691951295.492361\n",
      "One iteration: 1691951295.495976\n",
      "One iteration: 1691951295.499038\n",
      "One iteration: 1691951295.50268\n",
      "One iteration: 1691951295.507026\n",
      "One iteration: 1691951295.510943\n",
      "One iteration: 1691951295.514709\n",
      "One iteration: 1691951295.517937\n",
      "One iteration: 1691951295.5211709\n",
      "One iteration: 1691951295.524809\n",
      "One iteration: 1691951295.528409\n",
      "One iteration: 1691951295.5337942\n",
      "One iteration: 1691951295.539124\n",
      "One iteration: 1691951295.542294\n",
      "One iteration: 1691951295.546784\n",
      "One iteration: 1691951295.549956\n",
      "One iteration: 1691951295.553822\n",
      "One iteration: 1691951295.5580218\n",
      "One iteration: 1691951295.561654\n",
      "One iteration: 1691951295.565273\n",
      "One iteration: 1691951295.5684679\n",
      "One iteration: 1691951295.5718331\n",
      "One iteration: 1691951295.574668\n",
      "One iteration: 1691951295.578333\n",
      "One iteration: 1691951295.581316\n",
      "One iteration: 1691951295.5845828\n",
      "One iteration: 1691951295.588501\n",
      "One iteration: 1691951295.591557\n",
      "One iteration: 1691951295.5951161\n",
      "One iteration: 1691951295.597715\n",
      "One iteration: 1691951295.600907\n",
      "One iteration: 1691951295.604525\n",
      "One iteration: 1691951295.608175\n",
      "One iteration: 1691951295.611448\n",
      "One iteration: 1691951295.6144562\n",
      "One iteration: 1691951295.6185129\n",
      "One iteration: 1691951295.6225548\n",
      "One iteration: 1691951295.62544\n",
      "One iteration: 1691951295.628479\n",
      "One iteration: 1691951295.631055\n",
      "One iteration: 1691951295.6345081\n",
      "One iteration: 1691951295.6375859\n",
      "One iteration: 1691951295.640775\n",
      "One iteration: 1691951295.644274\n",
      "One iteration: 1691951295.646923\n",
      "One iteration: 1691951295.649952\n",
      "One iteration: 1691951295.652632\n",
      "One iteration: 1691951295.655827\n",
      "One iteration: 1691951295.6587179\n",
      "One iteration: 1691951295.662177\n",
      "One iteration: 1691951295.665045\n",
      "One iteration: 1691951295.66808\n",
      "One iteration: 1691951295.670882\n",
      "One iteration: 1691951295.673871\n",
      "One iteration: 1691951295.677075\n",
      "One iteration: 1691951295.679947\n",
      "One iteration: 1691951295.682613\n",
      "One iteration: 1691951295.685469\n",
      "One iteration: 1691951295.688441\n",
      "One iteration: 1691951295.691395\n",
      "One iteration: 1691951295.694598\n",
      "One iteration: 1691951295.698095\n",
      "One iteration: 1691951295.701407\n",
      "One iteration: 1691951295.704096\n",
      "One iteration: 1691951295.70716\n",
      "One iteration: 1691951295.7099469\n",
      "One iteration: 1691951295.713409\n",
      "One iteration: 1691951295.716599\n",
      "One iteration: 1691951295.7192872\n",
      "One iteration: 1691951295.72212\n",
      "One iteration: 1691951295.725099\n",
      "One iteration: 1691951295.72855\n",
      "One iteration: 1691951295.7323709\n",
      "One iteration: 1691951295.736185\n",
      "One iteration: 1691951295.739883\n",
      "One iteration: 1691951295.742744\n",
      "One iteration: 1691951295.74578\n",
      "One iteration: 1691951295.7495492\n",
      "One iteration: 1691951295.7526872\n",
      "One iteration: 1691951295.755931\n",
      "One iteration: 1691951295.75894\n",
      "One iteration: 1691951295.762033\n",
      "One iteration: 1691951295.76568\n",
      "One iteration: 1691951295.768723\n",
      "One iteration: 1691951295.772511\n",
      "One iteration: 1691951295.775579\n",
      "One iteration: 1691951295.778684\n",
      "One iteration: 1691951295.782784\n",
      "One iteration: 1691951295.786586\n",
      "One iteration: 1691951295.790009\n",
      "One iteration: 1691951295.792842\n",
      "One iteration: 1691951295.796025\n",
      "One iteration: 1691951295.79892\n",
      "One iteration: 1691951295.80174\n",
      "One iteration: 1691951295.804874\n",
      "One iteration: 1691951295.8078778\n",
      "One iteration: 1691951295.811303\n",
      "One iteration: 1691951295.8142579\n",
      "One iteration: 1691951295.817319\n",
      "One iteration: 1691951295.820076\n",
      "One iteration: 1691951295.823102\n",
      "One iteration: 1691951295.82618\n",
      "One iteration: 1691951295.830293\n",
      "One iteration: 1691951295.8340118\n",
      "One iteration: 1691951295.837258\n",
      "One iteration: 1691951295.840225\n",
      "One iteration: 1691951295.8434958\n",
      "One iteration: 1691951295.8465338\n",
      "One iteration: 1691951295.849347\n",
      "One iteration: 1691951295.852198\n",
      "One iteration: 1691951295.8554041\n",
      "One iteration: 1691951295.858145\n",
      "One iteration: 1691951295.861333\n",
      "One iteration: 1691951295.86398\n",
      "One iteration: 1691951295.86686\n",
      "One iteration: 1691951295.86951\n",
      "One iteration: 1691951295.87324\n",
      "One iteration: 1691951295.876193\n",
      "One iteration: 1691951295.879852\n",
      "One iteration: 1691951295.882883\n",
      "One iteration: 1691951295.885598\n",
      "One iteration: 1691951295.889018\n",
      "One iteration: 1691951295.891799\n",
      "One iteration: 1691951295.895079\n",
      "One iteration: 1691951295.8976939\n",
      "One iteration: 1691951295.9008498\n",
      "One iteration: 1691951295.903571\n",
      "One iteration: 1691951295.906368\n",
      "One iteration: 1691951295.9093041\n",
      "One iteration: 1691951295.912394\n",
      "One iteration: 1691951295.9155\n",
      "One iteration: 1691951295.918493\n",
      "One iteration: 1691951295.9212291\n",
      "One iteration: 1691951295.924054\n",
      "One iteration: 1691951295.927001\n",
      "One iteration: 1691951295.930332\n",
      "One iteration: 1691951295.9334028\n",
      "One iteration: 1691951295.93621\n",
      "One iteration: 1691951295.9401422\n",
      "One iteration: 1691951295.943404\n",
      "One iteration: 1691951295.9465861\n",
      "One iteration: 1691951295.9495418\n",
      "One iteration: 1691951295.952804\n",
      "One iteration: 1691951295.955938\n",
      "One iteration: 1691951295.9585788\n",
      "One iteration: 1691951295.961696\n",
      "One iteration: 1691951295.964792\n",
      "One iteration: 1691951295.9678028\n",
      "One iteration: 1691951295.9709408\n",
      "One iteration: 1691951295.9740539\n",
      "One iteration: 1691951295.9775429\n",
      "One iteration: 1691951295.981256\n",
      "One iteration: 1691951295.984493\n",
      "One iteration: 1691951295.987375\n",
      "One iteration: 1691951295.990717\n",
      "One iteration: 1691951295.993875\n",
      "One iteration: 1691951295.99793\n",
      "One iteration: 1691951296.001059\n",
      "One iteration: 1691951296.003855\n",
      "One iteration: 1691951296.006969\n",
      "One iteration: 1691951296.010247\n",
      "One iteration: 1691951296.0131898\n",
      "One iteration: 1691951296.016188\n",
      "One iteration: 1691951296.019638\n",
      "One iteration: 1691951296.023144\n",
      "One iteration: 1691951296.026291\n",
      "One iteration: 1691951296.0305011\n",
      "One iteration: 1691951296.034691\n",
      "One iteration: 1691951296.038592\n",
      "One iteration: 1691951296.041845\n",
      "One iteration: 1691951296.0451589\n",
      "One iteration: 1691951296.04788\n",
      "One iteration: 1691951296.050786\n",
      "One iteration: 1691951296.0535018\n",
      "One iteration: 1691951296.056988\n",
      "One iteration: 1691951296.060081\n",
      "One iteration: 1691951296.0631628\n",
      "One iteration: 1691951296.06659\n",
      "One iteration: 1691951296.069345\n",
      "One iteration: 1691951296.07276\n",
      "One iteration: 1691951296.0755572\n",
      "One iteration: 1691951296.078965\n",
      "One iteration: 1691951296.082179\n",
      "One iteration: 1691951296.085257\n",
      "One iteration: 1691951296.088104\n",
      "One iteration: 1691951296.090911\n",
      "One iteration: 1691951296.094323\n",
      "One iteration: 1691951296.096968\n",
      "One iteration: 1691951296.0998762\n",
      "One iteration: 1691951296.102615\n",
      "One iteration: 1691951296.105643\n",
      "One iteration: 1691951296.108413\n",
      "One iteration: 1691951296.111989\n",
      "One iteration: 1691951296.115019\n",
      "One iteration: 1691951296.118321\n",
      "One iteration: 1691951296.121741\n",
      "One iteration: 1691951296.124927\n",
      "One iteration: 1691951296.1295328\n",
      "One iteration: 1691951296.133132\n",
      "One iteration: 1691951296.1358342\n",
      "One iteration: 1691951296.138762\n",
      "One iteration: 1691951296.141453\n",
      "One iteration: 1691951296.145942\n",
      "One iteration: 1691951296.1492212\n",
      "One iteration: 1691951296.152212\n",
      "One iteration: 1691951296.155412\n",
      "One iteration: 1691951296.159161\n",
      "One iteration: 1691951296.1626601\n",
      "One iteration: 1691951296.165659\n",
      "One iteration: 1691951296.1692858\n",
      "One iteration: 1691951296.1723719\n",
      "One iteration: 1691951296.175224\n",
      "One iteration: 1691951296.178708\n",
      "One iteration: 1691951296.181524\n",
      "One iteration: 1691951296.184722\n",
      "One iteration: 1691951296.188222\n",
      "One iteration: 1691951296.19104\n",
      "One iteration: 1691951296.194104\n",
      "One iteration: 1691951296.197303\n",
      "One iteration: 1691951296.201351\n",
      "One iteration: 1691951296.203999\n",
      "One iteration: 1691951296.2066529\n",
      "One iteration: 1691951296.209476\n",
      "One iteration: 1691951296.212127\n",
      "One iteration: 1691951296.214626\n",
      "One iteration: 1691951296.2175941\n",
      "One iteration: 1691951296.221431\n",
      "One iteration: 1691951296.224756\n",
      "One iteration: 1691951296.228152\n",
      "One iteration: 1691951296.230968\n",
      "One iteration: 1691951296.2338529\n",
      "One iteration: 1691951296.2369668\n",
      "One iteration: 1691951296.240036\n",
      "One iteration: 1691951296.242761\n",
      "One iteration: 1691951296.2459\n",
      "One iteration: 1691951296.2487779\n",
      "One iteration: 1691951296.251758\n",
      "One iteration: 1691951296.254563\n",
      "One iteration: 1691951296.2577732\n",
      "One iteration: 1691951296.260549\n",
      "One iteration: 1691951296.263786\n",
      "One iteration: 1691951296.2670798\n",
      "One iteration: 1691951296.270397\n",
      "One iteration: 1691951296.273518\n",
      "One iteration: 1691951296.276737\n",
      "One iteration: 1691951296.2802188\n",
      "One iteration: 1691951296.2837691\n",
      "One iteration: 1691951296.2867131\n",
      "One iteration: 1691951296.290287\n",
      "One iteration: 1691951296.293335\n",
      "One iteration: 1691951296.297239\n",
      "One iteration: 1691951296.300534\n",
      "One iteration: 1691951296.303618\n",
      "One iteration: 1691951296.306793\n",
      "One iteration: 1691951296.309773\n",
      "One iteration: 1691951296.313545\n",
      "One iteration: 1691951296.316752\n",
      "One iteration: 1691951296.3199732\n",
      "One iteration: 1691951296.323454\n",
      "One iteration: 1691951296.327188\n",
      "One iteration: 1691951296.3305678\n",
      "One iteration: 1691951296.333878\n",
      "One iteration: 1691951296.336618\n",
      "One iteration: 1691951296.34\n",
      "One iteration: 1691951296.342884\n",
      "One iteration: 1691951296.3462029\n",
      "One iteration: 1691951296.349907\n",
      "One iteration: 1691951296.353152\n",
      "One iteration: 1691951296.3563552\n",
      "One iteration: 1691951296.359317\n",
      "One iteration: 1691951296.362933\n",
      "One iteration: 1691951296.3665159\n",
      "One iteration: 1691951296.3692641\n",
      "One iteration: 1691951296.372319\n",
      "One iteration: 1691951296.375241\n",
      "One iteration: 1691951296.379284\n",
      "One iteration: 1691951296.38274\n",
      "One iteration: 1691951296.385695\n",
      "One iteration: 1691951296.389478\n",
      "One iteration: 1691951296.395992\n",
      "One iteration: 1691951296.400021\n",
      "One iteration: 1691951296.4044642\n",
      "One iteration: 1691951296.408705\n",
      "One iteration: 1691951296.4114718\n",
      "One iteration: 1691951296.4141219\n",
      "One iteration: 1691951296.416883\n",
      "One iteration: 1691951296.419723\n",
      "One iteration: 1691951296.422668\n",
      "One iteration: 1691951296.425626\n",
      "One iteration: 1691951296.429303\n",
      "One iteration: 1691951296.4324632\n",
      "One iteration: 1691951296.435364\n",
      "One iteration: 1691951296.4387999\n",
      "One iteration: 1691951296.441783\n",
      "One iteration: 1691951296.445249\n",
      "One iteration: 1691951296.448607\n",
      "One iteration: 1691951296.4515939\n",
      "One iteration: 1691951296.454452\n",
      "One iteration: 1691951296.457954\n",
      "One iteration: 1691951296.4613812\n",
      "One iteration: 1691951296.4650612\n",
      "One iteration: 1691951296.469294\n",
      "One iteration: 1691951296.472935\n",
      "One iteration: 1691951296.475863\n",
      "One iteration: 1691951296.479146\n",
      "One iteration: 1691951296.4822059\n",
      "One iteration: 1691951296.4855921\n",
      "One iteration: 1691951296.4887\n",
      "One iteration: 1691951296.4913878\n",
      "One iteration: 1691951296.494379\n",
      "One iteration: 1691951296.497749\n",
      "One iteration: 1691951296.500737\n",
      "One iteration: 1691951296.503861\n",
      "One iteration: 1691951296.506763\n",
      "One iteration: 1691951296.509769\n",
      "One iteration: 1691951296.5136719\n",
      "One iteration: 1691951296.516741\n",
      "One iteration: 1691951296.51964\n",
      "One iteration: 1691951296.523447\n",
      "One iteration: 1691951296.526857\n",
      "One iteration: 1691951296.530366\n",
      "One iteration: 1691951296.533675\n",
      "One iteration: 1691951296.536799\n",
      "One iteration: 1691951296.539686\n",
      "One iteration: 1691951296.542551\n",
      "One iteration: 1691951296.547116\n",
      "One iteration: 1691951296.550886\n",
      "One iteration: 1691951296.554903\n",
      "One iteration: 1691951296.55808\n",
      "One iteration: 1691951296.5612679\n",
      "One iteration: 1691951296.5646122\n",
      "One iteration: 1691951296.568094\n",
      "One iteration: 1691951296.570838\n",
      "One iteration: 1691951296.57408\n",
      "One iteration: 1691951296.576908\n",
      "One iteration: 1691951296.580236\n",
      "One iteration: 1691951296.583364\n",
      "One iteration: 1691951296.5871449\n",
      "One iteration: 1691951296.590397\n",
      "One iteration: 1691951296.594144\n",
      "One iteration: 1691951296.598112\n",
      "One iteration: 1691951296.601289\n",
      "One iteration: 1691951296.6041129\n",
      "One iteration: 1691951296.607466\n",
      "One iteration: 1691951296.611316\n",
      "One iteration: 1691951296.614207\n",
      "One iteration: 1691951296.6175702\n",
      "One iteration: 1691951296.621675\n",
      "One iteration: 1691951296.625202\n",
      "One iteration: 1691951296.6290429\n",
      "One iteration: 1691951296.631961\n",
      "One iteration: 1691951296.635066\n",
      "One iteration: 1691951296.6390579\n",
      "One iteration: 1691951296.642868\n",
      "One iteration: 1691951296.646824\n",
      "One iteration: 1691951296.6504161\n",
      "One iteration: 1691951296.654206\n",
      "One iteration: 1691951296.657398\n",
      "One iteration: 1691951296.660635\n",
      "One iteration: 1691951296.6638222\n",
      "One iteration: 1691951296.6671958\n",
      "One iteration: 1691951296.669985\n",
      "One iteration: 1691951296.673646\n",
      "One iteration: 1691951296.676969\n",
      "One iteration: 1691951296.6799831\n",
      "One iteration: 1691951296.683373\n",
      "One iteration: 1691951296.686124\n",
      "One iteration: 1691951296.689197\n",
      "One iteration: 1691951296.692641\n",
      "One iteration: 1691951296.6962738\n",
      "One iteration: 1691951296.699351\n",
      "One iteration: 1691951296.702295\n",
      "One iteration: 1691951296.705499\n",
      "One iteration: 1691951296.710638\n",
      "One iteration: 1691951296.71402\n",
      "One iteration: 1691951296.717105\n",
      "One iteration: 1691951296.719766\n",
      "One iteration: 1691951296.722974\n",
      "One iteration: 1691951296.726261\n",
      "One iteration: 1691951296.729839\n",
      "One iteration: 1691951296.734802\n",
      "One iteration: 1691951296.737665\n",
      "One iteration: 1691951296.740575\n",
      "One iteration: 1691951296.744931\n",
      "One iteration: 1691951296.749803\n",
      "One iteration: 1691951296.752426\n",
      "One iteration: 1691951296.755223\n",
      "One iteration: 1691951296.759309\n",
      "One iteration: 1691951296.763181\n",
      "One iteration: 1691951296.7664871\n",
      "One iteration: 1691951296.769258\n",
      "One iteration: 1691951296.7722082\n",
      "One iteration: 1691951296.774966\n",
      "One iteration: 1691951296.778526\n",
      "One iteration: 1691951296.781795\n",
      "One iteration: 1691951296.7851121\n",
      "One iteration: 1691951296.7887511\n",
      "One iteration: 1691951296.791805\n",
      "One iteration: 1691951296.795116\n",
      "One iteration: 1691951296.798708\n",
      "One iteration: 1691951296.801774\n",
      "One iteration: 1691951296.8046699\n",
      "One iteration: 1691951296.807659\n",
      "One iteration: 1691951296.810923\n",
      "One iteration: 1691951296.8143098\n",
      "One iteration: 1691951296.817698\n",
      "One iteration: 1691951296.820884\n",
      "One iteration: 1691951296.823898\n",
      "One iteration: 1691951296.82714\n",
      "One iteration: 1691951296.829998\n",
      "One iteration: 1691951296.832921\n",
      "One iteration: 1691951296.835865\n",
      "One iteration: 1691951296.838986\n",
      "One iteration: 1691951296.841717\n",
      "One iteration: 1691951296.844813\n",
      "One iteration: 1691951296.847431\n",
      "One iteration: 1691951296.850483\n",
      "One iteration: 1691951296.8535302\n",
      "One iteration: 1691951296.856464\n",
      "One iteration: 1691951296.859529\n",
      "One iteration: 1691951296.862893\n",
      "One iteration: 1691951296.8657768\n",
      "One iteration: 1691951296.868599\n",
      "One iteration: 1691951296.871597\n",
      "One iteration: 1691951296.8744462\n",
      "One iteration: 1691951296.878179\n",
      "One iteration: 1691951296.880911\n",
      "One iteration: 1691951296.883851\n",
      "One iteration: 1691951296.886658\n",
      "One iteration: 1691951296.889781\n",
      "One iteration: 1691951296.892777\n",
      "One iteration: 1691951296.89603\n",
      "One iteration: 1691951296.899074\n",
      "One iteration: 1691951296.901792\n",
      "One iteration: 1691951296.904703\n",
      "One iteration: 1691951296.907818\n",
      "One iteration: 1691951296.911229\n",
      "One iteration: 1691951296.913874\n",
      "One iteration: 1691951296.9167502\n",
      "One iteration: 1691951296.919595\n",
      "One iteration: 1691951296.922745\n",
      "One iteration: 1691951296.9254808\n",
      "One iteration: 1691951296.928863\n",
      "One iteration: 1691951296.931926\n",
      "One iteration: 1691951296.935094\n",
      "One iteration: 1691951296.938663\n",
      "One iteration: 1691951296.9413729\n",
      "One iteration: 1691951296.944371\n",
      "One iteration: 1691951296.947679\n",
      "One iteration: 1691951296.9509609\n",
      "One iteration: 1691951296.954196\n",
      "One iteration: 1691951296.957464\n",
      "One iteration: 1691951296.960855\n",
      "One iteration: 1691951296.964909\n",
      "One iteration: 1691951296.968222\n",
      "One iteration: 1691951296.971436\n",
      "One iteration: 1691951296.974535\n",
      "One iteration: 1691951296.97789\n",
      "One iteration: 1691951296.981066\n",
      "One iteration: 1691951296.9840782\n",
      "One iteration: 1691951296.987155\n",
      "One iteration: 1691951296.990058\n",
      "One iteration: 1691951296.993372\n",
      "One iteration: 1691951296.997138\n",
      "One iteration: 1691951297.0000331\n",
      "One iteration: 1691951297.004044\n",
      "One iteration: 1691951297.009055\n",
      "One iteration: 1691951297.013403\n",
      "One iteration: 1691951297.017293\n",
      "One iteration: 1691951297.0207832\n",
      "One iteration: 1691951297.024223\n",
      "One iteration: 1691951297.0272992\n",
      "One iteration: 1691951297.030755\n",
      "One iteration: 1691951297.033761\n",
      "One iteration: 1691951297.037056\n",
      "One iteration: 1691951297.040084\n",
      "One iteration: 1691951297.043623\n",
      "One iteration: 1691951297.046547\n",
      "One iteration: 1691951297.049411\n",
      "One iteration: 1691951297.052279\n",
      "One iteration: 1691951297.055815\n",
      "One iteration: 1691951297.059514\n",
      "One iteration: 1691951297.0626938\n",
      "One iteration: 1691951297.065342\n",
      "One iteration: 1691951297.0681658\n",
      "One iteration: 1691951297.071628\n",
      "One iteration: 1691951297.0759602\n",
      "One iteration: 1691951297.0787451\n",
      "One iteration: 1691951297.0813959\n",
      "One iteration: 1691951297.084522\n",
      "One iteration: 1691951297.0875058\n",
      "One iteration: 1691951297.090912\n",
      "One iteration: 1691951297.09479\n",
      "One iteration: 1691951297.0974321\n",
      "One iteration: 1691951297.100485\n",
      "One iteration: 1691951297.1035771\n",
      "One iteration: 1691951297.1067898\n",
      "One iteration: 1691951297.110979\n",
      "One iteration: 1691951297.1138408\n",
      "One iteration: 1691951297.116881\n",
      "One iteration: 1691951297.1201642\n",
      "One iteration: 1691951297.123659\n",
      "One iteration: 1691951297.128566\n",
      "One iteration: 1691951297.131934\n",
      "One iteration: 1691951297.134999\n",
      "One iteration: 1691951297.138291\n",
      "One iteration: 1691951297.142441\n",
      "One iteration: 1691951297.1459851\n",
      "One iteration: 1691951297.148766\n",
      "One iteration: 1691951297.1517062\n",
      "One iteration: 1691951297.1550772\n",
      "One iteration: 1691951297.158952\n",
      "One iteration: 1691951297.163445\n",
      "One iteration: 1691951297.167295\n",
      "One iteration: 1691951297.1701481\n",
      "One iteration: 1691951297.1738062\n",
      "One iteration: 1691951297.1772969\n",
      "One iteration: 1691951297.1800032\n",
      "One iteration: 1691951297.182948\n",
      "One iteration: 1691951297.185686\n",
      "One iteration: 1691951297.1891682\n",
      "One iteration: 1691951297.192734\n",
      "One iteration: 1691951297.1954188\n",
      "One iteration: 1691951297.198826\n",
      "One iteration: 1691951297.201976\n",
      "One iteration: 1691951297.205579\n",
      "One iteration: 1691951297.209192\n",
      "One iteration: 1691951297.212238\n",
      "One iteration: 1691951297.215612\n",
      "One iteration: 1691951297.218765\n",
      "One iteration: 1691951297.222385\n",
      "One iteration: 1691951297.225981\n",
      "One iteration: 1691951297.228975\n",
      "One iteration: 1691951297.2319071\n",
      "One iteration: 1691951297.234952\n",
      "One iteration: 1691951297.2383518\n",
      "One iteration: 1691951297.242102\n",
      "One iteration: 1691951297.245229\n",
      "One iteration: 1691951297.247968\n",
      "One iteration: 1691951297.25138\n",
      "One iteration: 1691951297.254434\n",
      "One iteration: 1691951297.257781\n",
      "One iteration: 1691951297.261178\n",
      "One iteration: 1691951297.264245\n",
      "One iteration: 1691951297.267536\n",
      "One iteration: 1691951297.2705102\n",
      "One iteration: 1691951297.2739148\n",
      "One iteration: 1691951297.277633\n",
      "One iteration: 1691951297.280299\n",
      "One iteration: 1691951297.2844002\n",
      "One iteration: 1691951297.287306\n",
      "One iteration: 1691951297.290355\n",
      "One iteration: 1691951297.294037\n",
      "One iteration: 1691951297.296695\n",
      "One iteration: 1691951297.299666\n",
      "One iteration: 1691951297.30248\n",
      "One iteration: 1691951297.305677\n",
      "One iteration: 1691951297.309283\n",
      "One iteration: 1691951297.312105\n",
      "One iteration: 1691951297.3149672\n",
      "One iteration: 1691951297.317794\n",
      "One iteration: 1691951297.3205051\n",
      "One iteration: 1691951297.323829\n",
      "One iteration: 1691951297.327144\n",
      "One iteration: 1691951297.330175\n",
      "One iteration: 1691951297.3328729\n",
      "One iteration: 1691951297.3356109\n",
      "One iteration: 1691951297.338608\n",
      "One iteration: 1691951297.341565\n",
      "One iteration: 1691951297.345005\n",
      "One iteration: 1691951297.347687\n",
      "One iteration: 1691951297.3504179\n",
      "One iteration: 1691951297.3531032\n",
      "One iteration: 1691951297.356235\n",
      "One iteration: 1691951297.359804\n",
      "One iteration: 1691951297.362509\n",
      "One iteration: 1691951297.36577\n",
      "One iteration: 1691951297.369248\n",
      "One iteration: 1691951297.3722491\n",
      "One iteration: 1691951297.37509\n",
      "One iteration: 1691951297.3793778\n",
      "One iteration: 1691951297.3823922\n",
      "One iteration: 1691951297.386144\n",
      "One iteration: 1691951297.389487\n",
      "One iteration: 1691951297.396058\n",
      "One iteration: 1691951297.399056\n",
      "One iteration: 1691951297.402053\n",
      "One iteration: 1691951297.40582\n",
      "One iteration: 1691951297.410317\n",
      "One iteration: 1691951297.4138508\n",
      "One iteration: 1691951297.416681\n",
      "One iteration: 1691951297.4193559\n",
      "One iteration: 1691951297.4223402\n",
      "One iteration: 1691951297.4262211\n",
      "One iteration: 1691951297.429238\n",
      "One iteration: 1691951297.4319448\n",
      "One iteration: 1691951297.434995\n",
      "One iteration: 1691951297.4403672\n",
      "One iteration: 1691951297.4436831\n",
      "One iteration: 1691951297.446493\n",
      "One iteration: 1691951297.449187\n",
      "One iteration: 1691951297.452089\n",
      "One iteration: 1691951297.458577\n",
      "One iteration: 1691951297.4618711\n",
      "One iteration: 1691951297.465287\n",
      "One iteration: 1691951297.468299\n",
      "One iteration: 1691951297.473454\n",
      "One iteration: 1691951297.477622\n",
      "One iteration: 1691951297.48032\n",
      "One iteration: 1691951297.483367\n",
      "One iteration: 1691951297.486112\n",
      "One iteration: 1691951297.492119\n",
      "One iteration: 1691951297.495379\n",
      "One iteration: 1691951297.498125\n",
      "One iteration: 1691951297.50105\n",
      "One iteration: 1691951297.503765\n",
      "One iteration: 1691951297.508563\n",
      "One iteration: 1691951297.513267\n",
      "One iteration: 1691951297.5165849\n",
      "One iteration: 1691951297.519223\n",
      "One iteration: 1691951297.522034\n",
      "One iteration: 1691951297.528555\n",
      "One iteration: 1691951297.531724\n",
      "One iteration: 1691951297.534854\n",
      "One iteration: 1691951297.537588\n",
      "One iteration: 1691951297.543505\n",
      "One iteration: 1691951297.5465658\n",
      "One iteration: 1691951297.5496871\n",
      "One iteration: 1691951297.5523489\n",
      "One iteration: 1691951297.558721\n",
      "One iteration: 1691951297.5618932\n",
      "One iteration: 1691951297.565108\n",
      "One iteration: 1691951297.568908\n",
      "One iteration: 1691951297.5755572\n",
      "One iteration: 1691951297.5787241\n",
      "One iteration: 1691951297.581877\n",
      "One iteration: 1691951297.585064\n",
      "One iteration: 1691951297.591558\n",
      "One iteration: 1691951297.594899\n",
      "One iteration: 1691951297.598342\n",
      "One iteration: 1691951297.601387\n",
      "One iteration: 1691951297.604604\n",
      "One iteration: 1691951297.611861\n",
      "One iteration: 1691951297.614933\n",
      "One iteration: 1691951297.618088\n",
      "One iteration: 1691951297.621086\n",
      "One iteration: 1691951297.628739\n",
      "One iteration: 1691951297.63297\n",
      "One iteration: 1691951297.6366\n",
      "One iteration: 1691951297.6398132\n",
      "One iteration: 1691951297.643167\n",
      "One iteration: 1691951297.648842\n",
      "One iteration: 1691951297.6534662\n",
      "One iteration: 1691951297.656994\n",
      "One iteration: 1691951297.6602209\n",
      "One iteration: 1691951297.664064\n",
      "One iteration: 1691951297.672141\n",
      "One iteration: 1691951297.675624\n",
      "One iteration: 1691951297.679157\n",
      "One iteration: 1691951297.682009\n",
      "One iteration: 1691951297.68769\n",
      "One iteration: 1691951297.6913202\n",
      "One iteration: 1691951297.694717\n",
      "One iteration: 1691951297.697469\n",
      "One iteration: 1691951297.703566\n",
      "One iteration: 1691951297.707247\n",
      "One iteration: 1691951297.7106519\n",
      "One iteration: 1691951297.7136111\n",
      "One iteration: 1691951297.7207942\n",
      "One iteration: 1691951297.724129\n",
      "One iteration: 1691951297.727343\n",
      "One iteration: 1691951297.730152\n",
      "One iteration: 1691951297.733115\n",
      "One iteration: 1691951297.739719\n",
      "One iteration: 1691951297.7430558\n",
      "One iteration: 1691951297.7460039\n",
      "One iteration: 1691951297.752589\n",
      "One iteration: 1691951297.756006\n",
      "One iteration: 1691951297.759742\n",
      "One iteration: 1691951297.762951\n",
      "One iteration: 1691951297.768725\n",
      "One iteration: 1691951297.772688\n",
      "One iteration: 1691951297.776563\n",
      "One iteration: 1691951297.7800488\n",
      "One iteration: 1691951297.7851331\n",
      "One iteration: 1691951297.789423\n",
      "One iteration: 1691951297.792702\n",
      "One iteration: 1691951297.796093\n",
      "One iteration: 1691951297.7988942\n",
      "One iteration: 1691951297.802568\n",
      "One iteration: 1691951297.809175\n",
      "One iteration: 1691951297.812574\n",
      "One iteration: 1691951297.815488\n",
      "One iteration: 1691951297.820201\n",
      "One iteration: 1691951297.825464\n",
      "One iteration: 1691951297.829476\n",
      "One iteration: 1691951297.832706\n",
      "One iteration: 1691951297.835857\n",
      "One iteration: 1691951297.842674\n",
      "One iteration: 1691951297.846449\n",
      "One iteration: 1691951297.849395\n",
      "One iteration: 1691951297.852222\n",
      "One iteration: 1691951297.8554618\n",
      "One iteration: 1691951297.862609\n",
      "One iteration: 1691951297.865629\n",
      "One iteration: 1691951297.868515\n",
      "One iteration: 1691951297.871915\n",
      "One iteration: 1691951297.875901\n",
      "One iteration: 1691951297.881353\n",
      "One iteration: 1691951297.884439\n",
      "One iteration: 1691951297.8873641\n",
      "One iteration: 1691951297.890456\n",
      "One iteration: 1691951297.895168\n",
      "One iteration: 1691951297.899783\n",
      "One iteration: 1691951297.90255\n",
      "One iteration: 1691951297.905555\n",
      "One iteration: 1691951297.9086971\n",
      "One iteration: 1691951297.9147751\n",
      "One iteration: 1691951297.9177878\n",
      "One iteration: 1691951297.920492\n",
      "One iteration: 1691951297.923579\n",
      "One iteration: 1691951297.92979\n",
      "One iteration: 1691951297.932723\n",
      "One iteration: 1691951297.935452\n",
      "One iteration: 1691951297.9389398\n",
      "One iteration: 1691951297.944983\n",
      "One iteration: 1691951297.948218\n",
      "One iteration: 1691951297.9513578\n",
      "One iteration: 1691951297.9548008\n",
      "One iteration: 1691951297.958956\n",
      "One iteration: 1691951297.96426\n",
      "One iteration: 1691951297.96789\n",
      "One iteration: 1691951297.97204\n",
      "One iteration: 1691951297.977438\n",
      "One iteration: 1691951297.980525\n",
      "One iteration: 1691951297.9833589\n",
      "One iteration: 1691951297.986412\n",
      "One iteration: 1691951297.989401\n",
      "One iteration: 1691951297.993113\n",
      "One iteration: 1691951297.996245\n",
      "One iteration: 1691951297.999074\n",
      "One iteration: 1691951298.002153\n",
      "One iteration: 1691951298.005474\n",
      "One iteration: 1691951298.0120971\n",
      "One iteration: 1691951298.015465\n",
      "One iteration: 1691951298.0191832\n",
      "One iteration: 1691951298.02254\n",
      "One iteration: 1691951298.029995\n",
      "One iteration: 1691951298.0334778\n",
      "One iteration: 1691951298.036413\n",
      "One iteration: 1691951298.040067\n",
      "One iteration: 1691951298.043638\n",
      "One iteration: 1691951298.049997\n",
      "One iteration: 1691951298.052843\n",
      "One iteration: 1691951298.056479\n",
      "One iteration: 1691951298.059951\n",
      "One iteration: 1691951298.066036\n",
      "One iteration: 1691951298.069474\n",
      "One iteration: 1691951298.0728152\n",
      "One iteration: 1691951298.0764\n",
      "One iteration: 1691951298.079068\n",
      "One iteration: 1691951298.082099\n",
      "One iteration: 1691951298.088382\n",
      "One iteration: 1691951298.092237\n",
      "One iteration: 1691951298.095096\n",
      "One iteration: 1691951298.097812\n",
      "One iteration: 1691951298.103313\n",
      "One iteration: 1691951298.106934\n",
      "One iteration: 1691951298.110414\n",
      "One iteration: 1691951298.113437\n",
      "One iteration: 1691951298.120445\n",
      "One iteration: 1691951298.123915\n",
      "One iteration: 1691951298.127985\n",
      "One iteration: 1691951298.132075\n",
      "One iteration: 1691951298.138365\n",
      "One iteration: 1691951298.142972\n",
      "One iteration: 1691951298.146492\n",
      "One iteration: 1691951298.149607\n",
      "One iteration: 1691951298.155044\n",
      "One iteration: 1691951298.159034\n",
      "One iteration: 1691951298.1624188\n",
      "One iteration: 1691951298.1654441\n",
      "One iteration: 1691951298.169611\n",
      "One iteration: 1691951298.175164\n",
      "One iteration: 1691951298.1790872\n",
      "One iteration: 1691951298.1827772\n",
      "One iteration: 1691951298.186136\n",
      "One iteration: 1691951298.193544\n",
      "One iteration: 1691951298.196457\n",
      "One iteration: 1691951298.1993978\n",
      "One iteration: 1691951298.202401\n",
      "One iteration: 1691951298.209142\n",
      "One iteration: 1691951298.212308\n",
      "One iteration: 1691951298.215538\n",
      "One iteration: 1691951298.2185621\n",
      "One iteration: 1691951298.221667\n",
      "One iteration: 1691951298.2293582\n",
      "One iteration: 1691951298.232438\n",
      "One iteration: 1691951298.235383\n",
      "One iteration: 1691951298.238454\n",
      "One iteration: 1691951298.2446442\n",
      "One iteration: 1691951298.247377\n",
      "One iteration: 1691951298.250169\n",
      "One iteration: 1691951298.253024\n",
      "One iteration: 1691951298.259487\n",
      "One iteration: 1691951298.262107\n",
      "One iteration: 1691951298.265024\n",
      "One iteration: 1691951298.267859\n",
      "One iteration: 1691951298.27154\n",
      "One iteration: 1691951298.278111\n",
      "One iteration: 1691951298.28142\n",
      "One iteration: 1691951298.284768\n",
      "One iteration: 1691951298.288251\n",
      "One iteration: 1691951298.2947168\n",
      "One iteration: 1691951298.2984068\n",
      "One iteration: 1691951298.301441\n",
      "One iteration: 1691951298.308276\n",
      "One iteration: 1691951298.312613\n",
      "One iteration: 1691951298.315857\n",
      "One iteration: 1691951298.318839\n",
      "One iteration: 1691951298.32676\n",
      "One iteration: 1691951298.3305972\n",
      "One iteration: 1691951298.333817\n",
      "One iteration: 1691951298.33723\n",
      "One iteration: 1691951298.344512\n",
      "One iteration: 1691951298.3474932\n",
      "One iteration: 1691951298.3511019\n",
      "One iteration: 1691951298.3544981\n",
      "One iteration: 1691951298.360399\n",
      "One iteration: 1691951298.363748\n",
      "One iteration: 1691951298.3669791\n",
      "One iteration: 1691951298.370005\n",
      "One iteration: 1691951298.3763351\n",
      "One iteration: 1691951298.3792582\n",
      "One iteration: 1691951298.382976\n",
      "One iteration: 1691951298.386639\n",
      "One iteration: 1691951298.3901389\n",
      "One iteration: 1691951298.396826\n",
      "One iteration: 1691951298.399911\n",
      "One iteration: 1691951298.403142\n",
      "One iteration: 1691951298.406552\n",
      "One iteration: 1691951298.4131148\n",
      "One iteration: 1691951298.4162529\n",
      "One iteration: 1691951298.4193668\n",
      "One iteration: 1691951298.422783\n",
      "One iteration: 1691951298.4272661\n",
      "One iteration: 1691951298.43444\n",
      "One iteration: 1691951298.4374988\n",
      "One iteration: 1691951298.441634\n",
      "One iteration: 1691951298.4478889\n",
      "One iteration: 1691951298.451623\n",
      "One iteration: 1691951298.454816\n",
      "One iteration: 1691951298.458486\n",
      "One iteration: 1691951298.461549\n",
      "One iteration: 1691951298.4656792\n",
      "One iteration: 1691951298.4687092\n",
      "One iteration: 1691951298.472044\n",
      "One iteration: 1691951298.478498\n",
      "One iteration: 1691951298.481448\n",
      "One iteration: 1691951298.484595\n",
      "One iteration: 1691951298.487919\n",
      "One iteration: 1691951298.492151\n",
      "One iteration: 1691951298.4975202\n",
      "One iteration: 1691951298.500989\n",
      "One iteration: 1691951298.504278\n",
      "One iteration: 1691951298.507294\n",
      "One iteration: 1691951298.513504\n",
      "One iteration: 1691951298.516375\n",
      "One iteration: 1691951298.51951\n",
      "One iteration: 1691951298.524671\n",
      "One iteration: 1691951298.529152\n",
      "One iteration: 1691951298.53184\n",
      "One iteration: 1691951298.534952\n",
      "One iteration: 1691951298.5377972\n",
      "One iteration: 1691951298.54406\n",
      "One iteration: 1691951298.546779\n",
      "One iteration: 1691951298.54961\n",
      "One iteration: 1691951298.552702\n",
      "One iteration: 1691951298.560031\n",
      "One iteration: 1691951298.562992\n",
      "One iteration: 1691951298.5657861\n",
      "One iteration: 1691951298.568887\n",
      "One iteration: 1691951298.57443\n",
      "One iteration: 1691951298.579085\n",
      "One iteration: 1691951298.5820858\n",
      "One iteration: 1691951298.585644\n",
      "One iteration: 1691951298.589375\n",
      "One iteration: 1691951298.5964851\n",
      "One iteration: 1691951298.599814\n",
      "One iteration: 1691951298.602498\n",
      "One iteration: 1691951298.605846\n",
      "One iteration: 1691951298.61277\n",
      "One iteration: 1691951298.616117\n",
      "One iteration: 1691951298.619136\n",
      "One iteration: 1691951298.622714\n",
      "One iteration: 1691951298.625759\n",
      "One iteration: 1691951298.633141\n",
      "One iteration: 1691951298.635999\n",
      "One iteration: 1691951298.639658\n",
      "One iteration: 1691951298.643357\n",
      "One iteration: 1691951298.6502771\n",
      "One iteration: 1691951298.653909\n",
      "One iteration: 1691951298.656966\n",
      "One iteration: 1691951298.6602519\n",
      "One iteration: 1691951298.66655\n",
      "One iteration: 1691951298.6703098\n",
      "One iteration: 1691951298.673098\n",
      "One iteration: 1691951298.6761029\n",
      "One iteration: 1691951298.6821399\n",
      "One iteration: 1691951298.6852689\n",
      "One iteration: 1691951298.688529\n",
      "One iteration: 1691951298.6912892\n",
      "One iteration: 1691951298.696615\n",
      "One iteration: 1691951298.7001688\n",
      "One iteration: 1691951298.703388\n",
      "One iteration: 1691951298.706752\n",
      "One iteration: 1691951298.713491\n",
      "One iteration: 1691951298.7163148\n",
      "One iteration: 1691951298.719362\n",
      "One iteration: 1691951298.7224681\n",
      "One iteration: 1691951298.727406\n",
      "One iteration: 1691951298.731482\n",
      "One iteration: 1691951298.7348092\n",
      "One iteration: 1691951298.737588\n",
      "One iteration: 1691951298.7404191\n",
      "One iteration: 1691951298.746899\n",
      "One iteration: 1691951298.74985\n",
      "One iteration: 1691951298.7529228\n",
      "One iteration: 1691951298.75602\n",
      "One iteration: 1691951298.763147\n",
      "One iteration: 1691951298.766281\n",
      "One iteration: 1691951298.769176\n",
      "One iteration: 1691951298.772144\n",
      "One iteration: 1691951298.778562\n",
      "One iteration: 1691951298.781396\n",
      "One iteration: 1691951298.7846458\n",
      "One iteration: 1691951298.788388\n",
      "One iteration: 1691951298.795249\n",
      "One iteration: 1691951298.798142\n",
      "One iteration: 1691951298.8009799\n",
      "One iteration: 1691951298.803691\n",
      "One iteration: 1691951298.811112\n",
      "One iteration: 1691951298.8142898\n",
      "One iteration: 1691951298.81725\n",
      "One iteration: 1691951298.820013\n",
      "One iteration: 1691951298.824974\n",
      "One iteration: 1691951298.829746\n",
      "One iteration: 1691951298.83349\n",
      "One iteration: 1691951298.836298\n",
      "One iteration: 1691951298.8402948\n",
      "One iteration: 1691951298.846637\n",
      "One iteration: 1691951298.849657\n",
      "One iteration: 1691951298.852579\n",
      "One iteration: 1691951298.856138\n",
      "One iteration: 1691951298.862319\n",
      "One iteration: 1691951298.865258\n",
      "One iteration: 1691951298.8683\n",
      "One iteration: 1691951298.871389\n",
      "One iteration: 1691951298.875575\n",
      "One iteration: 1691951298.881223\n",
      "One iteration: 1691951298.884449\n",
      "One iteration: 1691951298.8873758\n",
      "One iteration: 1691951298.891565\n",
      "One iteration: 1691951298.8950348\n",
      "One iteration: 1691951298.897794\n",
      "One iteration: 1691951298.900776\n",
      "One iteration: 1691951298.90348\n",
      "One iteration: 1691951298.906466\n",
      "One iteration: 1691951298.911426\n",
      "One iteration: 1691951298.9152892\n",
      "One iteration: 1691951298.918602\n",
      "One iteration: 1691951298.92155\n",
      "One iteration: 1691951298.92448\n",
      "One iteration: 1691951298.9318259\n",
      "One iteration: 1691951298.935051\n",
      "One iteration: 1691951298.9380832\n",
      "One iteration: 1691951298.941549\n",
      "One iteration: 1691951298.947788\n",
      "One iteration: 1691951298.950978\n",
      "One iteration: 1691951298.953884\n",
      "One iteration: 1691951298.957078\n",
      "One iteration: 1691951298.961597\n",
      "One iteration: 1691951298.9648771\n",
      "One iteration: 1691951298.968033\n",
      "One iteration: 1691951298.972647\n",
      "One iteration: 1691951298.975622\n",
      "One iteration: 1691951298.97887\n",
      "One iteration: 1691951298.982367\n",
      "One iteration: 1691951298.985323\n",
      "One iteration: 1691951298.9899\n",
      "One iteration: 1691951298.99363\n",
      "One iteration: 1691951298.997076\n",
      "One iteration: 1691951299.000533\n",
      "One iteration: 1691951299.004839\n",
      "One iteration: 1691951299.009998\n",
      "One iteration: 1691951299.013451\n",
      "One iteration: 1691951299.0171368\n",
      "One iteration: 1691951299.020113\n",
      "One iteration: 1691951299.023283\n",
      "One iteration: 1691951299.02618\n",
      "One iteration: 1691951299.0290701\n",
      "One iteration: 1691951299.031748\n",
      "One iteration: 1691951299.034949\n",
      "One iteration: 1691951299.0381482\n",
      "One iteration: 1691951299.040917\n",
      "One iteration: 1691951299.046008\n",
      "One iteration: 1691951299.050626\n",
      "One iteration: 1691951299.053785\n",
      "One iteration: 1691951299.0568252\n",
      "One iteration: 1691951299.059953\n",
      "One iteration: 1691951299.0667741\n",
      "One iteration: 1691951299.0696561\n",
      "One iteration: 1691951299.0728152\n",
      "One iteration: 1691951299.075575\n",
      "One iteration: 1691951299.0797172\n",
      "One iteration: 1691951299.085666\n",
      "One iteration: 1691951299.089333\n",
      "One iteration: 1691951299.093039\n",
      "One iteration: 1691951299.096407\n",
      "One iteration: 1691951299.101488\n",
      "One iteration: 1691951299.105932\n",
      "One iteration: 1691951299.109047\n",
      "One iteration: 1691951299.112267\n",
      "One iteration: 1691951299.115701\n",
      "One iteration: 1691951299.1207612\n",
      "One iteration: 1691951299.1252842\n",
      "One iteration: 1691951299.128999\n",
      "One iteration: 1691951299.132519\n",
      "One iteration: 1691951299.135394\n",
      "One iteration: 1691951299.138319\n",
      "One iteration: 1691951299.141218\n",
      "One iteration: 1691951299.1442308\n",
      "One iteration: 1691951299.147677\n",
      "One iteration: 1691951299.1508648\n",
      "One iteration: 1691951299.1536078\n",
      "One iteration: 1691951299.156995\n",
      "One iteration: 1691951299.159761\n",
      "One iteration: 1691951299.16286\n",
      "One iteration: 1691951299.1661599\n",
      "One iteration: 1691951299.1690922\n",
      "One iteration: 1691951299.172247\n",
      "One iteration: 1691951299.175046\n",
      "One iteration: 1691951299.1780999\n",
      "One iteration: 1691951299.181398\n",
      "One iteration: 1691951299.184461\n",
      "One iteration: 1691951299.1880991\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m model_init \u001B[38;5;241m=\u001B[39m ntk_init(init_checkpoint,args\u001B[38;5;241m.\u001B[39mseed)\n\u001B[1;32m      3\u001B[0m t1 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m----> 4\u001B[0m G_r,f0_minus_y_r \u001B[38;5;241m=\u001B[39m delta_w_utils(copy\u001B[38;5;241m.\u001B[39mdeepcopy(model),retain_loader,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcomplete\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      5\u001B[0m t2 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m      6\u001B[0m ntk_time \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m t2\u001B[38;5;241m-\u001B[39mt1\n",
      "Cell \u001B[0;32mIn[28], line 16\u001B[0m, in \u001B[0;36mdelta_w_utils\u001B[0;34m(model_init, dataloader, name)\u001B[0m\n\u001B[1;32m     14\u001B[0m G_sample\u001B[38;5;241m=\u001B[39m[]\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_classes):\n\u001B[0;32m---> 16\u001B[0m     grads \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mgrad(output[\u001B[38;5;241m0\u001B[39m,\u001B[38;5;28mcls\u001B[39m],model_init\u001B[38;5;241m.\u001B[39mparameters(),retain_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     17\u001B[0m     grads \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate([g\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy() \u001B[38;5;28;01mfor\u001B[39;00m g \u001B[38;5;129;01min\u001B[39;00m grads])\n\u001B[1;32m     18\u001B[0m     G_sample\u001B[38;5;241m.\u001B[39mappend(grads)\n",
      "File \u001B[0;32m~/anaconda3/envs/unlearning_Version1/lib/python3.11/site-packages/torch/autograd/__init__.py:204\u001B[0m, in \u001B[0;36mgrad\u001B[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001B[0m\n\u001B[1;32m    197\u001B[0m     \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    199\u001B[0m     \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m    200\u001B[0m     Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    201\u001B[0m         tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001B[1;32m    202\u001B[0m         allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m--> 204\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgrad\u001B[39m(\n\u001B[1;32m    205\u001B[0m     outputs: _TensorOrTensors,\n\u001B[1;32m    206\u001B[0m     inputs: _TensorOrTensors,\n\u001B[1;32m    207\u001B[0m     grad_outputs: Optional[_TensorOrTensors] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    208\u001B[0m     retain_graph: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    209\u001B[0m     create_graph: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    210\u001B[0m     only_inputs: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    211\u001B[0m     allow_unused: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    212\u001B[0m     is_grads_batched: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    213\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]:\n\u001B[1;32m    214\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Computes and returns the sum of gradients of outputs with respect to\u001B[39;00m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;124;03m    the inputs.\u001B[39;00m\n\u001B[1;32m    216\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;124;03m            for your use case. Defaults to ``False``.\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    265\u001B[0m     t_outputs \u001B[38;5;241m=\u001B[39m cast(Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m], (outputs,) \u001B[38;5;28;01mif\u001B[39;00m is_tensor_like(outputs) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(outputs))\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "ntk_time = 0\n",
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "t1 = time.time()\n",
    "G_r,f0_minus_y_r = delta_w_utils(copy.deepcopy(model),retain_loader,'complete')\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/G_r.npy',G_r)\n",
    "np.save('NTK_data/f0_minus_y_r.npy',f0_minus_y_r)\n",
    "del G_r, f0_minus_y_r\n",
    "\n",
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "t1 = time.time()\n",
    "G_f,f0_minus_y_f = delta_w_utils(copy.deepcopy(model),forget_loader,'retain') \n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/G_f.npy',G_f)\n",
    "np.save('NTK_data/f0_minus_y_f.npy',f0_minus_y_f)\n",
    "del G_f, f0_minus_y_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NTK_data/G_r.npy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m G_r \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNTK_data/G_r.npy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m G_f \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNTK_data/G_f.npy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m G \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate([G_r,G_f],axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/unlearning_Version1/lib/python3.11/site-packages/numpy/lib/npyio.py:427\u001B[0m, in \u001B[0;36mload\u001B[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[0m\n\u001B[1;32m    425\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    426\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 427\u001B[0m     fid \u001B[38;5;241m=\u001B[39m stack\u001B[38;5;241m.\u001B[39menter_context(\u001B[38;5;28mopen\u001B[39m(os_fspath(file), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m    428\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    430\u001B[0m \u001B[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'NTK_data/G_r.npy'"
     ]
    }
   ],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "G_f = np.load('NTK_data/G_f.npy')\n",
    "G = np.concatenate([G_r,G_f],axis=1)\n",
    "\n",
    "np.save('NTK_data/G.npy',G)\n",
    "del G, G_f, G_r\n",
    "\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "f0_minus_y_f = np.load('NTK_data/f0_minus_y_f.npy')\n",
    "f0_minus_y = np.concatenate([f0_minus_y_r,f0_minus_y_f])\n",
    "\n",
    "np.save('NTK_data/f0_minus_y.npy',f0_minus_y)\n",
    "del f0_minus_y, f0_minus_y_r, f0_minus_y_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only requires access to the gradients and the initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w_lin(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NTK_data/G.npy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m G \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNTK_data/G.npy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m t1 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m      3\u001B[0m theta \u001B[38;5;241m=\u001B[39m G\u001B[38;5;241m.\u001B[39mtranspose()\u001B[38;5;241m.\u001B[39mdot(G) \u001B[38;5;241m+\u001B[39m num_total\u001B[38;5;241m*\u001B[39margs\u001B[38;5;241m.\u001B[39mweight_decay\u001B[38;5;241m*\u001B[39mnp\u001B[38;5;241m.\u001B[39meye(G\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[0;32m~/anaconda3/envs/unlearning_Version1/lib/python3.11/site-packages/numpy/lib/npyio.py:427\u001B[0m, in \u001B[0;36mload\u001B[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[0m\n\u001B[1;32m    425\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    426\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 427\u001B[0m     fid \u001B[38;5;241m=\u001B[39m stack\u001B[38;5;241m.\u001B[39menter_context(\u001B[38;5;28mopen\u001B[39m(os_fspath(file), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m    428\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    430\u001B[0m \u001B[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'NTK_data/G.npy'"
     ]
    }
   ],
   "source": [
    "G = np.load('NTK_data/G.npy')\n",
    "t1 = time.time()\n",
    "theta = G.transpose().dot(G) + num_total*args.weight_decay*np.eye(G.shape[1])\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "del G\n",
    "\n",
    "t1 = time.time()\n",
    "theta_inv = np.linalg.inv(theta)\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta.npy',theta)\n",
    "del theta\n",
    "\n",
    "G = np.load('NTK_data/G.npy')\n",
    "f0_minus_y = np.load('NTK_data/f0_minus_y.npy')\n",
    "t1 = time.time()\n",
    "w_complete = -G.dot(theta_inv.dot(f0_minus_y))\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_inv.npy',theta_inv)\n",
    "np.save('NTK_data/w_complete.npy',w_complete)\n",
    "\n",
    "del G, f0_minus_y, theta_inv, w_complete "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w_lin(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NTK_data/G_r.npy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m G_r \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNTK_data/G_r.npy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m t1 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m      3\u001B[0m theta_r \u001B[38;5;241m=\u001B[39m G_r\u001B[38;5;241m.\u001B[39mtranspose()\u001B[38;5;241m.\u001B[39mdot(G_r) \u001B[38;5;241m+\u001B[39m num_to_retain\u001B[38;5;241m*\u001B[39margs\u001B[38;5;241m.\u001B[39mweight_decay\u001B[38;5;241m*\u001B[39mnp\u001B[38;5;241m.\u001B[39meye(G_r\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[0;32m~/anaconda3/envs/unlearning_Version1/lib/python3.11/site-packages/numpy/lib/npyio.py:427\u001B[0m, in \u001B[0;36mload\u001B[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[0m\n\u001B[1;32m    425\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    426\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 427\u001B[0m     fid \u001B[38;5;241m=\u001B[39m stack\u001B[38;5;241m.\u001B[39menter_context(\u001B[38;5;28mopen\u001B[39m(os_fspath(file), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m    428\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    430\u001B[0m \u001B[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'NTK_data/G_r.npy'"
     ]
    }
   ],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "t1 = time.time()\n",
    "theta_r = G_r.transpose().dot(G_r) + num_to_retain*args.weight_decay*np.eye(G_r.shape[1])\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "del G_r\n",
    "\n",
    "t1 = time.time()\n",
    "theta_r_inv = np.linalg.inv(theta_r)\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_r.npy',theta_r)\n",
    "del theta_r\n",
    "\n",
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "t1 = time.time()\n",
    "w_retain = -G_r.dot(theta_r_inv.dot(f0_minus_y_r))\n",
    "t2 = time.time()\n",
    "ntk_time += t2-t1\n",
    "\n",
    "np.save('NTK_data/theta_r_inv.npy',theta_r_inv)\n",
    "np.save('NTK_data/w_retain.npy',w_retain)\n",
    "\n",
    "del G_r, f0_minus_y_r, theta_r_inv, w_retain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrubbing Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NTK_data/w_complete.npy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#### Scrubbing Direction\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m w_complete \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNTK_data/w_complete.npy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m w_retain \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNTK_data/w_retain.npy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m delta_w \u001B[38;5;241m=\u001B[39m (w_retain\u001B[38;5;241m-\u001B[39mw_complete)\u001B[38;5;241m.\u001B[39msqueeze()\n",
      "File \u001B[0;32m~/anaconda3/envs/unlearning_Version1/lib/python3.11/site-packages/numpy/lib/npyio.py:427\u001B[0m, in \u001B[0;36mload\u001B[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[0m\n\u001B[1;32m    425\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    426\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 427\u001B[0m     fid \u001B[38;5;241m=\u001B[39m stack\u001B[38;5;241m.\u001B[39menter_context(\u001B[38;5;28mopen\u001B[39m(os_fspath(file), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m    428\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    430\u001B[0m \u001B[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'NTK_data/w_complete.npy'"
     ]
    }
   ],
   "source": [
    "#### Scrubbing Direction\n",
    "w_complete = np.load('NTK_data/w_complete.npy')\n",
    "w_retain = np.load('NTK_data/w_retain.npy')\n",
    "delta_w = (w_retain-w_complete).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'delta_w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[35], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m delta_w_copy \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(delta_w)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'delta_w' is not defined"
     ]
    }
   ],
   "source": [
    "delta_w_copy = copy.deepcopy(delta_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual Change in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Norm-: 0.017239464446902275\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'delta_w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m delta_w_actual \u001B[38;5;241m=\u001B[39m vectorize_params(model0)\u001B[38;5;241m-\u001B[39mvectorize_params(model)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mActual Norm-: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(delta_w_actual)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPredtn Norm-: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(delta_w)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      5\u001B[0m scale_ratio \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(delta_w_actual)\u001B[38;5;241m/\u001B[39mnp\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(delta_w)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mActual Scale: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(scale_ratio))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'delta_w' is not defined"
     ]
    }
   ],
   "source": [
    "delta_w_actual = vectorize_params(model0)-vectorize_params(model)\n",
    "\n",
    "print(f'Actual Norm-: {np.linalg.norm(delta_w_actual)}')\n",
    "print(f'Predtn Norm-: {np.linalg.norm(delta_w)}')\n",
    "scale_ratio = np.linalg.norm(delta_w_actual)/np.linalg.norm(delta_w)\n",
    "print('Actual Scale: {}'.format(scale_ratio))\n",
    "log_dict['actual_scale_ratio']=scale_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trapezium Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w_retain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[37], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m m_pred_error \u001B[38;5;241m=\u001B[39m vectorize_params(model)\u001B[38;5;241m-\u001B[39mvectorize_params(model_init)\u001B[38;5;241m-\u001B[39mw_retain\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDelta w -------: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(delta_w)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m inner \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39minner(delta_w\u001B[38;5;241m/\u001B[39mnp\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(delta_w),m_pred_error\u001B[38;5;241m/\u001B[39mnp\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(m_pred_error))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'w_retain' is not defined"
     ]
    }
   ],
   "source": [
    "m_pred_error = vectorize_params(model)-vectorize_params(model_init)-w_retain.squeeze()\n",
    "print(f\"Delta w -------: {np.linalg.norm(delta_w)}\")\n",
    "\n",
    "inner = np.inner(delta_w/np.linalg.norm(delta_w),m_pred_error/np.linalg.norm(m_pred_error))\n",
    "print(f\"Inner Product--: {inner}\")\n",
    "\n",
    "if inner<0:\n",
    "    angle = np.arccos(inner)-np.pi/2\n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.sin(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "else:\n",
    "    angle = np.arccos(inner) \n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.cos(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "\n",
    "predicted_scale=predicted_norm/np.linalg.norm(delta_w)\n",
    "predicted_scale\n",
    "print(f\"Predicted Scale:  {predicted_scale}\")\n",
    "log_dict['predicted_scale_ratio']=predicted_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized Inner Product between Prediction and Actual Scrubbing Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NIP(v1,v2):\n",
    "    nip = (np.inner(v1/np.linalg.norm(v1),v2/np.linalg.norm(v2)))\n",
    "    print(nip)\n",
    "    return nip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'delta_w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[39], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m nip\u001B[38;5;241m=\u001B[39mNIP(delta_w_actual,delta_w)\n\u001B[1;32m      2\u001B[0m log_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnip\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m=\u001B[39mnip\n",
      "\u001B[0;31mNameError\u001B[0m: name 'delta_w' is not defined"
     ]
    }
   ],
   "source": [
    "nip=NIP(delta_w_actual,delta_w)\n",
    "log_dict['nip']=nip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape delta_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_w_dict(delta_w,model):\n",
    "    # Give normalized delta_w\n",
    "    delta_w_dict = OrderedDict()\n",
    "    params_visited = 0\n",
    "    for k,p in model.named_parameters():\n",
    "        num_params = np.prod(list(p.shape))\n",
    "        update_params = delta_w[params_visited:params_visited+num_params]\n",
    "        delta_w_dict[k] = torch.Tensor(update_params).view_as(p)\n",
    "        params_visited+=num_params\n",
    "    return delta_w_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_predictions(model,dataloader,name):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metrics,activations,predictions=get_metrics(model,dataloader,criterion,True)\n",
    "    print(f\"{name} -> Loss:{np.round(metrics['loss'],3)}, Error:{metrics['error']}\")\n",
    "    log_dict[f\"{name}_loss\"]=metrics['loss']\n",
    "    log_dict[f\"{name}_error\"]=metrics['error']\n",
    "\n",
    "    return activations,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_distance(l1,l2,name):\n",
    "    dist = np.sum(np.abs(l1-l2))\n",
    "    print(f\"Predictions Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_predictions\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_distance(a1,a2,name):\n",
    "    dist = np.linalg.norm(a1-a2,ord=1,axis=1).mean()\n",
    "    print(f\"Activations Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_activations\"]=dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrub using NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_scale' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m scale\u001B[38;5;241m=\u001B[39mpredicted_scale\n\u001B[1;32m      2\u001B[0m direction \u001B[38;5;241m=\u001B[39m get_delta_w_dict(delta_w,model)\n\u001B[1;32m      4\u001B[0m model_scrub \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(model)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'predicted_scale' is not defined"
     ]
    }
   ],
   "source": [
    "scale=predicted_scale\n",
    "direction = get_delta_w_dict(delta_w,model)\n",
    "\n",
    "model_scrub = copy.deepcopy(model)\n",
    "for k,p in model_scrub.named_parameters():\n",
    "    p.data += (direction[k]*scale).to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune and Fisher Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(args.device), target.to(args.device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(args.device), target.to(args.device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        if 'mnist' in args.dataset:\n",
    "            data=data.view(data.shape[0],-1)\n",
    "        output = model(data)\n",
    "        if scrub_act:\n",
    "            G = []\n",
    "            for cls in range(num_classes):\n",
    "                grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                grads = torch.cat([g.view(-1) for g in grads])\n",
    "                G.append(grads)\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            G = torch.stack(G).pow(2)\n",
    "            delta_f = torch.matmul(G,delta_w)\n",
    "            output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(model,model_init,weight_decay):\n",
    "    l2_loss = 0\n",
    "    for (k,p),(k_init,p_init) in zip(model.named_parameters(),model_init.named_parameters()):\n",
    "        if p.requires_grad:\n",
    "            l2_loss += (p-p_init).pow(2).sum()\n",
    "    l2_loss *= (weight_decay/2.)\n",
    "    return l2_loss\n",
    "\n",
    "def run_train_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    negative_gradient=False, negative_multiplier=-1, random_labels=False,\n",
    "                    quiet=False,delta_w=None,scrub_act=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            if split=='test' and scrub_act:\n",
    "                G = []\n",
    "                for cls in range(num_classes):\n",
    "                    grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                    grads = torch.cat([g.view(-1) for g in grads])\n",
    "                    G.append(grads)\n",
    "                grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "                G = torch.stack(G).pow(2)\n",
    "                delta_f = torch.matmul(G,delta_w)\n",
    "                output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "            \n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg\n",
    "\n",
    "def run_neggrad_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    forget_loader: torch.utils.data.DataLoader,\n",
    "                    alpha: float,\n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    quiet=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, (batch_retain,batch_forget) in enumerate(tqdm(zip(data_loader,cycle(forget_loader)), leave=False)):\n",
    "            batch_retain = [tensor.to(next(model.parameters()).device) for tensor in batch_retain]\n",
    "            batch_forget = [tensor.to(next(model.parameters()).device) for tensor in batch_forget]\n",
    "            input_r, target_r = batch_retain\n",
    "            input_f, target_f = batch_forget\n",
    "            output_r = model(input_r)\n",
    "            output_f = model(input_f)\n",
    "            loss = alpha*(loss_fn(output_r, target_r) + l2_penalty(model,model_init,args.weight_decay)) - (1-alpha)*loss_fn(output_f, target_f)\n",
    "            metrics.update(n=input_r.size(0), loss=loss_fn(output_r,target_r).item(), error=get_error(output_r, target_r))\n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model: nn.Module, data_loader: torch.utils.data.DataLoader, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_vanilla(epoch, data_loader, model, loss_fn, optimizer, args)\n",
    "\n",
    "def negative_grad(model: nn.Module, data_loader: torch.utils.data.DataLoader, forget_loader: torch.utils.data.DataLoader, alpha: float, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        run_neggrad_epoch(model, model_init, data_loader, forget_loader, alpha, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_negrad(epoch, data_loader, forget_loader, model, loss_fn, optimizer,  alpha)\n",
    "\n",
    "def fk_fientune(model: nn.Module, data_loader: torch.utils.data.DataLoader, args, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "        run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        #train_negrad(epoch, data_loader, forget_loader, model, loss_fn, optimizer,  alpha)\n",
    "def test(model, data_loader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model_init=copy.deepcopy(model)\n",
    "    return run_train_epoch(model, model_init, data_loader, loss_fn, optimizer=None, split='test', epoch=epoch, ignore_index=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout_retrain(model, data_loader, test_loader, lr=0.1, epochs=500, threshold=0.01, quiet=True):\n",
    "    torch.manual_seed(seed)\n",
    "    model = copy.deepcopy(model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=True, num_samples=500)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=data_loader.batch_size, sampler=sampler, num_workers=data_loader.num_workers)\n",
    "    metrics = []\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        metrics.append(run_train_epoch(model, model_init, test_loader, loss_fn, optimizer, split='test', epoch=epoch, ignore_index=None, quiet=quiet))\n",
    "        if metrics[-1]['loss'] <= threshold:\n",
    "            break\n",
    "        run_train_epoch(model, model_init, data_loader_small, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "    return epoch, metrics\n",
    "\n",
    "def extract_retrain_time(metrics, threshold=0.1):\n",
    "    losses = np.array([m['loss'] for m in metrics])\n",
    "    return np.argmax(losses < threshold)\n",
    "\n",
    "def all_readouts(model,thresh=0.1,name='method'):\n",
    "    train_loader = torch.utils.data.DataLoader(train_loader_full.dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    retrain_time, _ = readout_retrain(model, train_loader, forget_loader, epochs=100, lr=0.1, threshold=thresh)\n",
    "    test_error = test(model, test_loader_full)['error']\n",
    "    forget_error = test(model, forget_loader)['error']\n",
    "    retain_error = test(model, retain_loader)['error']\n",
    "    print(f\"{name} ->\"\n",
    "          f\"\\tFull test error: {test_error:.2%}\"\n",
    "          f\"\\tForget error: {forget_error:.2%}\\tRetain error: {retain_error:.2%}\"\n",
    "          f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "    log_dict[f\"{name}_retrain_time\"]=retrain_time+1\n",
    "    return(dict(test_error=test_error, forget_error=forget_error, retain_error=retain_error, retrain_time=retrain_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_scrub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[50], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model_scrubf \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(model_scrub)\n\u001B[1;32m      2\u001B[0m modelf \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(model)\n\u001B[1;32m      3\u001B[0m modelf0 \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(model0)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model_scrub' is not defined"
     ]
    }
   ],
   "source": [
    "model_scrubf = copy.deepcopy(model_scrub)\n",
    "modelf = copy.deepcopy(model)\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters(), model_scrubf.parameters()):\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(dataset, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc = 0\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(args.device), orig_target.to(args.device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad_acc += (orig_target == target).float() * p.grad.data\n",
    "                    p.grad2_acc += prob[:, y] * p.grad.data.pow(2)\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc /= len(train_loader)\n",
    "        p.grad2_acc /= len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_scrubf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[52], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m hessian(retain_loader\u001B[38;5;241m.\u001B[39mdataset, model_scrubf)\n\u001B[1;32m      2\u001B[0m hessian(retain_loader\u001B[38;5;241m.\u001B[39mdataset, modelf)\n\u001B[1;32m      3\u001B[0m hessian(retain_loader\u001B[38;5;241m.\u001B[39mdataset, modelf0)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model_scrubf' is not defined"
     ]
    }
   ],
   "source": [
    "hessian(retain_loader.dataset, model_scrubf)\n",
    "hessian(retain_loader.dataset, modelf)\n",
    "hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_var(p, is_base_dist=False, alpha=3e-6):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3)\n",
    "    if p.size(0) == num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var\n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    if not is_base_dist:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    else:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    if p.size(0) == num_classes and num_to_forget is None:\n",
    "        mu[class_to_forget] = 0\n",
    "        var[class_to_forget] = 0.0001\n",
    "    if p.size(0) == num_classes:\n",
    "        # Last layer\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        # BatchNorm\n",
    "        var *= 10\n",
    "#         var*=1\n",
    "    return mu, var\n",
    "\n",
    "def kl_divergence_fisher(mu0, var0, mu1, var1):\n",
    "    return ((mu1 - mu0).pow(2)/var0 + var1/var0 - torch.log(var1/var0) - 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Noise in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[54], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m total_kl \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m      4\u001B[0m torch\u001B[38;5;241m.\u001B[39mmanual_seed(seed)\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (k, p), (k0, p0) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(modelf\u001B[38;5;241m.\u001B[39mnamed_parameters(), modelf0\u001B[38;5;241m.\u001B[39mnamed_parameters()):\n\u001B[1;32m      6\u001B[0m     mu0, var0 \u001B[38;5;241m=\u001B[39m get_mean_var(p, \u001B[38;5;28;01mFalse\u001B[39;00m, alpha\u001B[38;5;241m=\u001B[39malpha)\n\u001B[1;32m      7\u001B[0m     mu1, var1 \u001B[38;5;241m=\u001B[39m get_mean_var(p0, \u001B[38;5;28;01mTrue\u001B[39;00m, alpha\u001B[38;5;241m=\u001B[39malpha)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'modelf' is not defined"
     ]
    }
   ],
   "source": [
    "# Computes the amount of information not forgotten at all layers using the given alpha\n",
    "alpha = 1e-7\n",
    "total_kl = 0\n",
    "torch.manual_seed(seed)\n",
    "for (k, p), (k0, p0) in zip(modelf.named_parameters(), modelf0.named_parameters()):\n",
    "    mu0, var0 = get_mean_var(p, False, alpha=alpha)\n",
    "    mu1, var1 = get_mean_var(p0, True, alpha=alpha)\n",
    "    kl = kl_divergence_fisher(mu0, var0, mu1, var1).item()\n",
    "    total_kl += kl\n",
    "    print(k, f'{kl:.1f}')\n",
    "print(\"Total:\", total_kl)\n",
    "log_dict['fisher_info']=total_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[55], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m alpha \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1e-6\u001B[39m\n\u001B[1;32m      3\u001B[0m torch\u001B[38;5;241m.\u001B[39mmanual_seed(seed)\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(modelf\u001B[38;5;241m.\u001B[39mparameters()):\n\u001B[1;32m      5\u001B[0m     mu, var \u001B[38;5;241m=\u001B[39m get_mean_var(p, \u001B[38;5;28;01mFalse\u001B[39;00m, alpha\u001B[38;5;241m=\u001B[39malpha)\n\u001B[1;32m      6\u001B[0m     p\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m mu \u001B[38;5;241m+\u001B[39m var\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39mempty_like(p\u001B[38;5;241m.\u001B[39mdata0)\u001B[38;5;241m.\u001B[39mnormal_()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'modelf' is not defined"
     ]
    }
   ],
   "source": [
    "fisher_dir = []\n",
    "alpha = 1e-6\n",
    "torch.manual_seed(seed)\n",
    "for i, p in enumerate(modelf.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "    fisher_dir.append(var.sqrt().view(-1).cpu().detach().numpy())\n",
    "\n",
    "for i, p in enumerate(modelf0.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[56], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(test(modelf, retain_loader))\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(test(modelf, forget_loader))\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(test(modelf, valid_loader_full))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'modelf' is not defined"
     ]
    }
   ],
   "source": [
    "print(test(modelf, retain_loader))\n",
    "print(test(modelf, forget_loader))\n",
    "print(test(modelf, valid_loader_full))\n",
    "print(test(modelf, test_loader_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/373 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff384a68fc694e75b819ab2c05e32699"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/373 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e16975aaee004041b8b759424a92a204"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/373 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a883a6c1e753487cbe3d7704d6ec46c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/373 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68014f5359da40f589f67bb90a53963a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/373 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "736ff9f569a34e339587fc7b74dc43d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/373 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "939524ab519540d3aaa7f4aea70dc2f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/373 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7518259fc1343e6bb63fd3b6dc2af31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/373 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ada61359db442fca64838ad8427095e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/373 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24ff3a8de0384d549b4b71bc8d957945"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/373 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be9d6f776faa48539e4aae5e2d7ab249"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_ft = copy.deepcopy(model)\n",
    "retain_loader = replace_loader_dataset(train_loader_full,retain_dataset, seed=seed, batch_size=args.batch_size, shuffle=True)    \n",
    "finetune(model_ft, retain_loader, epochs=10, quiet=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "134ec7f16e944bc4b4ab23f30da520d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "70132e3a65b54c83a2af86496d71fb39"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f5e9ce3f18c4236abfb96e9d1663e08"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "070c22579d3140419d3e6f7839109306"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46a2a63900864a9ca24470c475ec7be1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f5b3a59860a4a8eabcb3703d95dff1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa47d85334bf4d0a99f9c6a2dfe657ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8e2c952a4204570800a4f6b62bd2ffd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04495976b513486ba37ad026985dfb08"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8fb3d19640884fc8a1b321272493e454"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args.ng_alpha = 0.95\n",
    "args.ng_epochs = 10\n",
    "args.ng_lr = 0.01\n",
    "model_ng = copy.deepcopy(model)    \n",
    "negative_grad(model_ng, retain_loader, forget_loader, alpha=args.ng_alpha, epochs=args.ng_epochs, quiet=True, lr=args.ng_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catastrophic Forgetting k layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[59], line 21\u001B[0m\n\u001B[1;32m     18\u001B[0m         param\u001B[38;5;241m.\u001B[39mrequires_grad_(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 21\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m\n\u001B[1;32m     24\u001B[0m fk_fientune(model_cfk, retain_loader, args\u001B[38;5;241m=\u001B[39margs, epochs\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39mcfk_epochs, quiet\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, lr\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39mcfk_lr)\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "args.lr_decay_epochs = [10,15,20]\n",
    "args.cfk_lr = 0.01\n",
    "args.cfk_epochs = 10\n",
    "\n",
    "model_cfk = copy.deepcopy(model)\n",
    "\n",
    "for param in model_cfk.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "if args.model == 'allcnn':\n",
    "    layers = [9]\n",
    "    for k in layers:\n",
    "        for param in model_cfk.features[k].parameters():\n",
    "            param.requires_grad_(True)\n",
    "    \n",
    "elif args.model == \"resnet\":\n",
    "    for param in model_cfk.layer4.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "fk_fientune(model_cfk, retain_loader, args=args, epochs=args.cfk_epochs, quiet=True, lr=args.cfk_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Unlearning k layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "' The last block and classifier of allcnn\\nAllCNN(\\n  (features): Sequential(\\n    ...\\n    (9): Conv(\\n      (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\\n      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n      (2): ReLU()\\n    )\\n    (10): AvgPool2d(kernel_size=8, stride=8, padding=0)\\n    (11): Flatten()\\n  )\\n  (classifier): Sequential(\\n    (0): Linear(in_features=192, out_features=5, bias=True)\\n  )\\n)\\n'"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" The last block and classifier of resnet-18\n",
    "(layer4): Sequential(\n",
    "    (0): _ResBlock(\n",
    "      (bn1): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv1): Conv2d(102, 204, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "      (bn2): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (shortcut): Sequential(\n",
    "        (0): Conv2d(102, 204, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "      )\n",
    "    )\n",
    "    (1): _ResBlock(\n",
    "      (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv1): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (bn2): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (conv2): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    )\n",
    "  )\n",
    "(linear): Linear(in_features=204, out_features=5, bias=True)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" The last block and classifier of allcnn\n",
    "AllCNN(\n",
    "  (features): Sequential(\n",
    "    ...\n",
    "    (9): Conv(\n",
    "      (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (2): ReLU()\n",
    "    )\n",
    "    (10): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
    "    (11): Flatten()\n",
    "  )\n",
    "  (classifier): Sequential(\n",
    "    (0): Linear(in_features=192, out_features=5, bias=True)\n",
    "  )\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[61], line 71\u001B[0m\n\u001B[1;32m     68\u001B[0m         param\u001B[38;5;241m.\u001B[39mrequires_grad_(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 71\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m\n\u001B[1;32m     73\u001B[0m fk_fientune(model_euk, retain_loader, epochs\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39meuk_epochs, quiet\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, lr\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39meuk_lr, args\u001B[38;5;241m=\u001B[39margs)\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "args.lr_decay_epochs = [10,15,20]\n",
    "args.euk_lr = 0.01\n",
    "args.euk_epochs = training_epochs\n",
    "model_euk = copy.deepcopy(model)\n",
    "\n",
    "for param in model_euk.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "if args.model == 'allcnn':\n",
    "    with torch.no_grad():\n",
    "        for k in layers:\n",
    "            for i in range(0,3):\n",
    "                try:\n",
    "                    model_euk.features[k][i].weight.copy_(model_initial.features[k][i].weight)\n",
    "                except:\n",
    "                    print (\"block {}, layer {} does not have weights\".format(k,i))\n",
    "                try:\n",
    "                    model_euk.features[k][i].bias.copy_(model_initial.features[k][i].bias)\n",
    "                except:\n",
    "                    print (\"block {}, layer {} does not have bias\".format(k,i))\n",
    "        model_euk.classifier[0].weight.copy_(model_initial.classifier[0].weight)\n",
    "        model_euk.classifier[0].bias.copy_(model_initial.classifier[0].bias)\n",
    "    \n",
    "    for k in layers:\n",
    "        for param in model_euk.features[k].parameters():\n",
    "            param.requires_grad_(True)\n",
    "    \n",
    "elif args.model == \"resnet\":\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,2):\n",
    "            try:\n",
    "                model_euk.layer4[i].bn1.weight.copy_(model_initial.layer4[i].bn1.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].bn1.bias.copy_(model_initial.layer4[i].bn1.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv1.weight.copy_(model_initial.layer4[i].conv1.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv1.bias.copy_(model_initial.layer4[i].conv1.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "\n",
    "            try:\n",
    "                model_euk.layer4[i].bn2.weight.copy_(model_initial.layer4[i].bn2.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].bn2.bias.copy_(model_initial.layer4[i].bn2.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv2.weight.copy_(model_initial.layer4[i].conv2.weight)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have weight\".format(i))\n",
    "            try:\n",
    "                model_euk.layer4[i].conv2.bias.copy_(model_initial.layer4[i].conv2.bias)\n",
    "            except:\n",
    "                print (\"block 4, layer {} does not have bias\".format(i))\n",
    "\n",
    "        model_euk.layer4[0].shortcut[0].weight.copy_(model_initial.layer4[0].shortcut[0].weight)\n",
    "        \n",
    "    for param in model_euk.layer4.parameters():\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "fk_fientune(model_euk, retain_loader, epochs=args.euk_epochs, quiet=True, lr=args.euk_lr, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_Model_D_f -> Loss:0.605, Error:0.14\n",
      "0.6054256307534625\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b91b121a052544c2b350da476f7bd820"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/79 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a82d41a90e9448e890b83896cee5f59"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d3f2ffe112342c696b87ac07d57f73a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/373 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e709e7f8cb7418887b174e9eb6bad63"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ->\tFull test error: 11.74%\tForget error: 14.00%\tRetain error: 12.45%\tFine-tune time: 1 steps\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c80c7371b9646219f110cdfb70b96ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "048bff02c010448ca60987991eabb902"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b6585d3f40649559718735343272961"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/79 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33a80ced9094470184b9f3fd411411a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b0cf1792008423397656cf65f7e622f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/373 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8dff376b3aad4fb899eb5369b3f2ebfe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain ->\tFull test error: 11.72%\tForget error: 14.00%\tRetain error: 12.48%\tFine-tune time: 2 steps\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "404c62c0b1034dafa23c3f35ee560b9c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/79 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab2fdb27717246638d4ea7caad7decd3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "859d59c59667408687e72be67237dc9b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/373 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44a29fd0bd3d47bd876de354b9e9380f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune ->\tFull test error: 9.63%\tForget error: 9.67%\tRetain error: 10.12%\tFine-tune time: 1 steps\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8310c8c784a45ce96bbb9d3f86ee2fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/79 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc84a046000e457894be3fa173ee40e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d26020c15c1466f81ca691f69c21739"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/373 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3f337a68d054bae8ec10ef43c04f9c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NegGrad ->\tFull test error: 9.70%\tForget error: 12.00%\tRetain error: 10.36%\tFine-tune time: 1 steps\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8b31f6ff0a745cab1c8faeaa7a08071"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/79 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65a564f425be48f4a1a2fc106ab5881b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b16b1f07bc942d6902b156781ba2edc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/373 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7ca21ba9da148dfa0d42317f19918df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF-k ->\tFull test error: 11.74%\tForget error: 14.00%\tRetain error: 12.45%\tFine-tune time: 1 steps\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5833ee22504f4fce9bfed1f33a799b93"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/79 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8078cab54a1446b1b0056cb8cc3c7262"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7cc21952acf7441da3dd1dfbc9b24a79"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/373 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14fad45172384dbbb13d1a663bca831e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU-k ->\tFull test error: 11.74%\tForget error: 14.00%\tRetain error: 12.45%\tFine-tune time: 1 steps\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'modelf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[62], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m readouts[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124me\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m all_readouts(copy\u001B[38;5;241m.\u001B[39mdeepcopy(model_cfk),thresh,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCF-k\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     12\u001B[0m readouts[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m all_readouts(copy\u001B[38;5;241m.\u001B[39mdeepcopy(model_euk),thresh,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEU-k\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m readouts[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mg\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m all_readouts(copy\u001B[38;5;241m.\u001B[39mdeepcopy(modelf),thresh,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFisher\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     14\u001B[0m readouts[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mh\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m all_readouts(copy\u001B[38;5;241m.\u001B[39mdeepcopy(model_scrub),thresh,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNTK\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     15\u001B[0m readouts[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mi\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m all_readouts(copy\u001B[38;5;241m.\u001B[39mdeepcopy(model_s),thresh,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSCRUB\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'modelf' is not defined"
     ]
    }
   ],
   "source": [
    "try: readouts\n",
    "except: readouts = {}\n",
    "\n",
    "_,_=activations_predictions(copy.deepcopy(model),forget_loader,'Original_Model_D_f')\n",
    "thresh=log_dict['Original_Model_D_f_loss']+1e-5\n",
    "print(thresh)\n",
    "readouts[\"a\"] = all_readouts(copy.deepcopy(model),thresh,'Original')\n",
    "readouts[\"b\"] = all_readouts(copy.deepcopy(model0),thresh,'Retrain')\n",
    "readouts[\"c\"] = all_readouts(copy.deepcopy(model_ft),thresh,'Finetune')\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model_ng),thresh,'NegGrad')\n",
    "readouts[\"e\"] = all_readouts(copy.deepcopy(model_cfk),thresh,'CF-k')\n",
    "readouts[\"f\"] = all_readouts(copy.deepcopy(model_euk),thresh,'EU-k')\n",
    "readouts[\"g\"] = all_readouts(copy.deepcopy(modelf),thresh,'Fisher')\n",
    "readouts[\"h\"] = all_readouts(copy.deepcopy(model_scrub),thresh,'NTK')\n",
    "readouts[\"i\"] = all_readouts(copy.deepcopy(model_s),thresh,'SCRUB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
